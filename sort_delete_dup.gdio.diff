diff --git a/.gitconfig b/.gitconfig
--- ./.gitconfig
+++ ./.gitconfig
@@ -5,4 +5,5 @@
     remote = origin
 [gsync]
     remote = origin
-    target = origin/dev
+    target = origin/hashSizeKey
+
diff --git a/data_serialization.go b/data_serialization.go
--- ./data_serialization.go
+++ ./data_serialization.go
@@ -1,11 +1,3 @@
 package main
 
-import (
-	"time"
-)
-
-// FileInfo holds file information
-type FileInfo struct {
-	Size    int64
-	ModTime time.Time
-}
+import ()
diff --git a/file_processing.go b/file_processing.go
--- ./file_processing.go
+++ ./file_processing.go
@@ -2,10 +2,9 @@
 package main
 
 import (
-	"bufio"
 	"bytes"
 	"context"
-	"crypto/sha256"
+	"crypto/sha512"
 	"encoding/gob"
 	"fmt"
 	"github.com/go-redis/redis/v8"
@@ -13,7 +12,6 @@ import (
 	"os"
 	"path/filepath"
 	"sort"
-	"strconv"
 	"strings"
 	"sync/atomic"
 )
@@ -105,7 +103,7 @@ func processFile(path string, typ os.FileMode, rdb *redis.Client, ctx context.Co
 		return
 	}
 
-	// fmt.Printf("File %s not found in Redis, processing.\n", path) // 添加打印信息
+	fmt.Printf("File %s not found in Redis, processing.\n", path) // 添加打印信息
 
 	info, err := os.Stat(path)
 	if err != nil {
@@ -120,31 +118,16 @@ func processFile(path string, typ os.FileMode, rdb *redis.Client, ctx context.Co
 		return
 	}
 
-	// 计算文件的SHA-256哈希值
-	fileHash, err := calculateFileHash(path)
+	// 计算文件的SHA-256哈希值（只读取前4KB）
+	fileHash, err := calculateFileHash(path, false)
 	if err != nil {
 		fmt.Printf("Error calculating hash for file %s: %s\n", path, err)
 		return
 	}
 
-	// 构造包含前缀的hashSizeKey
-	hashSizeKey := "fileHashSize:" + fileHash + "_" + strconv.FormatInt(info.Size(), 10)
-
-	// 使用管道批量处理Redis命令
-	pipe := rdb.Pipeline()
-
-	// 这里我们添加命令到管道，但不立即检查错误
-	pipe.Set(ctx, "fileInfo:"+hashedKey, buf.Bytes(), 0)
-	pipe.Set(ctx, "path:"+hashedKey, path, 0)
-	pipe.Set(ctx, "updateTime:"+hashedKey, startTime, 0)
-	pipe.Set(ctx, "hash:"+hashedKey, fileHash, 0) // 存储文件哈希值
-	// 存储从路径到hashedKey的映射
-	pipe.Set(ctx, "pathToHash:"+path, hashedKey, 0)
-	// 使用SAdd而不是Set，将路径添加到集合中
-	pipe.SAdd(ctx, hashSizeKey, path)
-
-	if _, err = pipe.Exec(ctx); err != nil {
-		fmt.Printf("Error executing pipeline for file: %s: %s\n", path, err)
+	// 调用saveFileInfoToRedis函数来保存文件信息到Redis，不需要完整文件的哈希值
+	if err := saveFileInfoToRedis(rdb, ctx, hashedKey, path, buf, startTime, fileHash, ""); err != nil {
+		fmt.Printf("Error saving file info to Redis for file %s: %s\n", path, err)
 		return
 	}
 }
@@ -156,22 +139,32 @@ func formatFileInfoLine(fileInfo FileInfo, relativePath string, sortByModTime bo
 	return fmt.Sprintf("%d,\"./%s\"", fileInfo.Size, relativePath)
 }
 
-// calculateFileHash 计算文件的SHA-256哈希值
-func calculateFileHash(path string) (string, error) {
+// calculateFileHash 计算文件的 SHA-512 哈希值
+// 读取前4KB的数据，除非fullRead参数为true
+func calculateFileHash(path string, fullRead bool) (string, error) {
 	file, err := os.Open(path)
 	if err != nil {
 		return "", err
 	}
 	defer file.Close()
 
-	const readLimit = 4 * 1024 // 限制读取的数据量为 4 KB
-	reader := bufio.NewReaderSize(file, readLimit)
-	limitedReader := io.LimitReader(reader, readLimit)
+	hasher := sha512.New()
+	if fullRead {
+		// 打印正在计算完整哈希的文件路径
+		fmt.Printf("Calculating full hash for file: %s\n", path)
 
-	hasher := sha256.New()
-	if _, err := io.Copy(hasher, limitedReader); err != nil {
+		// 读取整个文件
+		if _, err := io.Copy(hasher, file); err != nil {
+			return "", err
+		}
+	} else {
+		// 只读取前100KB的数据
+		const readLimit = 100 * 1024
+		reader := io.LimitReader(file, readLimit)
+		if _, err := io.Copy(hasher, reader); err != nil {
 			return "", err
 		}
+	}
 
 	return fmt.Sprintf("%x", hasher.Sum(nil)), nil
 }
@@ -191,16 +184,8 @@ func processSymlink(path string) {
 func processKeyword(keyword string, keywordFiles []string, rdb *redis.Client, ctx context.Context, rootDir string) {
 	// 对 keywordFiles 进行排序
 	sort.Slice(keywordFiles, func(i, j int) bool {
-		fullPath := filepath.Join(rootDir, cleanPath(keywordFiles[i]))
-		sizeI, errI := getFileSizeFromRedis(rdb, ctx, fullPath)
-		if errI != nil {
-			fmt.Printf("Error getting size for %s: %v\n", fullPath, errI)
-		}
-		fullPath = filepath.Join(rootDir, cleanPath(keywordFiles[j]))
-		sizeJ, errJ := getFileSizeFromRedis(rdb, ctx, fullPath)
-		if errJ != nil {
-			fmt.Printf("Error getting size for %s: %v\n", fullPath, errJ)
-		}
+		sizeI, _ := getFileSize(rdb, ctx, filepath.Join(rootDir, cleanPath(keywordFiles[i])))
+		sizeJ, _ := getFileSize(rdb, ctx, filepath.Join(rootDir, cleanPath(keywordFiles[j])))
 		return sizeI > sizeJ
 	})
 
@@ -208,11 +193,7 @@ func processKeyword(keyword string, keywordFiles []string, rdb *redis.Client, ct
 	var outputData strings.Builder
 	outputData.WriteString(keyword + "\n")
 	for _, filePath := range keywordFiles {
-		fullPath := filepath.Join(rootDir, cleanPath(filePath))
-		fileSize, err := getFileSizeFromRedis(rdb, ctx, fullPath)
-		if err != nil {
-			fmt.Printf("Error getting size for %s : %v\n", fullPath, err)
-		}
+		fileSize, _ := getFileSize(rdb, ctx, filepath.Join(rootDir, cleanPath(filePath)))
 		outputData.WriteString(fmt.Sprintf("%d,%s\n", fileSize, filePath))
 	}
 
@@ -250,3 +231,12 @@ func cleanPath(path string) string {
 
 	return path
 }
+
+func getFileSize(rdb *redis.Client, ctx context.Context, fullPath string) (int64, error) {
+	size, err := getFileSizeFromRedis(rdb, ctx, fullPath)
+	if err != nil {
+		fmt.Printf("Error getting size for %s: %v\n", fullPath, err)
+		return 0, err
+	}
+	return size, nil
+}
diff --git a/go.mod b/go.mod
--- ./go.mod
+++ ./go.mod
@@ -4,7 +4,7 @@ go 1.18
 
 require (
 	github.com/allegro/bigcache v1.2.1 // indirect
-	github.com/cespare/xxhash/v2 v2.2.0 // indirect
+	github.com/cespare/xxhash/v2 v2.3.0 // indirect
 	github.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f // indirect
 	github.com/go-redis/redis/v8 v8.11.5 // indirect
 	github.com/karrick/godirwalk v1.17.0 // indirect
diff --git a/go.sum b/go.sum
--- ./go.sum
+++ ./go.sum
@@ -2,6 +2,8 @@ github.com/allegro/bigcache v1.2.1 h1:hg1sY1raCwic3Vnsvje6TT7/pnZba83LeFck5NrFKS
 github.com/allegro/bigcache v1.2.1/go.mod h1:Cb/ax3seSYIx7SuZdm2G2xzfwmv3TPSk2ucNfQESPXM=
 github.com/cespare/xxhash/v2 v2.2.0 h1:DC2CZ1Ep5Y4k3ZQ899DldepgrayRUGE6BBZ/cd9Cj44=
 github.com/cespare/xxhash/v2 v2.2.0/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=
+github.com/cespare/xxhash/v2 v2.3.0 h1:UL815xU9SqsFlibzuggzjXhog7bL6oX9BbNZnL2UFvs=
+github.com/cespare/xxhash/v2 v2.3.0/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=
 github.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f h1:lO4WD4F/rVNCu3HqELle0jiPLLBs70cWOduZpkS1E78=
 github.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f/go.mod h1:cuUVRXasLTGF7a8hSLbxyZXjz+1KgoB3wDUb6vlszIc=
 github.com/go-redis/redis/v8 v8.11.5 h1:AcZZR7igkdvfVmQTPnu9WE37LRrO/YrBH5zWyjDC0oI=
diff --git a/main.go b/main.go
--- ./main.go
+++ ./main.go
@@ -13,7 +13,6 @@ import (
 	"path/filepath"
 	"regexp"
 	"sort"
-	"sync"
 	"sync/atomic"
 	"time"
 )
@@ -22,24 +21,48 @@ var progressCounter int32 // Progress counter
 
 func main() {
 	startTime := time.Now().Unix()
-	rootDir, minSizeBytes, excludeRegexps, rdb, ctx, err := initializeApp(os.Args)
+	rootDir, minSizeBytes, excludeRegexps, rdb, ctx, deleteDuplicates, outputDuplicates, err := initializeApp(os.Args)
 	if err != nil {
 		fmt.Println(err)
 		return
 	}
 
-	ctx, cancel := context.WithCancel(context.Background())
-	defer cancel()
+	// 根据参数决定是否输出重复文件结果到文件
+	if outputDuplicates {
+		err = writeDuplicateFilesToFile(rootDir, "fav.log.dup", rdb, ctx)
+		if err != nil {
+			fmt.Println("Error writing duplicates to file:", err)
+		}
+		return // 如果输出重复文件，则结束程序
+	}
+
+	// 根据参数决定是否删除重复文件
+	if deleteDuplicates {
+		err = deleteDuplicateFiles(rootDir, rdb, ctx)
+		if err != nil {
+			fmt.Println("Error deleting duplicate files:", err)
+		}
+		return // 如果删除重复文件，则结束程序
+	}
+
+	// 创建一个新的上下文和取消函数
+	progressCtx, progressCancel := context.WithCancel(ctx)
+	defer progressCancel()
 
-	go monitorProgress(ctx, &progressCounter)
+	// 启动进度监控 Goroutine
+	go monitorProgress(progressCtx, &progressCounter)
 
 	workerCount := 500
-	taskQueue, poolWg := NewWorkerPool(workerCount)
+	taskQueue, poolWg, stopPool := NewWorkerPool(workerCount) // 修改此处
 
 	walkFiles(rootDir, minSizeBytes, excludeRegexps, taskQueue, rdb, ctx, startTime)
 
-	close(taskQueue)
+	stopPool() // 使用停止函数来关闭任务队列
 	poolWg.Wait()
+
+	// 此时所有任务已经完成，取消进度监控上下文
+	progressCancel()
+
 	fmt.Printf("Final progress: %d files processed.\n", atomic.LoadInt32(&progressCounter))
 
 	err = cleanUpOldRecords(rdb, ctx, startTime)
@@ -50,18 +73,19 @@ func main() {
 	// 文件处理完成后的保存操作
 	performSaveOperation(rootDir, "fav.log", false, rdb, ctx)
 	performSaveOperation(rootDir, "fav.log.sort", true, rdb, ctx)
-	findAndLogDuplicates(rootDir, "fav.log.dup", rdb, ctx)
+
+	// 查找重复文件并记录结果到Redis
+	err = findAndLogDuplicates(rootDir, "fav.log.dup", rdb, ctx)
+	if err != nil {
+		fmt.Println("Error finding and logging duplicates:", err)
+	}
 
 	// 新增逻辑：处理 fav.log 文件，类似于 find_sort_similar_filenames 函数的操作
-	favLogPath := filepath.Join(rootDir, "fav.log") // 假设 fav.log 在 rootDir 目录下
-	// 重新初始化工作池和等待组，用于第二批任务
-	taskQueue, poolWg = NewWorkerPool(workerCount)
-	processFavLog(favLogPath, rootDir, rdb, ctx, taskQueue, poolWg)
-	close(taskQueue)
-	poolWg.Wait()
+	// favLogPath := filepath.Join(rootDir, "fav.log") // 假设 fav.log 在 rootDir 目录下
+	// processFavLog(favLogPath, rootDir, rdb, ctx)
 }
 
-func processFavLog(filePath string, rootDir string, rdb *redis.Client, ctx context.Context, taskQueue chan<- Task, poolWg *sync.WaitGroup) {
+func processFavLog(filePath string, rootDir string, rdb *redis.Client, ctx context.Context) {
 	file, err := os.Open(filePath)
 	if err != nil {
 		fmt.Println("Error opening file:", err)
@@ -78,26 +102,38 @@ func processFavLog(filePath string, rootDir string, rdb *redis.Client, ctx conte
 		fileNames = append(fileNames, extractFileName(line))
 	}
 
+	// 确定工作池的大小并调用 extractKeywords
 	keywords := extractKeywords(fileNames)
+
 	closeFiles := findCloseFiles(fileNames, filePaths, keywords)
 
+	// 排序关键词
 	sort.Slice(keywords, func(i, j int) bool {
 		return len(closeFiles[keywords[i]]) > len(closeFiles[keywords[j]])
 	})
 
 	totalKeywords := len(keywords)
+
+	workerCount := 500
+	taskQueue, poolWg, stopPool := NewWorkerPool(workerCount) // 修改此处
+
 	for i, keyword := range keywords {
 		keywordFiles := closeFiles[keyword]
 		if len(keywordFiles) >= 2 {
-			// 直接定义并执行一个匿名函数
+			poolWg.Add(1) // 在将任务发送到队列之前增加计数
 			taskQueue <- func(kw string, kf []string, idx int) Task {
 				return func() {
+					defer poolWg.Done() // 确保在任务结束时减少计数
 					fmt.Printf("Processing keyword %d of %d: %s\n", idx+1, totalKeywords, kw)
 					processKeyword(kw, kf, rdb, ctx, rootDir)
 				}
-			}(keyword, keywordFiles, i) // 立即传递当前迭代的变量
+			}(keyword, keywordFiles, i)
 		}
 	}
+
+	stopPool() // 使用停止函数来关闭任务队列
+
+	poolWg.Wait()
 }
 
 // 初始化Redis客户端
@@ -114,13 +150,24 @@ func newRedisClient(ctx context.Context) *redis.Client {
 }
 
 // initializeApp 初始化应用程序设置
-func initializeApp(args []string) (string, int64, []*regexp.Regexp, *redis.Client, context.Context, error) {
+func initializeApp(args []string) (string, int64, []*regexp.Regexp, *redis.Client, context.Context, bool, bool, error) {
 	if len(args) < 2 {
-		return "", 0, nil, nil, nil, fmt.Errorf("usage: ./find_large_files_with_cache <directory>")
+		return "", 0, nil, nil, nil, false, false, fmt.Errorf("Usage: %s <rootDir> [--delete-duplicates] [--output-duplicates]", args[0])
 	}
 
 	// Root directory to start the search
 	rootDir := args[1]
+	deleteDuplicates := false
+	outputDuplicates := false
+
+	// 解析参数
+	for _, arg := range args {
+		if arg == "--delete-duplicates" {
+			deleteDuplicates = true
+		} else if arg == "--output-duplicates" {
+			outputDuplicates = true
+		}
+	}
 
 	// Minimum file size in bytes
 	minSize := 200 // Default size is 200MB
@@ -132,7 +179,7 @@ func initializeApp(args []string) (string, int64, []*regexp.Regexp, *redis.Clien
 	ctx := context.Background()
 	rdb := newRedisClient(ctx)
 
-	return rootDir, minSizeBytes, excludeRegexps, rdb, ctx, nil
+	return rootDir, minSizeBytes, excludeRegexps, rdb, ctx, deleteDuplicates, outputDuplicates, nil
 }
 
 // walkFiles 遍历指定目录下的文件，并根据条件进行处理
diff --git a/main.go.sh b/main.go.sh
--- ./main.go.sh
+++ ./main.go.sh
@@ -10,9 +10,22 @@ go get -u github.com/go-redis/redis/v8
 go get -u github.com/mattn/go-zglob/fastwalk
 go get -u github.com/karrick/godirwalk
 
-#docker-compose down -v
-#docker-compose restart
+docker-compose down -v
+docker-compose restart
 docker-compose up -d
 
-#go run . /media/secure_bcache/av/onlyfans/
-go run . /media
+#rm /media/av162/cartoon/dragonball/test/*.txt
+#go run . /media/av162/cartoon/dragonball/test/
+#ls -al /media/av162/cartoon/dragonball/test/*.txt
+
+# 定义路径变量，确保处理包含空格和特殊字符的情况
+rootDir="/media/av162/av/旬果/"
+
+# 正常运行
+go run . "$rootDir"
+
+# 输出重复文件结果
+go run . "$rootDir" --output-duplicates
+
+# 删除重复文件（示例，实际运行时取消注释）
+# go run . "$rootDir" --delete-duplicates
diff --git a/redis_client.go b/redis_client.go
--- ./redis_client.go
+++ ./redis_client.go
@@ -9,7 +9,8 @@ import (
 	"encoding/hex"
 	"fmt"
 	"github.com/go-redis/redis/v8"
-	"strconv"
+	"os"
+	"path/filepath" // 添加导入
 	"strings"
 )
 
@@ -20,63 +21,130 @@ func generateHash(s string) string {
 	return hex.EncodeToString(hasher.Sum(nil))
 }
 
+// 将重复文件的信息存储到 Redis
+func saveDuplicateFileInfoToRedis(rdb *redis.Client, ctx context.Context, fullHash string, info fileInfo) error {
+	// 使用管道批量处理 Redis 命令
+	pipe := rdb.Pipeline()
+
+	// 将路径添加到有序集合 duplicateFiles:<fullHash> 中，并使用文件名长度的负值作为分数
+	fileNameLength := len(filepath.Base(info.path))
+	pipe.ZAdd(ctx, "duplicateFiles:"+fullHash, &redis.Z{
+		Score:  float64(-fileNameLength), // 取负值
+		Member: info.path,
+	})
+
+	if _, err := pipe.Exec(ctx); err != nil {
+		return fmt.Errorf("error executing pipeline for duplicate file: %s: %w", info.path, err)
+	}
+	return nil
+}
+
+func saveFileInfoToRedis(rdb *redis.Client, ctx context.Context, hashedKey string, path string, buf bytes.Buffer, startTime int64, fileHash string, fullHash string) error {
+	// 使用管道批量处理Redis命令
+	pipe := rdb.Pipeline()
+
+	// 这里我们添加命令到管道，但不立即检查错误
+	pipe.Set(ctx, "fileInfo:"+hashedKey, buf.Bytes(), 0)
+	pipe.Set(ctx, "path:"+hashedKey, path, 0)
+	pipe.Set(ctx, "updateTime:"+hashedKey, startTime, 0)
+	pipe.SAdd(ctx, "hash:"+fileHash, path) // 将文件路径存储为集合
+	if fullHash != "" {
+		pipe.Set(ctx, "fullHash:"+hashedKey, fullHash, 0) // 存储完整文件哈希值
+	}
+	// 存储从路径到hashedKey的映射
+	pipe.Set(ctx, "pathToHash:"+path, hashedKey, 0)
+
+	if _, err := pipe.Exec(ctx); err != nil {
+		return fmt.Errorf("error executing pipeline for file: %s: %w", path, err)
+	}
+	return nil
+}
+
 func cleanUpOldRecords(rdb *redis.Client, ctx context.Context, startTime int64) error {
+	fmt.Printf("func cleanUpOldRecords \n")
 	iter := rdb.Scan(ctx, 0, "updateTime:*", 0).Iterator()
 	for iter.Next(ctx) {
 		updateTimeKey := iter.Val()
-		updateTime, err := rdb.Get(ctx, updateTimeKey).Int64()
-		if err != nil {
-			fmt.Printf("Error retrieving updateTime for key %s: %s\n", updateTimeKey, err)
-			continue
-		}
 
-		if updateTime < startTime {
 		// 解析出原始的hashedKey
 		hashedKey := strings.TrimPrefix(updateTimeKey, "updateTime:")
 
 		// 获取文件路径和文件哈希值
 		filePath, err := rdb.Get(ctx, "path:"+hashedKey).Result()
-			fileHash, errHash := rdb.Get(ctx, "hash:"+hashedKey).Result()
-			if err != nil || errHash != nil {
-				fmt.Printf("Error retrieving filePath or file hash for key %s: %s, %s\n", hashedKey, err, errHash)
+		if err != nil && err != redis.Nil {
+			fmt.Printf("Error retrieving filePath for key %s: %s\n", hashedKey, err)
 			continue
 		}
 
+		fileHash, err := calculateFileHash(filePath, false)
+		if err != nil {
+			fmt.Printf("Error calculating hash for file %s: %s\n", filePath, err)
+			continue
+		}
+
+		filePaths, errHash := rdb.SMembers(ctx, "hash:"+fileHash).Result()
+		if errHash != nil && errHash != redis.Nil {
+			fmt.Printf("Error retrieving file paths for key %s: %s\n", hashedKey, errHash)
+			continue
+		}
+
+		// 检查文件是否存在
+		if _, err := os.Stat(filePath); os.IsNotExist(err) {
+			fmt.Printf("File does not exist, cleaning up records: %s\n", filePath)
+
 			// 获取文件信息
 			fileInfoData, err := rdb.Get(ctx, "fileInfo:"+hashedKey).Bytes()
-			if err != nil {
+			if err != nil && err != redis.Nil {
 				fmt.Printf("Error retrieving fileInfo for key %s: %s\n", hashedKey, err)
 				continue
 			}
 
 			// 解码文件信息
 			var fileInfo FileInfo
+			if len(fileInfoData) > 0 {
 				buf := bytes.NewBuffer(fileInfoData)
 				dec := gob.NewDecoder(buf)
 				if err := dec.Decode(&fileInfo); err != nil {
 					fmt.Printf("Error decoding fileInfo for key %s: %s\n", hashedKey, err)
 					continue
 				}
+			}
+
+			// 构造 duplicateFilesKey
+			for _, filePath := range filePaths {
+				fmt.Printf("Calculating hash for file: %s\n", filePath)
+				fileHash, err := calculateFileHash(filePath, false)
+				if err != nil {
+					fmt.Printf("Error calculating hash for file %s: %s\n", filePath, err)
+					continue
+				}
 
-			// 构造hashSizeKey
-			hashSizeKey := "fileHashSize:" + fileHash + "_" + strconv.FormatInt(fileInfo.Size, 10)
+				duplicateFilesKey := "duplicateFiles:" + fileHash
 
 				// 删除记录
-			pipe := rdb.Pipeline()
+				pipe := rdb.TxPipeline()
 				pipe.Del(ctx, updateTimeKey)                // 删除updateTime键
 				pipe.Del(ctx, "fileInfo:"+hashedKey)        // 删除fileInfo相关数据
 				pipe.Del(ctx, "path:"+hashedKey)            // 删除path相关数据
-			pipe.Del(ctx, "hash:"+hashedKey)      // 删除hash相关数据
+				pipe.SRem(ctx, "hash:"+fileHash, filePath)  // 从集合中移除文件路径
 				pipe.Del(ctx, "pathToHash:"+filePath)       // 删除从路径到hashedKey的映射
-			pipe.SRem(ctx, hashSizeKey, filePath) // 从fileHashSize集合中移除路径
+				pipe.Del(ctx, "fullHash:"+hashedKey)        // 删除完整文件哈希相关数据
+				pipe.ZRem(ctx, duplicateFilesKey, filePath) // 从 duplicateFiles 有序集合中移除路径
 
 				_, err = pipe.Exec(ctx)
 				if err != nil {
 					fmt.Printf("Error deleting keys for outdated record %s: %s\n", hashedKey, err)
 				} else {
-				fmt.Printf("Deleted outdated record: path=%s, hash=%s, size=%d\n", filePath, fileHash, fileInfo.Size)
+					fmt.Printf("Deleted outdated record: path=%s, size=%d\n", filePath, fileInfo.Size)
 				}
 			}
 		}
+	}
+
+	if err := iter.Err(); err != nil {
+		fmt.Printf("Error during iteration: %s\n", err)
+		return err
+	}
+
 	return nil
 }
diff --git a/utils.go b/utils.go
--- ./utils.go
+++ ./utils.go
@@ -14,8 +14,9 @@ import (
 	"path/filepath"
 	"regexp"
 	"sort"
-	"strconv"
 	"strings"
+	"sync"
+	"time"
 )
 
 func loadExcludePatterns(filename string) ([]string, error) {
@@ -87,78 +88,292 @@ func writeLinesToFile(filename string, lines []string) error {
 	return nil
 }
 
-func findAndLogDuplicates(rootDir string, outputFile string, rdb *redis.Client, ctx context.Context) error {
-	fmt.Println("Starting to find duplicates...")
+// FileInfo holds file information
+type FileInfo struct {
+	Size    int64
+	ModTime time.Time
+}
 
-	// Scan用于查找所有fileHashSize键
-	iter := rdb.Scan(ctx, 0, "fileHashSize:*", 0).Iterator()
-	type hashSize struct {
-		key  string
-		size int64
+type fileInfo struct {
+	name      string
+	path      string
+	buf       bytes.Buffer
+	startTime int64
+	fileHash  string
+	fullHash  string
+	line      string
+	header    string
+	FileInfo  // 嵌入已有的FileInfo结构体
 }
-	var hashSizes []hashSize
 
-	for iter.Next(ctx) {
-		fileHashSizeKey := iter.Val()
-		parts := strings.Split(fileHashSizeKey, "_")
-		if len(parts) != 2 {
-			fmt.Printf("Invalid key format: %s\n", fileHashSizeKey)
+var mu sync.Mutex
+
+// 用信号量来限制并发数
+var semaphore = make(chan struct{}, 100) // 同时最多打开100个文件
+
+// 处理每个文件哈希并查找重复文件
+func processFileHash(rootDir string, fileHash string, filePaths []string, rdb *redis.Client, ctx context.Context, processedFullHashes map[string]bool) (int, error) {
+	fileCount := 0
+
+	if len(filePaths) > 1 {
+		fmt.Printf("Found duplicate files for hash %s: %v\n", fileHash, filePaths)
+		header := fmt.Sprintf("Duplicate files for hash %s:", fileHash)
+		hashes := make(map[string][]fileInfo)
+		for _, fullPath := range filePaths {
+			// 确保只处理rootDir下的文件
+			if !strings.HasPrefix(fullPath, rootDir) {
+				fmt.Printf("Skipping file outside root directory: %s\n", fullPath)
 				continue
 			}
-		size, err := strconv.ParseInt(parts[1], 10, 64)
-		if err != nil {
-			fmt.Printf("Error parsing size from key %s: %s\n", fileHashSizeKey, err)
+
+			// 检查文件是否存在
+			if _, err := os.Stat(fullPath); os.IsNotExist(err) {
+				fmt.Printf("File does not exist: %s\n", fullPath)
 				continue
 			}
 
-		hashSizes = append(hashSizes, hashSize{key: fileHashSizeKey, size: size})
+			semaphore <- struct{}{} // 获取一个信号量
+			relativePath, err := filepath.Rel(rootDir, fullPath)
+			if err != nil {
+				fmt.Printf("Error converting to relative path: %s\n", err)
+				<-semaphore // 释放信号量
+				continue
 			}
+			fileName := filepath.Base(relativePath)
 
-	if err := iter.Err(); err != nil {
-		fmt.Println("Iterator error:", err)
-		return err
+			// 获取或计算完整文件的SHA-256哈希值
+			hashedKey := generateHash(fullPath)
+			fullHash, err := rdb.Get(ctx, "fullHash:"+hashedKey).Result()
+			var buf bytes.Buffer
+			if err == redis.Nil {
+				fullHash, err = calculateFileHash(fullPath, true)
+				if err != nil {
+					fmt.Printf("Error calculating full hash for file %s: %s\n", fullPath, err)
+					<-semaphore // 释放信号量
+					continue
 				}
 
-	// 根据文件大小排序哈希
-	sort.Slice(hashSizes, func(i, j int) bool {
-		return hashSizes[i].size > hashSizes[j].size
-	})
+				fileHash, err := calculateFileHash(fullPath, false)
+				if err != nil {
+					fmt.Printf("Error calculating hash for file %s: %s\n", fullPath, err)
+					continue
+				} else {
+					fmt.Printf("Calculated hash for file %s: %s\n", fullPath, fileHash)
+				}
 
-	var lines []string
-	for _, hs := range hashSizes {
-		filePaths, err := rdb.SMembers(ctx, hs.key).Result()
+				// 获取文件信息并编码
+				info, err := os.Stat(fullPath)
 				if err != nil {
-			fmt.Printf("Error getting file paths for key %s: %s\n", hs.key, err)
+					fmt.Printf("Error stating file: %s, Error: %s\n", fullPath, err)
+					<-semaphore // 释放信号量
 					continue
 				}
 
-		if len(filePaths) > 1 {
-			line := fmt.Sprintf("Duplicate files for hash size %d:", hs.size)
-			lines = append(lines, line)
-			for _, fullPath := range filePaths {
-				relativePath, err := filepath.Rel(rootDir, fullPath)
+				enc := gob.NewEncoder(&buf)
+				if err := enc.Encode(FileInfo{Size: info.Size(), ModTime: info.ModTime()}); err != nil {
+					fmt.Printf("Error encoding: %s, File: %s\n", err, fullPath)
+					<-semaphore // 释放信号量
+					continue
+				}
+
+				// 调用saveFileInfoToRedis函数来保存文件信息到Redis
+				if err := saveFileInfoToRedis(rdb, ctx, hashedKey, fullPath, buf, time.Now().Unix(), fileHash, fullHash); err != nil {
+					fmt.Printf("Error saving file info to Redis for file %s: %s\n", fullPath, err)
+					<-semaphore // 释放信号量
+					continue
+				}
+			} else if err != nil {
+				fmt.Printf("Error getting full hash for file %s from Redis: %s\n", fullPath, err)
+				<-semaphore // 释放信号量
+				continue
+			}
+
+			info, err := os.Stat(fullPath) // 获取文件信息
 			if err != nil {
-					fmt.Printf("Error converting to relative path: %s\n", err)
+				fmt.Printf("Error stating file %s: %s\n", fullPath, err)
+				<-semaphore // 释放信号量
 				continue
 			}
-				lines = append(lines, fmt.Sprintf("%d,\"./%s\"", hs.size, relativePath))
+
+			infoStruct := fileInfo{
+				name:      fileName,
+				path:      fullPath,
+				buf:       buf,
+				startTime: time.Now().Unix(),
+				fileHash:  hashedKey,
+				fullHash:  fullHash,
+				line:      fmt.Sprintf("%d,\"./%s\"", info.Size(), relativePath),
+				header:    header,
+				FileInfo:  FileInfo{Size: info.Size(), ModTime: info.ModTime()},
+			}
+			hashes[fileHash] = append(hashes[fileHash], infoStruct)
+			fileCount++
+			<-semaphore // 释放信号量
+		}
+		for fileHash, infos := range hashes {
+			if len(infos) > 1 {
+				fmt.Printf("Writing duplicate files for hash %s: %v\n", fileHash, infos)
+				mu.Lock()
+				if !processedFullHashes[fileHash] {
+					processedFullHashes[fileHash] = true
+					for _, info := range infos {
+						err := saveDuplicateFileInfoToRedis(rdb, ctx, fileHash, info)
+						if err != nil {
+							fmt.Printf("Error saving duplicate file info to Redis for hash %s: %s\n", fileHash, err)
+						}
+					}
+				}
+				mu.Unlock()
+			}
+		}
+	}
+	return fileCount, nil
+}
+
+// 主函数
+func findAndLogDuplicates(rootDir string, outputFile string, rdb *redis.Client, ctx context.Context) error {
+	fmt.Println("Starting to find duplicates...")
+
+	fileHashes, err := scanFileHashes(rdb, ctx)
+	if err != nil {
+		return err
 	}
+
+	fmt.Printf("Scanned file hashes: %v\n", fileHashes)
+
+	fileCount := 0
+
+	// 用于存储已处理的文件哈希
+	processedFullHashes := make(map[string]bool)
+
+	var wg sync.WaitGroup // 等待 goroutine 完成
+	var mu sync.Mutex     // 用于保护对 fileCount 的并发访问
+
+	for fileHash, filePaths := range fileHashes {
+		fmt.Printf("Processing fileHash: %s, filePaths: %v\n", fileHash, filePaths)
+		wg.Add(1)
+		go func(fileHash string, filePaths []string) {
+			defer wg.Done()
+
+			count, err := processFileHash(rootDir, fileHash, filePaths, rdb, ctx, processedFullHashes)
+			if err != nil {
+				fmt.Printf("Error processing file hash %s: %s\n", fileHash, err)
+				return
 			}
+			mu.Lock()
+			fileCount += count
+			mu.Unlock()
+		}(fileHash, filePaths)
 	}
 
-	if len(lines) == 0 {
+	wg.Wait()
+
+	fmt.Printf("Processed %d files\n", fileCount)
+
+	if fileCount == 0 {
 		fmt.Println("No duplicates found.")
 		return nil
 	}
 
-	outputFile = filepath.Join(rootDir, outputFile)
-	err := writeLinesToFile(outputFile, lines)
+	return nil
+}
+
+func scanFileHashes(rdb *redis.Client, ctx context.Context) (map[string][]string, error) {
+	iter := rdb.Scan(ctx, 0, "hash:*", 0).Iterator()
+	fileHashes := make(map[string][]string)
+	for iter.Next(ctx) {
+		hashKey := iter.Val()
+		fileHash := strings.TrimPrefix(hashKey, "hash:")
+		duplicateFiles, err := rdb.SMembers(ctx, hashKey).Result()
+		if err != nil {
+			return nil, fmt.Errorf("error retrieving duplicate files for key %s: %w", hashKey, err)
+		}
+		fmt.Printf("Found hashKey: %s with files: %v\n", hashKey, duplicateFiles)
+		fileHashes[fileHash] = duplicateFiles
+	}
+	if err := iter.Err(); err != nil {
+		return nil, fmt.Errorf("error during iteration: %w", err)
+	}
+	return fileHashes, nil
+}
+
+func writeDuplicateFilesToFile(rootDir string, outputFile string, rdb *redis.Client, ctx context.Context) error {
+	file, err := os.Create(filepath.Join(rootDir, outputFile))
 	if err != nil {
-		fmt.Printf("Error writing to file %s: %s\n", outputFile, err)
+		return fmt.Errorf("Error creating output file: %s", err)
+	}
+	defer file.Close()
+
+	iter := rdb.Scan(ctx, 0, "duplicateFiles:*", 0).Iterator()
+	for iter.Next(ctx) {
+		duplicateFilesKey := iter.Val()
+
+		// 获取 fullHash
+		fullHash := strings.TrimPrefix(duplicateFilesKey, "duplicateFiles:")
+
+		// 获取重复文件列表，按文件名长度（score）排序
+		duplicateFiles, err := rdb.ZRange(ctx, duplicateFilesKey, 0, -1).Result()
+		if err != nil {
+			fmt.Printf("Error retrieving duplicate files for key %s: %s\n", duplicateFilesKey, err)
+			continue
+		}
+
+		if len(duplicateFiles) > 1 {
+			header := fmt.Sprintf("Duplicate files for hash %s:\n", fullHash)
+			if _, err := file.WriteString(header); err != nil {
+				fmt.Printf("Error writing header to file %s: %s\n", outputFile, err)
+				continue
+			}
+			for _, duplicateFile := range duplicateFiles {
+				// 获取文件信息
+				hashedKey, err := getHashedKeyFromPath(rdb, ctx, duplicateFile)
+				if err != nil {
+					fmt.Printf("Error retrieving hashed key for path %s: %s\n", duplicateFile, err)
+					continue
+				}
+				fileInfoData, err := rdb.Get(ctx, "fileInfo:"+hashedKey).Bytes()
+				if err != nil {
+					fmt.Printf("Error retrieving fileInfo for key %s: %s\n", hashedKey, err)
+					continue
+				}
+
+				// 解码文件信息
+				var fileInfo FileInfo
+				buf := bytes.NewBuffer(fileInfoData)
+				dec := gob.NewDecoder(buf)
+				if err := dec.Decode(&fileInfo); err != nil {
+					fmt.Printf("Error decoding fileInfo for key %s: %s\n", hashedKey, err)
+					continue
+				}
+
+				// 获取相对路径
+				relativePath, err := filepath.Rel(rootDir, duplicateFile)
+				if err != nil {
+					fmt.Printf("Error getting relative path for %s: %s\n", duplicateFile, err)
+					continue
+				}
+
+				// 使用 filepath.Clean 来规范化路径
+				cleanPath := filepath.Clean(relativePath)
+				line := fmt.Sprintf("%d,\"./%s\"\n", fileInfo.Size, cleanPath)
+				if _, err := file.WriteString(line); err != nil {
+					fmt.Printf("Error writing file path to file %s: %s\n", outputFile, err)
+					continue
+				}
+			}
+			if _, err := file.WriteString("\n"); err != nil {
+				fmt.Printf("Error writing newline to file %s: %s\n", outputFile, err)
+				continue
+			}
+		}
+	}
+
+	if err := iter.Err(); err != nil {
+		fmt.Printf("Error during iteration: %s\n", err)
 		return err
 	}
 
-	fmt.Printf("Duplicates written to %s\n", outputFile)
+	fmt.Printf("Duplicates written to %s\n", filepath.Join(rootDir, outputFile))
 	return nil
 }
 
@@ -194,25 +409,53 @@ func extractFileName(filePath string) string {
 	return strings.ToLower(filepath.Base(filePath))
 }
 
-// extractKeywords extracts keywords from a slice of file names.
+var pattern = regexp.MustCompile(`\b(?:\d{2}\.\d{2}\.\d{2}|(?:\d+|[a-z]+(?:\d+[a-z]*)?))\b`)
+
 func extractKeywords(fileNames []string) []string {
-	keywords := make(map[string]struct{})
-	pattern := regexp.MustCompile(`\b(?:\d{2}\.\d{2}\.\d{2}|(?:\d+|[a-z]+(?:\d+[a-z]*)?))\b`)
+	workerCount := 100
+	// 创建自己的工作池
+	taskQueue, poolWg, stopPool := NewWorkerPool(workerCount)
+
+	keywordsCh := make(chan string, len(fileNames)*10) // 假设每个文件名大约有10个关键词
 
 	for _, fileName := range fileNames {
-		nameWithoutExt := strings.TrimSuffix(fileName, filepath.Ext(fileName))
+		poolWg.Add(1) // 确保在任务开始前递增计数
+		taskQueue <- func(name string) Task {
+			return func() {
+				defer func() {
+					poolWg.Done() // 任务结束时递减计数
+				}()
+
+				nameWithoutExt := strings.TrimSuffix(name, filepath.Ext(name))
 				matches := pattern.FindAllString(nameWithoutExt, -1)
 				for _, match := range matches {
-			keywords[match] = struct{}{}
+					keywordsCh <- match
+				}
 			}
+		}(fileName)
+	}
+
+	stopPool() // 使用停止函数来关闭任务队列
+
+	// 关闭通道的逻辑保持不变
+	go func() {
+		poolWg.Wait()
+		close(keywordsCh)
+	}()
+
+	// 收集关键词
+	keywordsMap := make(map[string]struct{})
+	for keyword := range keywordsCh {
+		keywordsMap[keyword] = struct{}{}
 	}
 
-	var keywordList []string
-	for keyword := range keywords {
-		keywordList = append(keywordList, keyword)
+	// 将map转换为slice
+	var keywords []string
+	for keyword := range keywordsMap {
+		keywords = append(keywords, keyword)
 	}
 
-	return keywordList
+	return keywords
 }
 
 func findCloseFiles(fileNames, filePaths, keywords []string) map[string][]string {
@@ -228,3 +471,53 @@ func findCloseFiles(fileNames, filePaths, keywords []string) map[string][]string
 
 	return closeFiles
 }
+
+func deleteDuplicateFiles(rootDir string, rdb *redis.Client, ctx context.Context) error {
+	iter := rdb.Scan(ctx, 0, "duplicateFiles:*", 0).Iterator()
+	for iter.Next(ctx) {
+		duplicateFilesKey := iter.Val()
+
+		// 获取 fullHash
+		fullHash := strings.TrimPrefix(duplicateFilesKey, "duplicateFiles:")
+
+		// 获取重复文件列表
+		duplicateFiles, err := rdb.ZRange(ctx, duplicateFilesKey, 0, -1).Result()
+		if err != nil {
+			fmt.Printf("Error retrieving duplicate files for key %s: %s\n", duplicateFilesKey, err)
+			continue
+		}
+
+		if len(duplicateFiles) > 1 {
+			fmt.Printf("Processing duplicate files for hash %s:\n", fullHash)
+
+			// 保留第一个文件（你可以根据自己的需求修改保留策略）
+			fileToKeep := duplicateFiles[0]
+
+			// 检查第一个文件是否存在
+			if _, err := os.Stat(fileToKeep); os.IsNotExist(err) {
+				fmt.Printf("File to keep does not exist: %s\n", fileToKeep)
+				continue
+			}
+
+			filesToDelete := duplicateFiles[1:]
+
+			for _, duplicateFile := range filesToDelete {
+				// 删除文件
+				err := os.Remove(duplicateFile)
+				if err != nil {
+					fmt.Printf("Error deleting file %s: %s\n", duplicateFile, err)
+					continue
+				}
+				fmt.Printf("Deleted duplicate file: %s\n", duplicateFile)
+			}
+		}
+	}
+
+	if err := iter.Err(); err != nil {
+		fmt.Printf("Error during iteration: %s\n", err)
+		return err
+	}
+
+	fmt.Println("Duplicate files deleted successfully.")
+	return nil
+}
diff --git a/worker_pool.go b/worker_pool.go
--- ./worker_pool.go
+++ ./worker_pool.go
@@ -1,3 +1,4 @@
+// worker_pool.go
 package main
 
 import (
@@ -7,8 +8,7 @@ import (
 // Task 定义了工作池中的任务类型
 type Task func()
 
-// NewWorkerPool 创建并返回一个工作池
-func NewWorkerPool(workerCount int) (chan<- Task, *sync.WaitGroup) {
+func NewWorkerPool(workerCount int) (chan<- Task, *sync.WaitGroup, func()) {
 	var wg sync.WaitGroup
 	taskQueue := make(chan Task)
 
@@ -22,5 +22,9 @@ func NewWorkerPool(workerCount int) (chan<- Task, *sync.WaitGroup) {
 		}()
 	}
 
-	return taskQueue, &wg
+	stopFunc := func() {
+		close(taskQueue) // 关闭包装后的任务队列
+	}
+
+	return taskQueue, &wg, stopFunc
 }
