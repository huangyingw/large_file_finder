diff --git a/.gitconfig b/.gitconfig
--- ./.gitconfig
+++ ./.gitconfig
@@ -1,5 +1,5 @@
 [remote "origin"]
-	url = git@github.com:huangyingw/large_file_finder.git
+    url = git@github.com:huangyingw/efficient_file_scanner.git
     fetch = +refs/heads/*:refs/remotes/origin/*
 [push]
     remote = origin
diff --git a/data_serialization.go b/data_serialization.go
new file mode 100644
--- /dev/null
+++ ./data_serialization.go
@@ -0,0 +1,11 @@
+package main
+
+import (
+	"time"
+)
+
+// FileInfo holds file information
+type FileInfo struct {
+	Size    int64
+	ModTime time.Time
+}
diff --git a/file_processing.go b/file_processing.go
new file mode 100644
--- /dev/null
+++ ./file_processing.go
@@ -0,0 +1,223 @@
+// file_processing.go
+package main
+
+import (
+	"bufio"
+	"bytes"
+	"context"
+	"crypto/sha256"
+	"encoding/gob"
+	"fmt"
+	"github.com/go-redis/redis/v8"
+	"io"
+	"os"
+	"path/filepath"
+	"sort"
+	"strings"
+	"sync/atomic"
+)
+
+func getFileInfoFromRedis(rdb *redis.Client, ctx context.Context, hashedKey string) (FileInfo, error) {
+	var fileInfo FileInfo
+	value, err := rdb.Get(ctx, hashedKey).Bytes()
+	if err != nil {
+		return fileInfo, err
+	}
+
+	buf := bytes.NewBuffer(value)
+	dec := gob.NewDecoder(buf)
+	err = dec.Decode(&fileInfo)
+	return fileInfo, err
+}
+
+func saveToFile(dir, filename string, sortByModTime bool, rdb *redis.Client, ctx context.Context) error {
+	iter := rdb.Scan(ctx, 0, "*", 0).Iterator()
+	var data = make(map[string]FileInfo)
+	for iter.Next(ctx) {
+		hashedKey := iter.Val()
+		originalPath, err := rdb.Get(ctx, "path:"+hashedKey).Result()
+		if err != nil {
+			continue
+		}
+
+		fileInfo, err := getFileInfoFromRedis(rdb, ctx, hashedKey)
+		if err == nil {
+			data[originalPath] = fileInfo
+		}
+	}
+
+	var lines []string
+	var keys []string
+	for k := range data {
+		keys = append(keys, k)
+	}
+	sortKeys(keys, data, sortByModTime)
+	for _, k := range keys {
+		relativePath, _ := filepath.Rel(dir, k)
+		line := formatFileInfoLine(data[k], relativePath, sortByModTime)
+		lines = append(lines, line)
+	}
+
+	return writeLinesToFile(filepath.Join(dir, filename), lines)
+}
+
+func processFile(path string, typ os.FileMode, rdb *redis.Client, ctx context.Context, startTime int64) {
+	// Update progress counter atomically
+	atomic.AddInt32(&progressCounter, 1)
+
+	// 生成文件路径的哈希作为键
+	hashedKey := generateHash(path)
+
+	// 检查Redis中是否已存在该文件的信息
+	exists, err := rdb.Exists(ctx, hashedKey).Result()
+	if err != nil {
+		fmt.Printf("Error checking existence in Redis for file %s: %s\n", path, err)
+		return
+	}
+	if exists > 0 {
+		// 文件已存在于Redis中，更新其更新时间戳
+		_, err := rdb.Set(ctx, "updateTime:"+hashedKey, startTime, 0).Result()
+		if err != nil {
+			fmt.Printf("Error updating updateTime for file %s: %s\n", path, err)
+		}
+		return
+	}
+
+	info, err := os.Stat(path)
+	if err != nil {
+		fmt.Printf("Error stating file: %s, Error: %s\n", path, err)
+		return
+	}
+
+	var buf bytes.Buffer
+	enc := gob.NewEncoder(&buf)
+	if err := enc.Encode(FileInfo{Size: info.Size(), ModTime: info.ModTime()}); err != nil {
+		fmt.Printf("Error encoding: %s, File: %s\n", err, path)
+		return
+	}
+
+	// 计算文件的SHA-256哈希值
+	fileHash, err := calculateFileHash(path)
+	if err != nil {
+		fmt.Printf("Error calculating hash for file %s: %s\n", path, err)
+		return
+	}
+
+	// 使用管道批量处理Redis命令
+	pipe := rdb.Pipeline()
+
+	// 这里我们添加命令到管道，但不立即检查错误
+	pipe.Set(ctx, hashedKey, buf.Bytes(), 0)
+	pipe.Set(ctx, "path:"+hashedKey, path, 0)
+	pipe.Set(ctx, "updateTime:"+hashedKey, startTime, 0)
+	pipe.Set(ctx, "hash:"+hashedKey, fileHash, 0) // 存储文件哈希值
+	// 存储从路径到hashedKey的映射
+	pipe.Set(ctx, "pathToHash:"+path, hashedKey, 0)
+
+	if _, err = pipe.Exec(ctx); err != nil {
+		fmt.Printf("Error executing pipeline for file: %s: %s\n", path, err)
+		return
+	}
+}
+
+func formatFileInfoLine(fileInfo FileInfo, relativePath string, sortByModTime bool) string {
+	if sortByModTime {
+		return fmt.Sprintf("\"./%s\"", relativePath)
+	}
+	return fmt.Sprintf("%d,\"./%s\"", fileInfo.Size, relativePath)
+}
+
+// calculateFileHash 计算文件的SHA-256哈希值
+func calculateFileHash(path string) (string, error) {
+	file, err := os.Open(path)
+	if err != nil {
+		return "", err
+	}
+	defer file.Close()
+
+	const readLimit = 8 * 1024 // 限制读取的数据量为 4 KB
+	reader := bufio.NewReaderSize(file, readLimit)
+	limitedReader := io.LimitReader(reader, readLimit)
+
+	hasher := sha256.New()
+	if _, err := io.Copy(hasher, limitedReader); err != nil {
+		return "", err
+	}
+
+	return fmt.Sprintf("%x", hasher.Sum(nil)), nil
+}
+
+func processDirectory(path string) {
+	// 处理目录的逻辑
+	fmt.Printf("Processing directory: %s\n", path)
+	// 可能的操作：遍历目录下的文件等
+}
+
+func processSymlink(path string) {
+	// 处理软链接的逻辑
+	fmt.Printf("Processing symlink: %s\n", path)
+	// 可能的操作：解析软链接，获取实际文件等
+}
+
+func processKeyword(keyword string, keywordFiles []string, rdb *redis.Client, ctx context.Context, rootDir string) {
+	// 对 keywordFiles 进行排序
+	sort.Slice(keywordFiles, func(i, j int) bool {
+		fullPath := filepath.Join(rootDir, cleanPath(keywordFiles[i]))
+		sizeI, errI := getFileSizeFromRedis(rdb, ctx, fullPath)
+		if errI != nil {
+			fmt.Printf("Error getting size for %s: %v\n", fullPath, errI)
+		}
+		fullPath = filepath.Join(rootDir, cleanPath(keywordFiles[j]))
+		sizeJ, errJ := getFileSizeFromRedis(rdb, ctx, fullPath)
+		if errJ != nil {
+			fmt.Printf("Error getting size for %s: %v\n", fullPath, errJ)
+		}
+		return sizeI > sizeJ
+	})
+
+	// 准备要写入的数据
+	var outputData strings.Builder
+	outputData.WriteString(keyword + "\n")
+	for _, filePath := range keywordFiles {
+		fullPath := filepath.Join(rootDir, cleanPath(filePath))
+		fileSize, err := getFileSizeFromRedis(rdb, ctx, fullPath)
+		if err != nil {
+			fmt.Printf("Error getting size for %s : %v\n", fullPath, err)
+		}
+		outputData.WriteString(fmt.Sprintf("%d,%s\n", fileSize, filePath))
+	}
+
+	// 创建并写入文件
+	outputFilePath := filepath.Join(rootDir, keyword+".txt")
+	outputFile, err := os.Create(outputFilePath)
+	if err != nil {
+		fmt.Println("Error creating file:", err)
+		return
+	}
+	defer outputFile.Close() // 确保文件会被关闭
+
+	_, err = outputFile.WriteString(outputData.String())
+	if err != nil {
+		fmt.Println("Error writing to file:", err)
+	}
+}
+
+// cleanPath 用于清理和标准化路径
+func cleanPath(path string) string {
+	// 先去除路径开头的引号（如果存在）
+	if strings.HasPrefix(path, `"`) {
+		path = strings.TrimPrefix(path, `"`)
+	}
+
+	// 再去除 "./"（如果路径以 "./" 开头）
+	if strings.HasPrefix(path, "./") {
+		path = strings.TrimPrefix(path, "./")
+	}
+
+	// 最后去除路径末尾的引号（如果存在）
+	if strings.HasSuffix(path, `"`) {
+		path = strings.TrimSuffix(path, `"`)
+	}
+
+	return path
+}
diff --git a/find_large_files_with_cache.go b/find_large_files_with_cache.go
deleted file mode 100644
--- ./find_large_files_with_cache.go
+++ /dev/null
@@ -1,298 +0,0 @@
-package main
-
-import (
-	"bufio"
-	"bytes"
-	"context"
-	"crypto/sha256"
-	"encoding/gob"
-	"encoding/hex"
-	"fmt"
-	"github.com/go-redis/redis/v8"
-	"github.com/karrick/godirwalk"
-	"os"
-	"path/filepath"
-	"regexp"
-	"sort"
-	"strings"
-	"sync"
-	"sync/atomic"
-	"time"
-)
-
-var progressCounter int32 // Progress counter
-var rdb *redis.Client     // Redis client
-var ctx = context.Background()
-
-// FileInfo holds file information
-type FileInfo struct {
-	Size    int64
-	ModTime time.Time
-}
-
-// Task 定义了工作池中的任务类型
-type Task func()
-
-// NewWorkerPool 创建并返回一个工作池
-func NewWorkerPool(workerCount int) (chan<- Task, *sync.WaitGroup) {
-	var wg sync.WaitGroup
-	taskQueue := make(chan Task)
-
-	for i := 0; i < workerCount; i++ {
-		wg.Add(1)
-		go func() {
-			defer wg.Done()
-			for task := range taskQueue {
-				task()
-			}
-		}()
-	}
-
-	return taskQueue, &wg
-}
-
-var wg sync.WaitGroup
-var workerPool = make(chan struct{}, 20) // Limit concurrency to 20
-
-// Initialize Redis client
-func init() {
-	rdb = redis.NewClient(&redis.Options{
-		Addr: "localhost:6379",
-	})
-	_, err := rdb.Ping(ctx).Result()
-	if err != nil {
-		fmt.Println("Error connecting to Redis:", err)
-		os.Exit(1)
-	}
-}
-
-// Generate a SHA-256 hash for the given string
-func generateHash(s string) string {
-	hasher := sha256.New()
-	hasher.Write([]byte(s))
-	return hex.EncodeToString(hasher.Sum(nil))
-}
-
-func processDirectory(path string) {
-	// 处理目录的逻辑
-	fmt.Printf("Processing directory: %s\n", path)
-	// 可能的操作：遍历目录下的文件等
-}
-
-func processSymlink(path string) {
-	// 处理软链接的逻辑
-	fmt.Printf("Processing symlink: %s\n", path)
-	// 可能的操作：解析软链接，获取实际文件等
-}
-
-func loadExcludePatterns(filename string) ([]string, error) {
-	file, err := os.Open(filename)
-	if err != nil {
-		return nil, err
-	}
-	defer file.Close()
-
-	var patterns []string
-	scanner := bufio.NewScanner(file)
-	for scanner.Scan() {
-		pattern := scanner.Text()
-		fmt.Printf("Loaded exclude pattern: %s\n", pattern) // 打印每个加载的模式
-		patterns = append(patterns, pattern)
-	}
-	return patterns, scanner.Err()
-}
-
-func saveToFile(dir, filename string, sortByModTime bool) error {
-	file, err := os.Create(filepath.Join(dir, filename))
-	if err != nil {
-		return err
-	}
-	defer file.Close()
-
-	iter := rdb.Scan(ctx, 0, "*", 0).Iterator()
-	var data = make(map[string]FileInfo)
-	for iter.Next(ctx) {
-		hashedKey := iter.Val()
-		originalPath, err := rdb.Get(ctx, "path:"+hashedKey).Result()
-		if err != nil {
-			continue
-		}
-		value, err := rdb.Get(ctx, hashedKey).Bytes()
-		if err != nil {
-			continue
-		}
-		var fileInfo FileInfo
-		buf := bytes.NewBuffer(value)
-		dec := gob.NewDecoder(buf)
-		if err := dec.Decode(&fileInfo); err == nil {
-			data[originalPath] = fileInfo
-		}
-	}
-
-	var keys []string
-	for k := range data {
-		keys = append(keys, k)
-	}
-
-	sortKeys(keys, data, sortByModTime)
-
-	for _, k := range keys {
-		relativePath, _ := filepath.Rel(dir, k)
-		if sortByModTime {
-			utcTimestamp := data[k].ModTime.UTC().Unix()
-			fmt.Fprintf(file, "%d,\"./%s\"\n", utcTimestamp, relativePath)
-		} else {
-			fmt.Fprintf(file, "%d,\"./%s\"\n", data[k].Size, relativePath)
-		}
-	}
-	return nil
-}
-
-func sortKeys(keys []string, data map[string]FileInfo, sortByModTime bool) {
-	if sortByModTime {
-		sort.Slice(keys, func(i, j int) bool {
-			return data[keys[i]].ModTime.After(data[keys[j]].ModTime)
-		})
-	} else {
-		sort.Slice(keys, func(i, j int) bool {
-			return data[keys[i]].Size > data[keys[j]].Size
-		})
-	}
-}
-
-func processFile(path string, typ os.FileMode) {
-	if typ.IsDir() {
-		return
-	}
-
-	info, err := os.Stat(path)
-	if err != nil {
-		fmt.Printf("Error stating file: %s, Error: %s\n", path, err)
-		return
-	}
-
-	var buf bytes.Buffer
-	enc := gob.NewEncoder(&buf)
-	if err := enc.Encode(FileInfo{Size: info.Size(), ModTime: info.ModTime()}); err != nil {
-		fmt.Printf("Error encoding: %s, File: %s\n", err, path)
-		return
-	}
-
-	// Generate hash for the file path
-	hashedKey := generateHash(path)
-
-	// 使用管道批量处理Redis命令
-	pipe := rdb.Pipeline()
-
-	// 这里我们添加命令到管道，但不立即检查错误
-	pipe.Set(ctx, hashedKey, buf.Bytes(), 0)
-	pipe.Set(ctx, "path:"+hashedKey, path, 0)
-
-	if _, err = pipe.Exec(ctx); err != nil {
-		fmt.Printf("Error executing pipeline for file: %s: %s\n", path, err)
-		return
-	}
-
-	// Update progress counter atomically
-	atomic.AddInt32(&progressCounter, 1)
-}
-
-func main() {
-	if len(os.Args) < 2 {
-		fmt.Println("Usage: ./find_large_files_with_cache <directory>")
-		return
-	}
-
-	// Root directory to start the search
-	rootDir := os.Args[1]
-
-	// Minimum file size in bytes
-	minSize := 200 // Default size is 200MB
-	minSizeBytes := int64(minSize * 1024 * 1024)
-
-	excludePatterns, err := loadExcludePatterns(filepath.Join(rootDir, "exclude_patterns.txt"))
-	if err != nil {
-		fmt.Println("Warning: Could not read exclude patterns:", err)
-	}
-
-	excludeRegexps := make([]*regexp.Regexp, len(excludePatterns))
-	for i, pattern := range excludePatterns {
-		// 将通配符模式转换为正则表达式
-		regexPattern := strings.Replace(pattern, "*", ".*", -1)
-		excludeRegexps[i], err = regexp.Compile(regexPattern)
-		if err != nil {
-			fmt.Printf("Invalid regex pattern '%s': %s\n", regexPattern, err)
-			return
-		}
-	}
-
-	// Start a goroutine to periodically print progress
-	go func() {
-		for {
-			time.Sleep(1 * time.Second)
-			fmt.Printf("Progress: %d files processed.\n", atomic.LoadInt32(&progressCounter))
-		}
-	}()
-
-	// Use godirwalk.Walk instead of fastwalk.Walk or filepath.Walk
-	// 初始化工作池
-	workerCount := 20 // 可以根据需要调整工作池的大小
-	taskQueue, poolWg := NewWorkerPool(workerCount)
-
-	// 使用 godirwalk.Walk 遍历文件
-	err = godirwalk.Walk(rootDir, &godirwalk.Options{
-		Callback: func(osPathname string, de *godirwalk.Dirent) error {
-			// 排除模式匹配
-			for _, re := range excludeRegexps {
-				if re.MatchString(osPathname) {
-					return nil
-				}
-			}
-
-			fileInfo, err := os.Lstat(osPathname)
-			if err != nil {
-				fmt.Printf("Error getting file info: %s\n", err)
-				return err
-			}
-
-			// 检查文件大小是否满足最小阈值
-			if fileInfo.Size() < minSizeBytes {
-				return nil
-			}
-
-			// 将任务发送到工作池
-			taskQueue <- func() {
-				if fileInfo.Mode().IsDir() {
-					processDirectory(osPathname)
-				} else if fileInfo.Mode().IsRegular() {
-					processFile(osPathname, fileInfo.Mode())
-				} else if fileInfo.Mode()&os.ModeSymlink != 0 {
-					processSymlink(osPathname)
-				} else {
-					fmt.Printf("Skipping unknown type: %s\n", osPathname)
-				}
-			}
-
-			return nil
-		},
-		Unsorted: true,
-	})
-
-	// 关闭任务队列，并等待所有任务完成
-	close(taskQueue)
-	poolWg.Wait()
-	fmt.Printf("Final progress: %d files processed.\n", atomic.LoadInt32(&progressCounter))
-
-	// 文件处理完成后的保存操作
-	if err := saveToFile(rootDir, "fav.log", false); err != nil {
-		fmt.Printf("Error saving to fav.log: %s\n", err)
-	} else {
-		fmt.Printf("Saved data to %s\n", filepath.Join(rootDir, "fav.log"))
-	}
-
-	if err := saveToFile(rootDir, "fav.log.sort", true); err != nil {
-		fmt.Printf("Error saving to fav.log.sort: %s\n", err)
-	} else {
-		fmt.Printf("Saved sorted data to %s\n", filepath.Join(rootDir, "fav.log.sort"))
-	}
-}
diff --git a/main.go b/main.go
new file mode 100644
--- /dev/null
+++ ./main.go
@@ -0,0 +1,187 @@
+// main.go
+// 此文件是Go程序的主要入口点，包含了文件处理程序的核心逻辑和初始化代码。
+
+package main
+
+import (
+	"bufio"
+	"context"
+	"fmt"
+	"github.com/go-redis/redis/v8"
+	"github.com/karrick/godirwalk"
+	"os"
+	"path/filepath"
+	"regexp"
+	"sort"
+	"sync"
+	"sync/atomic"
+	"time"
+)
+
+var progressCounter int32 // Progress counter
+
+func main() {
+	startTime := time.Now().Unix()
+	rootDir, minSizeBytes, excludeRegexps, rdb, ctx, err := initializeApp(os.Args)
+	if err != nil {
+		fmt.Println(err)
+		return
+	}
+
+	ctx, cancel := context.WithCancel(context.Background())
+	defer cancel()
+
+	go monitorProgress(ctx, &progressCounter)
+
+	workerCount := 500
+	taskQueue, poolWg := NewWorkerPool(workerCount)
+
+	walkFiles(rootDir, minSizeBytes, excludeRegexps, taskQueue, rdb, ctx, startTime)
+
+	close(taskQueue)
+	poolWg.Wait()
+	fmt.Printf("Final progress: %d files processed.\n", atomic.LoadInt32(&progressCounter))
+
+	err = cleanUpOldRecords(rdb, ctx, startTime)
+	if err != nil {
+		fmt.Println("Error cleaning up old records:", err)
+	}
+
+	// 文件处理完成后的保存操作
+	performSaveOperation(rootDir, "fav.log", false, rdb, ctx)
+	performSaveOperation(rootDir, "fav.log.sort", true, rdb, ctx)
+	findAndLogDuplicates(rootDir, "fav.log.dup", rdb, ctx)
+
+	// 新增逻辑：处理 fav.log 文件，类似于 find_sort_similar_filenames 函数的操作
+	favLogPath := filepath.Join(rootDir, "fav.log") // 假设 fav.log 在 rootDir 目录下
+	processFavLog(favLogPath, rootDir, rdb, ctx, taskQueue, poolWg)
+}
+
+func processFavLog(filePath string, rootDir string, rdb *redis.Client, ctx context.Context, taskQueue chan<- Task, poolWg *sync.WaitGroup) {
+	file, err := os.Open(filePath)
+	if err != nil {
+		fmt.Println("Error opening file:", err)
+		return
+	}
+	defer file.Close()
+
+	var fileNames, filePaths []string
+	scanner := bufio.NewScanner(file)
+	for scanner.Scan() {
+		line := scanner.Text()
+		line = regexp.MustCompile(`^\d+,`).ReplaceAllString(line, "")
+		filePaths = append(filePaths, line)
+		fileNames = append(fileNames, extractFileName(line))
+	}
+
+	keywords := extractKeywords(fileNames)
+	closeFiles := findCloseFiles(fileNames, filePaths, keywords)
+
+	sort.Slice(keywords, func(i, j int) bool {
+		return len(closeFiles[keywords[i]]) > len(closeFiles[keywords[j]])
+	})
+
+	totalKeywords := len(keywords)
+	for i, keyword := range keywords {
+		keywordFiles := closeFiles[keyword]
+		if len(keywordFiles) >= 2 {
+			poolWg.Add(1) // 在提交任务之前增加等待组的计数
+			go func(kw string, kf []string) {
+				defer poolWg.Done() // 任务完成后调用 Done
+				fmt.Printf("Processing keyword %d of %d: %s\n", i+1, totalKeywords, kw)
+				processKeyword(kw, kf, rdb, ctx, rootDir)
+			}(keyword, keywordFiles)
+		}
+	}
+}
+
+// 初始化Redis客户端
+func newRedisClient(ctx context.Context) *redis.Client {
+	rdb := redis.NewClient(&redis.Options{
+		Addr: "localhost:6379", // 该地址应从配置中获取
+	})
+	_, err := rdb.Ping(ctx).Result()
+	if err != nil {
+		fmt.Println("Error connecting to Redis:", err)
+		os.Exit(1)
+	}
+	return rdb
+}
+
+// initializeApp 初始化应用程序设置
+func initializeApp(args []string) (string, int64, []*regexp.Regexp, *redis.Client, context.Context, error) {
+	if len(args) < 2 {
+		return "", 0, nil, nil, nil, fmt.Errorf("usage: ./find_large_files_with_cache <directory>")
+	}
+
+	// Root directory to start the search
+	rootDir := args[1]
+
+	// Minimum file size in bytes
+	minSize := 200 // Default size is 200MB
+	minSizeBytes := int64(minSize * 1024 * 1024)
+
+	excludeRegexps, _ := compileExcludePatterns(filepath.Join(rootDir, "exclude_patterns.txt"))
+
+	// 创建 Redis 客户端
+	ctx := context.Background()
+	rdb := newRedisClient(ctx)
+
+	return rootDir, minSizeBytes, excludeRegexps, rdb, ctx, nil
+}
+
+// walkFiles 遍历指定目录下的文件，并根据条件进行处理
+func walkFiles(rootDir string, minSizeBytes int64, excludeRegexps []*regexp.Regexp, taskQueue chan<- Task, rdb *redis.Client, ctx context.Context, startTime int64) error {
+	return godirwalk.Walk(rootDir, &godirwalk.Options{
+		Callback: func(osPathname string, dirent *godirwalk.Dirent) error {
+			// 排除模式匹配
+			for _, re := range excludeRegexps {
+				if re.MatchString(osPathname) {
+					return nil
+				}
+			}
+
+			fileInfo, err := os.Lstat(osPathname)
+			if err != nil {
+				fmt.Printf("Error getting file info: %s\n", err)
+				return err
+			}
+
+			// 检查文件大小是否满足最小阈值
+			if fileInfo.Size() < minSizeBytes {
+				return nil
+			}
+
+			// 将任务发送到工作池
+			taskQueue <- func() {
+				if fileInfo.Mode().IsDir() {
+					processDirectory(osPathname)
+				} else if fileInfo.Mode().IsRegular() {
+					processFile(osPathname, fileInfo.Mode(), rdb, ctx, startTime)
+				} else if fileInfo.Mode()&os.ModeSymlink != 0 {
+					processSymlink(osPathname)
+				} else {
+					fmt.Printf("Skipping unknown type: %s\n", osPathname)
+				}
+			}
+			return nil
+		},
+		Unsorted: true, // 设置为true以提高性能
+	})
+}
+
+// monitorProgress 在给定的上下文中定期打印处理进度
+func monitorProgress(ctx context.Context, progressCounter *int32) {
+	ticker := time.NewTicker(1 * time.Second)
+	defer ticker.Stop()
+
+	for {
+		select {
+		case <-ctx.Done(): // 检查上下文是否被取消
+			return
+		case <-ticker.C: // 每秒触发一次
+			processed := atomic.LoadInt32(progressCounter)
+			fmt.Printf("Progress: %d files processed.\n", processed)
+		}
+	}
+}
diff --git a/find_large_files_with_cache.go.sh b/main.go.sh
similarity index 64%
rename from find_large_files_with_cache.go.sh
rename to main.go.sh
--- ./find_large_files_with_cache.go.sh
+++ ./main.go.sh
@@ -9,8 +10,9 @@ go get -u github.com/go-redis/redis/v8
 go get -u github.com/mattn/go-zglob/fastwalk
 go get -u github.com/karrick/godirwalk
 
-#docker-compose down -v && docker-compose up -d
-docker-compose restart
+#docker-compose down -v
+#docker-compose restart
+docker-compose up -d
 
-go build find_large_files_with_cache.go && \
-    sudo ./find_large_files_with_cache /media
+go run . /media/secure_bcache/av/onlyfans
+#go run . /media
diff --git a/redis_client.go b/redis_client.go
new file mode 100644
--- /dev/null
+++ ./redis_client.go
@@ -0,0 +1,123 @@
+// redis_client.go
+package main
+
+import (
+	"bytes"
+	"context"
+	"crypto/sha256"
+	"encoding/gob"
+	"encoding/hex"
+	"fmt"
+	"github.com/go-redis/redis/v8"
+	"strings"
+)
+
+// Generate a SHA-256 hash for the given string
+func generateHash(s string) string {
+	hasher := sha256.New()
+	hasher.Write([]byte(s))
+	return hex.EncodeToString(hasher.Sum(nil))
+}
+
+func cleanUpOldRecords(rdb *redis.Client, ctx context.Context, startTime int64) error {
+	iter := rdb.Scan(ctx, 0, "updateTime:*", 0).Iterator()
+	for iter.Next(ctx) {
+		updateTimeKey := iter.Val()
+		updateTime, err := rdb.Get(ctx, updateTimeKey).Int64()
+		if err != nil {
+			fmt.Printf("Error retrieving updateTime for key %s: %s\n", updateTimeKey, err)
+			continue
+		}
+
+		if updateTime < startTime {
+			// 解析出原始的hashedKey
+			hashedKey := strings.TrimPrefix(updateTimeKey, "updateTime:")
+
+			// 获取与文件相关的数据
+			fileInfoData, err := rdb.Get(ctx, hashedKey).Bytes()
+			if err != nil {
+				fmt.Printf("Error retrieving fileInfo for key %s: %s\n", hashedKey, err)
+				continue
+			}
+
+			// 解码文件信息
+			var fileInfo FileInfo
+			buf := bytes.NewBuffer(fileInfoData)
+			dec := gob.NewDecoder(buf)
+			if err := dec.Decode(&fileInfo); err != nil {
+				fmt.Printf("Error decoding fileInfo for key %s: %s\n", hashedKey, err)
+				continue
+			}
+
+			// 获取文件路径
+			filePath, err := rdb.Get(ctx, "path:"+hashedKey).Result()
+			if err != nil {
+				fmt.Printf("Error retrieving filePath for key %s: %s\n", hashedKey, err)
+				continue
+			}
+
+			// 删除记录
+			pipe := rdb.Pipeline()
+			pipe.Del(ctx, updateTimeKey)     // 删除updateTime键
+			pipe.Del(ctx, hashedKey)         // 删除与hashedKey相关的数据
+			pipe.Del(ctx, "path:"+hashedKey) // 删除与path:hashedKey相关的数据
+
+			// 获取文件哈希值
+			fileHash, err := rdb.Get(ctx, "hash:"+hashedKey).Result()
+			if err != nil {
+				fmt.Printf("Error retrieving file hash for key %s: %s\n", hashedKey, err)
+			} else {
+				fmt.Printf("Deleting record with hash %s\n", fileHash)
+			}
+
+			pipe.Del(ctx, "hash:"+hashedKey) // 删除与文件哈希值相关的键
+
+			// 新增：删除从路径到hashedKey的映射
+			pipe.Del(ctx, "pathToHash:"+filePath) // 删除与pathToHash:filePath相关的键
+
+			_, err = pipe.Exec(ctx)
+			if err != nil {
+				fmt.Printf("Error deleting keys for outdated record %s: %s\n", hashedKey, err)
+			} else {
+				fmt.Printf("Deleted outdated record: path=%s, size=%d, modTime=%s, hash=%s\n", filePath, fileInfo.Size, fileInfo.ModTime, fileHash)
+			}
+		}
+	}
+	return nil
+}
+
+// getAllFileHashes 从Redis中检索所有文件的哈希值及其对应的路径。
+func getAllFileHashes(rdb *redis.Client, ctx context.Context) (map[string][]string, error) {
+	fileHashes := make(map[string][]string)
+
+	// Scan用于查找所有哈希键
+	iter := rdb.Scan(ctx, 0, "hash:*", 0).Iterator()
+	for iter.Next(ctx) {
+		hashKey := iter.Val()
+
+		// 获取文件内容的哈希值
+		fileContentHash, err := rdb.Get(ctx, hashKey).Result()
+		if err != nil {
+			fmt.Println("Error getting file content hash:", err)
+			continue
+		}
+
+		hashedKey := strings.TrimPrefix(hashKey, "hash:")
+
+		// 获取与hashedKey相关的文件路径
+		filePath, err := rdb.Get(ctx, "path:"+hashedKey).Result()
+		if err != nil {
+			fmt.Println("Error getting file path:", err)
+			continue
+		}
+
+		fileHashes[fileContentHash] = append(fileHashes[fileContentHash], filePath)
+	}
+
+	if err := iter.Err(); err != nil {
+		fmt.Println("Iterator error:", err)
+		return nil, err
+	}
+
+	return fileHashes, nil
+}
diff --git a/redis_path_query.py b/redis_path_query.py
new file mode 100644
--- /dev/null
+++ ./redis_path_query.py
@@ -0,0 +1,49 @@
+import redis
+import hashlib
+import sys
+
+
+def query_redis(path, redis_host="localhost", redis_port=6379, redis_db=0):
+    # 连接到 Redis
+    r = redis.Redis(host=redis_host, port=redis_port, db=redis_db)
+
+    # 生成哈希键
+    hashed_key = generate_hash(path)  # 替换为您的哈希函数
+    file_info = r.get(hashed_key)
+    if file_info:
+        print(f"Data found for {path}: {file_info}")
+
+        # 获取并打印文件哈希值
+        file_hash = r.get("hash:" + hashed_key)
+        if file_hash:
+            print(f"File hash for {path}: {file_hash.decode('utf-8')}")
+        else:
+            print(f"No file hash found for {path}")
+    else:
+        print(f"No data found for {path}")
+
+
+def generate_hash(s):
+    hasher = hashlib.sha256()
+    hasher.update(s.encode())
+    hash_value = hasher.hexdigest()
+    print(f"Generated hash for '{s}': {hash_value}")
+    return hash_value
+
+
+if __name__ == "__main__":
+    query_redis(
+        "/media/mirror2/music/经典老歌/华语群星/郑秀文/郑秀文/【车载CD定制、自选歌曲、自排曲目】郑秀文《2002 我左眼见到鬼电影原声碟》[WAV 分轨]/郑秀文 - 我左眼见到鬼电影原声CD.wav"
+    )
+    query_redis(
+        "/media/av91/av/旬果/Pizzaboy and Hubby Creampied Me Successively.vd.1080.mp4.bak"
+    )
+    query_redis(
+        "/media/av91/av/旬果/Pizzaboy and Hubby Creampied Me Successively.vd.1080.mp4"
+    )
+    query_redis(
+        "/media/secure_bcache/av/onlyfans/OnlyFans.2022.Musclebarbie.Primal.Instincts.Anal.XXX.1080p.HEVC.x265.PRT[XvX]/OnlyFans.2022.Musclebarbie.Primal.Instincts.Anal.XXX.1080p.HEVC.x265.PRT.mkv"
+    )
+    query_redis(
+        "/media/secure_bcache/av/onlyfans/OnlyFans.22.11.10.Lani.Rails.Aka.HotSouthernFreedom.A27hopsonxxx.XXX.720p.HEVC.x265.PRT[XvX]/OnlyFans.22.11.10.Lani.Rails.Aka.HotSouthernFreedom.A27hopsonxxx.XXX.720p.HEVC.x265.PRT.mkv"
+    )
diff --git a/utils.go b/utils.go
new file mode 100644
--- /dev/null
+++ ./utils.go
@@ -0,0 +1,193 @@
+// utils.go
+// 该文件包含用于整个应用程序的通用工具函数。
+
+package main
+
+import (
+	"bufio"
+	"bytes"
+	"context"
+	"encoding/gob"
+	"fmt"
+	"github.com/go-redis/redis/v8"
+	"os"
+	"path/filepath"
+	"regexp"
+	"sort"
+	"strings"
+)
+
+func loadExcludePatterns(filename string) ([]string, error) {
+	file, err := os.Open(filename)
+	if err != nil {
+		return nil, err
+	}
+	defer file.Close()
+
+	var patterns []string
+	scanner := bufio.NewScanner(file)
+	for scanner.Scan() {
+		pattern := scanner.Text()
+		fmt.Printf("Loaded exclude pattern: %s\n", pattern) // 打印每个加载的模式
+		patterns = append(patterns, pattern)
+	}
+	return patterns, scanner.Err()
+}
+
+func sortKeys(keys []string, data map[string]FileInfo, sortByModTime bool) {
+	if sortByModTime {
+		sort.Slice(keys, func(i, j int) bool {
+			return data[keys[i]].ModTime.After(data[keys[j]].ModTime)
+		})
+	} else {
+		sort.Slice(keys, func(i, j int) bool {
+			return data[keys[i]].Size > data[keys[j]].Size
+		})
+	}
+}
+
+func compileExcludePatterns(filename string) ([]*regexp.Regexp, error) {
+	excludePatterns, err := loadExcludePatterns(filename)
+	if err != nil {
+		return nil, err
+	}
+
+	excludeRegexps := make([]*regexp.Regexp, len(excludePatterns))
+	for i, pattern := range excludePatterns {
+		regexPattern := strings.Replace(pattern, "*", ".*", -1)
+		excludeRegexps[i], err = regexp.Compile(regexPattern)
+		if err != nil {
+			return nil, fmt.Errorf("Invalid regex pattern '%s': %v", regexPattern, err)
+		}
+	}
+	return excludeRegexps, nil
+}
+
+func performSaveOperation(rootDir, filename string, sortByModTime bool, rdb *redis.Client, ctx context.Context) {
+	if err := saveToFile(rootDir, filename, sortByModTime, rdb, ctx); err != nil {
+		fmt.Printf("Error saving to %s: %s\n", filepath.Join(rootDir, filename), err)
+	} else {
+		fmt.Printf("Saved data to %s\n", filepath.Join(rootDir, filename))
+	}
+}
+
+func writeLinesToFile(filename string, lines []string) error {
+	file, err := os.Create(filename)
+	if err != nil {
+		return err
+	}
+	defer file.Close()
+
+	for _, line := range lines {
+		if _, err := fmt.Fprintln(file, line); err != nil {
+			return err
+		}
+	}
+	return nil
+}
+
+func findAndLogDuplicates(rootDir string, outputFile string, rdb *redis.Client, ctx context.Context) error {
+	fmt.Println("Starting to find duplicates...")
+
+	hashes, err := getAllFileHashes(rdb, ctx)
+	if err != nil {
+		fmt.Printf("Error getting file hashes: %s\n", err)
+		return err
+	}
+
+	var lines []string
+	for _, paths := range hashes {
+		if len(paths) > 1 {
+			for _, path := range paths {
+				relPath, err := filepath.Rel(rootDir, path)
+				if err != nil {
+					fmt.Printf("Error converting to relative path: %s\n", err)
+					continue
+				}
+				lines = append(lines, fmt.Sprintf("\"./%s\"", relPath))
+			}
+		}
+	}
+
+	if len(lines) == 0 {
+		fmt.Println("No duplicates found.")
+		return nil
+	}
+
+	outputFile = filepath.Join(rootDir, outputFile)
+	err = writeLinesToFile(outputFile, lines)
+	if err != nil {
+		fmt.Printf("Error writing to file %s: %s\n", outputFile, err)
+		return err
+	}
+
+	fmt.Printf("Duplicates written to %s\n", outputFile)
+	return nil
+}
+
+func getFileSizeFromRedis(rdb *redis.Client, ctx context.Context, fullPath string) (int64, error) {
+	// 首先从 fullPath 获取 hashedKey
+	hashedKey, err := getHashedKeyFromPath(rdb, ctx, fullPath)
+	if err != nil {
+		return 0, fmt.Errorf("error getting hashed key for %s: %w", fullPath, err)
+	}
+
+	// 然后使用 hashedKey 从 Redis 获取文件信息
+	fileInfoData, err := rdb.Get(ctx, hashedKey).Bytes()
+	if err != nil {
+		return 0, fmt.Errorf("error getting file info for hashed key %s: %w", hashedKey, err)
+	}
+
+	var fileInfo FileInfo
+	buf := bytes.NewBuffer(fileInfoData)
+	dec := gob.NewDecoder(buf)
+	if err := dec.Decode(&fileInfo); err != nil {
+		return 0, fmt.Errorf("error decoding file info for hashed key %s: %w", hashedKey, err)
+	}
+
+	return fileInfo.Size, nil
+}
+
+func getHashedKeyFromPath(rdb *redis.Client, ctx context.Context, path string) (string, error) {
+	return rdb.Get(ctx, "pathToHash:"+path).Result()
+}
+
+// extractFileName extracts the file name from a given file path.
+func extractFileName(filePath string) string {
+	return strings.ToLower(filepath.Base(filePath))
+}
+
+// extractKeywords extracts keywords from a slice of file names.
+func extractKeywords(fileNames []string) []string {
+	keywords := make(map[string]struct{})
+	pattern := regexp.MustCompile(`\b(?:\d{2}\.\d{2}\.\d{2}|(?:\d+|[a-z]+(?:\d+[a-z]*)?))\b`)
+
+	for _, fileName := range fileNames {
+		nameWithoutExt := strings.TrimSuffix(fileName, filepath.Ext(fileName))
+		matches := pattern.FindAllString(nameWithoutExt, -1)
+		for _, match := range matches {
+			keywords[match] = struct{}{}
+		}
+	}
+
+	var keywordList []string
+	for keyword := range keywords {
+		keywordList = append(keywordList, keyword)
+	}
+
+	return keywordList
+}
+
+func findCloseFiles(fileNames, filePaths, keywords []string) map[string][]string {
+	closeFiles := make(map[string][]string)
+
+	for _, kw := range keywords {
+		for i, fileName := range fileNames {
+			if strings.Contains(strings.ToLower(fileName), strings.ToLower(kw)) {
+				closeFiles[kw] = append(closeFiles[kw], filePaths[i])
+			}
+		}
+	}
+
+	return closeFiles
+}
diff --git a/worker_pool.go b/worker_pool.go
new file mode 100644
--- /dev/null
+++ ./worker_pool.go
@@ -0,0 +1,26 @@
+package main
+
+import (
+	"sync"
+)
+
+// Task 定义了工作池中的任务类型
+type Task func()
+
+// NewWorkerPool 创建并返回一个工作池
+func NewWorkerPool(workerCount int) (chan<- Task, *sync.WaitGroup) {
+	var wg sync.WaitGroup
+	taskQueue := make(chan Task)
+
+	for i := 0; i < workerCount; i++ {
+		wg.Add(1)
+		go func() {
+			defer wg.Done()
+			for task := range taskQueue {
+				task()
+			}
+		}()
+	}
+
+	return taskQueue, &wg
+}
