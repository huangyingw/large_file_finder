diff --git a/data_serialization.go b/data_serialization.go
new file mode 100644
--- /dev/null
+++ ./data_serialization.go
@@ -0,0 +1,3 @@
+package main
+
+import ()
diff --git a/file_processing.go b/file_processing.go
new file mode 100644
--- /dev/null
+++ ./file_processing.go
@@ -0,0 +1,361 @@
+// file_processing.go
+package main
+
+import (
+	"bytes"
+	"context"
+	"crypto/sha512"
+	"encoding/gob"
+	"fmt"
+	"github.com/go-redis/redis/v8"
+	"io"
+	"log"
+	"os"
+	"path/filepath"
+	"regexp"
+	"sort"
+	"strings"
+	"time"
+)
+
+const (
+	ReadLimit       = 100 * 1024 // 100KB
+	FullFileReadCmd = -1
+)
+
+type FileInfo struct {
+	Size    int64
+	ModTime time.Time
+}
+
+type FileProcessor struct {
+	Rdb                   *redis.Client
+	Ctx                   context.Context
+	generateHashFunc      func(string) string                            // 新增字段
+	calculateFileHashFunc func(path string, limit int64) (string, error) // 重命名字段
+}
+
+func NewFileProcessor(Rdb *redis.Client, Ctx context.Context) *FileProcessor {
+	return &FileProcessor{
+		Rdb:              Rdb,
+		Ctx:              Ctx,
+		generateHashFunc: generateHash, // 使用默认的 generateHash 函数
+	}
+}
+
+var TimestampRegex = regexp.MustCompile(`(\d{1,2}:\d{2}(?::\d{2})?)`)
+
+func ExtractTimestamp(filePath string) string {
+	match := TimestampRegex.FindStringSubmatch(filePath)
+	if len(match) > 1 {
+		return match[1]
+	}
+	return ""
+}
+
+func CalculateScore(timestamp string, fileNameLength int) float64 {
+	timestampLength := len(timestamp)
+	return float64(-(timestampLength*1000 + fileNameLength))
+}
+
+func (fp *FileProcessor) SaveDuplicateFileInfoToRedis(fullHash string, info FileInfo, filePath string) error {
+	timestamp := ExtractTimestamp(filePath)
+	fileNameLength := len(filepath.Base(filePath))
+	score := CalculateScore(timestamp, fileNameLength)
+
+	_, err := fp.Rdb.ZAdd(fp.Ctx, "duplicateFiles:"+fullHash, &redis.Z{
+		Score:  score,
+		Member: filePath,
+	}).Result()
+
+	if err != nil {
+		return fmt.Errorf("error adding duplicate file to Redis: %w", err)
+	}
+
+	return nil
+}
+
+// getFileInfoFromRedis retrieves file info from Redis
+func (fp *FileProcessor) getFileInfoFromRedis(hashedKey string) (FileInfo, error) {
+	var fileInfo FileInfo
+	value, err := fp.Rdb.Get(fp.Ctx, "fileInfo:"+hashedKey).Bytes()
+	if err != nil {
+		return fileInfo, fmt.Errorf("error getting file info from Redis: %w", err)
+	}
+
+	buf := bytes.NewBuffer(value)
+	dec := gob.NewDecoder(buf)
+	err = dec.Decode(&fileInfo)
+	if err != nil {
+		return fileInfo, fmt.Errorf("error decoding file info: %w", err)
+	}
+	return fileInfo, nil
+}
+
+// saveToFile saves file information to a file
+func (fp *FileProcessor) saveToFile(dir, filename string, sortByModTime bool) error {
+	iter := fp.Rdb.Scan(fp.Ctx, 0, "fileInfo:*", 0).Iterator()
+	data := make(map[string]FileInfo)
+
+	for iter.Next(fp.Ctx) {
+		hashedKey := strings.TrimPrefix(iter.Val(), "fileInfo:")
+
+		originalPath, err := fp.Rdb.Get(fp.Ctx, "hashedKeyToPath:"+hashedKey).Result()
+		if err != nil {
+			log.Printf("Error getting original path for key %s: %v", hashedKey, err)
+			continue
+		}
+
+		fileInfo, err := fp.getFileInfoFromRedis(hashedKey)
+		if err != nil {
+			log.Printf("Error getting file info for key %s: %v", hashedKey, err)
+			continue
+		}
+
+		data[originalPath] = fileInfo
+	}
+
+	if err := iter.Err(); err != nil {
+		return fmt.Errorf("error iterating over Redis keys: %w", err)
+	}
+
+	if len(data) == 0 {
+		return nil
+	}
+
+	return fp.writeDataToFile(dir, filename, data, sortByModTime)
+}
+
+// writeDataToFile writes the processed data to a file
+func (fp *FileProcessor) writeDataToFile(dir, filename string, data map[string]FileInfo, sortByModTime bool) error {
+	var lines []string
+	keys := make([]string, 0, len(data))
+	for k := range data {
+		keys = append(keys, k)
+	}
+
+	sortKeys(keys, data, sortByModTime)
+
+	for _, k := range keys {
+		relativePath, err := filepath.Rel(dir, k)
+		if err != nil {
+			log.Printf("Error getting relative path for %s: %v", k, err)
+			continue
+		}
+		line := formatFileInfoLine(data[k], relativePath, sortByModTime)
+		lines = append(lines, line)
+	}
+
+	return writeLinesToFile(filepath.Join(dir, filename), lines)
+}
+
+// ProcessFile processes a single file
+func (fp *FileProcessor) ProcessFile(path string) error {
+	info, err := os.Stat(path)
+	if err != nil {
+		return fmt.Errorf("error getting file info: %w", err)
+	}
+
+	fileHash, err := fp.calculateFileHashFunc(path, ReadLimit) // 使用新的字段名
+	if err != nil {
+		return fmt.Errorf("error calculating file hash: %w", err)
+	}
+
+	fullHash, err := fp.calculateFileHashFunc(path, FullFileReadCmd) // 使用新的字段名
+	if err != nil {
+		return fmt.Errorf("error calculating full file hash: %w", err)
+	}
+
+	fileInfo := FileInfo{
+		Size:    info.Size(),
+		ModTime: info.ModTime(),
+	}
+
+	err = fp.saveFileInfoToRedis(path, fileInfo, fileHash, fullHash)
+	if err != nil {
+		return fmt.Errorf("error saving file info to Redis: %w", err)
+	}
+
+	return nil
+}
+
+// calculateFileHash calculates the hash of a file
+func (fp *FileProcessor) calculateFileHash(path string, limit int64) (string, error) {
+	f, err := os.Open(path)
+	if err != nil {
+		return "", fmt.Errorf("error opening file: %w", err)
+	}
+	defer f.Close()
+
+	h := sha512.New()
+	if limit == FullFileReadCmd {
+		if _, err := io.Copy(h, f); err != nil {
+			return "", fmt.Errorf("error reading full file: %w", err)
+		}
+	} else {
+		if _, err := io.CopyN(h, f, limit); err != nil && err != io.EOF {
+			return "", fmt.Errorf("error reading file: %w", err)
+		}
+	}
+
+	return fmt.Sprintf("%x", h.Sum(nil)), nil
+}
+
+// saveFileInfoToRedis saves file info to Redis
+func (fp *FileProcessor) saveFileInfoToRedis(path string, info FileInfo, fileHash, fullHash string) error {
+	hashedKey := fp.generateHashFunc(path)
+
+	var buf bytes.Buffer
+	enc := gob.NewEncoder(&buf)
+	if err := enc.Encode(info); err != nil {
+		return fmt.Errorf("error encoding file info: %w", err)
+	}
+
+	pipe := fp.Rdb.Pipeline()
+	pipe.SetNX(fp.Ctx, "fileInfo:"+hashedKey, buf.Bytes(), 0)
+	pipe.Set(fp.Ctx, "hashedKeyToPath:"+hashedKey, path, 0)
+	pipe.SAdd(fp.Ctx, "fileHashToPathSet:"+fileHash, path)
+	pipe.Set(fp.Ctx, "hashedKeyToFullHash:"+hashedKey, fullHash, 0)
+	pipe.Set(fp.Ctx, "pathToHashedKey:"+path, hashedKey, 0)
+	pipe.Set(fp.Ctx, "hashedKeyToFileHash:"+hashedKey, fileHash, 0)
+
+	_, err := pipe.Exec(fp.Ctx)
+	if err != nil {
+		return fmt.Errorf("error executing Redis pipeline: %w", err)
+	}
+	return nil
+}
+
+const readLimit = 100 * 1024 // 100KB
+
+// 处理目录
+func processDirectory(path string) {
+}
+
+// 处理符号链接
+func processSymlink(path string) {
+}
+
+// 处理关键词
+func processKeyword(keyword string, keywordFiles []string, Rdb *redis.Client, Ctx context.Context, rootDir string) {
+	// 对 keywordFiles 进行排序
+	sort.Slice(keywordFiles, func(i, j int) bool {
+		sizeI, _ := getFileSize(Rdb, Ctx, filepath.Join(rootDir, cleanPath(keywordFiles[i])))
+		sizeJ, _ := getFileSize(Rdb, Ctx, filepath.Join(rootDir, cleanPath(keywordFiles[j])))
+		return sizeI > sizeJ
+	})
+
+	// 准备要写入的数据
+	var outputData strings.Builder
+	outputData.WriteString(keyword + "\n")
+	for _, filePath := range keywordFiles {
+		fileSize, _ := getFileSize(Rdb, Ctx, filepath.Join(rootDir, cleanPath(filePath)))
+		outputData.WriteString(fmt.Sprintf("%d,%s\n", fileSize, filePath))
+	}
+
+	// 创建并写入文件
+	outputFilePath := filepath.Join(rootDir, keyword+".txt")
+	outputFile, err := os.Create(outputFilePath)
+	if err != nil {
+		return
+	}
+	defer outputFile.Close()
+
+	_, err = outputFile.WriteString(outputData.String())
+	if err != nil {
+	}
+}
+
+// 清理和标准化路径
+func cleanPath(path string) string {
+	path = strings.Trim(path, `"`)
+	if strings.HasPrefix(path, "./") {
+		path = strings.TrimPrefix(path, "./")
+	}
+	return path
+}
+
+// 获取文件大小
+func getFileSize(Rdb *redis.Client, Ctx context.Context, fullPath string) (int64, error) {
+	size, err := getFileSizeFromRedis(Rdb, Ctx, fullPath)
+	if err != nil {
+		return 0, err
+	}
+	return size, nil
+}
+
+// 获取文件哈希值
+func getHash(path string, Rdb *redis.Client, Ctx context.Context, keyPrefix string, limit int64) (string, error) {
+	hashedKey := generateHash(path) // 使用全局的 generateHash 函数
+	hashKey := keyPrefix + hashedKey
+
+	// 尝试从Redis获取哈希值
+	hash, err := Rdb.Get(Ctx, hashKey).Result()
+	if err == nil && hash != "" {
+		return hash, nil
+	}
+
+	if err != nil && err != redis.Nil {
+		return "", err
+	}
+
+	// 计算哈希值
+	file, err := os.Open(path)
+	if err != nil {
+		return "", err
+	}
+	defer file.Close()
+
+	hasher := sha512.New()
+
+	if limit > 0 {
+		// 只读取前 limit 字节
+		reader := io.LimitReader(file, limit)
+		if _, err := io.Copy(hasher, reader); err != nil {
+			return "", err
+		}
+	} else {
+		// 读取整个文件
+		buf := make([]byte, 4*1024*1024) // 每次读取 4MB 数据
+		for {
+			n, err := file.Read(buf)
+			if err != nil && err != io.EOF {
+				return "", err
+			}
+			if n == 0 {
+				break
+			}
+			if _, err := hasher.Write(buf[:n]); err != nil {
+				return "", err
+			}
+		}
+	}
+
+	hashBytes := hasher.Sum(nil)
+	hash = fmt.Sprintf("%x", hashBytes)
+
+	// 将计算出的哈希值保存到Redis
+	err = Rdb.Set(Ctx, hashKey, hash, 0).Err()
+	if err != nil {
+		return "", err
+	}
+	return hash, nil
+}
+
+// 获取文件哈希
+func getFileHash(path string, Rdb *redis.Client, Ctx context.Context) (string, error) {
+	const readLimit = 100 * 1024 // 100KB
+	return getHash(path, Rdb, Ctx, "hashedKeyToFileHash:", readLimit)
+}
+
+// 获取完整文件哈希
+func getFullFileHash(path string, Rdb *redis.Client, Ctx context.Context) (string, error) {
+	const noLimit = -1 // No limit for full file hash
+	hash, err := getHash(path, Rdb, Ctx, "hashedKeyToFullHash:", noLimit)
+	if err != nil {
+		log.Printf("Error calculating full hash for file %s: %v", path, err)
+	} else {
+		log.Printf("Full hash for file %s: %s", path, hash)
+	}
+	return hash, err
+}
diff --git a/file_processing_test.go b/file_processing_test.go
new file mode 100644
--- /dev/null
+++ ./file_processing_test.go
@@ -0,0 +1,155 @@
+// file_processing_test.go
+package main
+
+import (
+	"context"
+	"github.com/alicebob/miniredis/v2"
+	"github.com/go-redis/redis/v8"
+	"github.com/go-redis/redismock/v8"
+	"github.com/stretchr/testify/assert"
+	"io/ioutil"
+	"os"
+	"testing"
+	"time"
+)
+
+func TestProcessFile(t *testing.T) {
+	// 创建一个临时文件
+	tmpfile, err := ioutil.TempFile("", "example")
+	if err != nil {
+		t.Fatal(err)
+	}
+	defer os.Remove(tmpfile.Name()) // 清理
+
+	// 写入一些数据
+	if _, err := tmpfile.Write([]byte("hello world")); err != nil {
+		t.Fatal(err)
+	}
+	if err := tmpfile.Close(); err != nil {
+		t.Fatal(err)
+	}
+
+	// 创建一个 miniredis 实例
+	mr, err := miniredis.Run()
+	if err != nil {
+		t.Fatal(err)
+	}
+	defer mr.Close()
+
+	// 创建一个真实的 Redis 客户端，连接到 miniredis
+	rdb := redis.NewClient(&redis.Options{
+		Addr: mr.Addr(),
+	})
+	ctx := context.Background()
+
+	// 创建一个 FileProcessor
+	fp := NewFileProcessor(rdb, ctx)
+
+	// 保存原始的 generateHash 函数
+	originalGenerateHash := fp.generateHashFunc
+
+	// 创建一个新的 generateHash 函数
+	fp.generateHashFunc = func(s string) string {
+		return "mockedHash"
+	}
+
+	// 在测试结束时恢复原始函数
+	defer func() { fp.generateHashFunc = originalGenerateHash }()
+
+	// 模拟 calculateFileHash 方法
+	calculateFileHashCalls := 0
+	fp.calculateFileHashFunc = func(path string, limit int64) (string, error) {
+		calculateFileHashCalls++
+		if calculateFileHashCalls == 1 {
+			return "file_hash", nil
+		}
+		return "full_hash", nil
+	}
+
+	// 处理文件
+	err = fp.ProcessFile(tmpfile.Name())
+	if err != nil {
+		t.Errorf("ProcessFile() error = %v", err)
+	}
+
+	// 验证 Redis 中的数据
+	fileInfoData, err := rdb.Get(ctx, "fileInfo:mockedHash").Bytes()
+	assert.NoError(t, err)
+
+	var storedFileInfo FileInfo
+	err = decodeGob(fileInfoData, &storedFileInfo)
+	assert.NoError(t, err)
+
+	assert.Equal(t, int64(11), storedFileInfo.Size) // "hello world" 的长度
+
+	// 验证其他 Redis 键
+	assert.Equal(t, tmpfile.Name(), rdb.Get(ctx, "hashedKeyToPath:mockedHash").Val())
+	assert.True(t, rdb.SIsMember(ctx, "fileHashToPathSet:file_hash", tmpfile.Name()).Val())
+	assert.Equal(t, "full_hash", rdb.Get(ctx, "hashedKeyToFullHash:mockedHash").Val())
+	assert.Equal(t, "mockedHash", rdb.Get(ctx, "pathToHashedKey:"+tmpfile.Name()).Val())
+	assert.Equal(t, "file_hash", rdb.Get(ctx, "hashedKeyToFileHash:mockedHash").Val())
+
+	// 验证 calculateFileHash 被调用了两次
+	assert.Equal(t, 2, calculateFileHashCalls)
+}
+
+func TestExtractTimestamp(t *testing.T) {
+	tests := []struct {
+		name     string
+		filePath string
+		want     string
+	}{
+		{"With timestamp", "/path/to/file_12:34:56.txt", "12:34:56"},
+		{"Without timestamp", "/path/to/file.txt", ""},
+		{"With date", "/path/2021-05-01/file.txt", ""},
+	}
+
+	for _, tt := range tests {
+		t.Run(tt.name, func(t *testing.T) {
+			got := ExtractTimestamp(tt.filePath)
+			assert.Equal(t, tt.want, got)
+		})
+	}
+}
+
+func TestCalculateScore(t *testing.T) {
+	tests := []struct {
+		name           string
+		timestamp      string
+		fileNameLength int
+		want           float64
+	}{
+		{"With timestamp", "12:34:56", 10, -8010},
+		{"Without timestamp", "", 10, -10},
+		{"Short timestamp", "12:34", 5, -5005},
+	}
+
+	for _, tt := range tests {
+		t.Run(tt.name, func(t *testing.T) {
+			got := CalculateScore(tt.timestamp, tt.fileNameLength)
+			assert.Equal(t, tt.want, got)
+		})
+	}
+}
+
+func TestSaveDuplicateFileInfoToRedis(t *testing.T) {
+	rdb, mock := redismock.NewClientMock()
+	ctx := context.Background()
+	fp := NewFileProcessor(rdb, ctx)
+
+	fullHash := "testhash"
+	info := FileInfo{Size: 1000, ModTime: time.Unix(1620000000, 0)}
+	filePath := "/path/to/file_12:34:56.txt"
+
+	mock.ExpectZAdd("duplicateFiles:"+fullHash, &redis.Z{
+		Score:  -8017, // 更新期望的分数
+		Member: filePath,
+	}).SetVal(1)
+
+	err := fp.SaveDuplicateFileInfoToRedis(fullHash, info, filePath)
+	assert.NoError(t, err)
+
+	assert.NoError(t, mock.ExpectationsWereMet())
+}
+
+// Add more tests for other functions...
diff --git a/main.go b/main.go
new file mode 100644
--- /dev/null
+++ ./main.go
@@ -0,0 +1,258 @@
+// main.go
+// 此文件是Go程序的主要入口点，包含了文件处理程序的核心逻辑和初始化代码。
+
+package main
+
+import (
+	"bufio"
+	"context"
+	"flag"
+	"fmt"
+	"github.com/go-redis/redis/v8"
+	"github.com/karrick/godirwalk"
+	"log"
+	"os"
+	"path/filepath"
+	"regexp"
+	"runtime"
+	"sort"
+	"sync"
+	"time"
+)
+
+var (
+	rootDir          string
+	redisAddr        string
+	workerCount      int
+	minSizeBytes     int64
+	deleteDuplicates bool
+	findDuplicates   bool
+	outputDuplicates bool
+	maxDuplicates    int
+	semaphore        chan struct{}
+)
+
+func init() {
+	flag.StringVar(&rootDir, "rootDir", "", "Root directory to start the search")
+	flag.StringVar(&redisAddr, "redisAddr", "localhost:6379", "Redis server address")
+	flag.IntVar(&workerCount, "workers", runtime.NumCPU(), "Number of worker goroutines")
+	flag.Int64Var(&minSizeBytes, "minSize", 200*1024*1024, "Minimum file size in bytes")
+	flag.BoolVar(&deleteDuplicates, "delete-duplicates", false, "Delete duplicate files")
+	flag.BoolVar(&findDuplicates, "find-duplicates", false, "Find duplicate files")
+	flag.BoolVar(&outputDuplicates, "output-duplicates", false, "Output duplicate files")
+	flag.IntVar(&maxDuplicates, "max-duplicates", 50, "Maximum number of duplicates to process")
+}
+
+func main() {
+	flag.Parse()
+
+	if rootDir == "" {
+		log.Fatal("rootDir must be specified")
+	}
+
+	ctx, cancel := context.WithCancel(context.Background())
+	defer cancel()
+
+	rdb := redis.NewClient(&redis.Options{
+		Addr: redisAddr,
+	})
+	defer rdb.Close()
+
+	semaphore = make(chan struct{}, runtime.NumCPU())
+
+	fp := NewFileProcessor(rdb, ctx)
+
+	if err := cleanUpOldRecords(rdb, ctx); err != nil {
+		log.Printf("Error cleaning up old records: %v", err)
+	}
+
+	if findDuplicates {
+		if err := findAndLogDuplicates(rootDir, rdb, ctx, maxDuplicates); err != nil {
+			log.Fatalf("Error finding duplicates: %v", err)
+		}
+		return
+	}
+
+	if outputDuplicates {
+		if err := writeDuplicateFilesToFile(rootDir, "fav.log.dup", rdb, ctx); err != nil {
+			log.Fatalf("Error writing duplicates to file: %v", err)
+		}
+		return
+	}
+
+	if deleteDuplicates {
+		if err := deleteDuplicateFiles(rootDir, rdb, ctx); err != nil {
+			log.Fatalf("Error deleting duplicate files: %v", err)
+		}
+		return
+	}
+
+	fileChan := make(chan string, workerCount)
+	var wg sync.WaitGroup
+
+	// Start worker goroutines
+	for i := 0; i < workerCount; i++ {
+		wg.Add(1)
+		go func() {
+			defer wg.Done()
+			for filePath := range fileChan {
+				if err := fp.ProcessFile(filePath); err != nil {
+					log.Printf("Error processing file %s: %v", filePath, err)
+				}
+			}
+		}()
+	}
+
+	// Start progress monitoring
+	go monitorProgress(ctx)
+
+	// Walk through files
+	err := walkFiles(rootDir, minSizeBytes, fileChan)
+	close(fileChan)
+	if err != nil {
+		log.Printf("Error walking files: %v", err)
+	}
+
+	wg.Wait()
+
+	// Save results
+	if err := fp.saveToFile(rootDir, "fav.log", false); err != nil {
+		log.Printf("Error saving to fav.log: %v", err)
+	}
+	if err := fp.saveToFile(rootDir, "fav.log.sort", true); err != nil {
+		log.Printf("Error saving to fav.log.sort: %v", err)
+	}
+
+	log.Println("Processing complete")
+}
+
+func processFavLog(filePath string, rootDir string, rdb *redis.Client, ctx context.Context) {
+	file, err := os.Open(filePath)
+	if err != nil {
+		log.Println("Error opening file:", err)
+		return
+	}
+	defer file.Close()
+
+	var fileNames, filePaths []string
+	scanner := bufio.NewScanner(file)
+	for scanner.Scan() {
+		line := scanner.Text()
+		line = regexp.MustCompile(`^\d+,`).ReplaceAllString(line, "")
+		filePaths = append(filePaths, line)
+		fileNames = append(fileNames, extractFileName(line))
+	}
+
+	// 确定工作池的大小并调用 extractKeywords
+	var stopProcessing bool
+	keywords := extractKeywords(fileNames, &stopProcessing)
+
+	closeFiles := findCloseFiles(fileNames, filePaths, keywords)
+
+	// 排序关键词
+	sort.Slice(keywords, func(i, j int) bool {
+		return len(closeFiles[keywords[i]]) > len(closeFiles[keywords[j]])
+	})
+
+	workerCount := 500
+	taskQueue, poolWg, stopFunc, _ := NewWorkerPool(workerCount, &stopProcessing)
+
+	for i, keyword := range keywords {
+		keywordFiles := closeFiles[keyword]
+		if len(keywordFiles) >= 2 {
+			poolWg.Add(1) // 在将任务发送到队列之前增加计数
+			taskQueue <- func(kw string, kf []string, idx int) Task {
+				return func() {
+					defer poolWg.Done()
+					log.Printf("Processing keyword %d of %d: %s\n", idx+1, len(keywords), kw)
+					processKeyword(kw, kf, rdb, ctx, rootDir)
+				}
+			}(keyword, keywordFiles, i)
+		}
+	}
+
+	stopFunc() // 使用停止函数来关闭任务队列
+	poolWg.Wait()
+}
+
+// 初始化Redis客户端
+func newRedisClient(ctx context.Context) *redis.Client {
+	rdb := redis.NewClient(&redis.Options{
+		Addr: "localhost:6379", // 该地址应从配置中获取
+	})
+	_, err := rdb.Ping(ctx).Result()
+	if err != nil {
+		log.Println("Error connecting to Redis:", err)
+		os.Exit(1)
+	}
+	return rdb
+}
+
+func initializeApp() (string, int64, []*regexp.Regexp, *redis.Client, context.Context, bool, bool, bool, int, error) {
+	rootDir := flag.String("rootDir", "", "Root directory to start the search")
+	deleteDuplicates := flag.Bool("delete-duplicates", false, "Delete duplicate files")
+	findDuplicates := flag.Bool("find-duplicates", false, "Find duplicate files")
+	outputDuplicates := flag.Bool("output-duplicates", false, "Output duplicate files")
+	maxDuplicates := flag.Int("max-duplicates", 50, "Maximum number of duplicates to process")
+	flag.Parse()
+
+	if *rootDir == "" {
+		return "", 0, nil, nil, nil, false, false, false, 0, fmt.Errorf("rootDir must be specified")
+	}
+
+	// Minimum file size in bytes
+	minSize := 200 // Default size is 200MB
+	minSizeBytes := int64(minSize * 1024 * 1024)
+
+	// 获取当前运行目录
+	currentDir, err := os.Getwd()
+	if err != nil {
+		log.Fatalf("Failed to get current directory: %v", err)
+	}
+
+	// 拼接当前目录和文件名
+	excludePatternsFilePath := filepath.Join(currentDir, "exclude_patterns.txt")
+
+	excludeRegexps, _ := compileExcludePatterns(excludePatternsFilePath)
+
+	// 创建 Redis 客户端
+	ctx := context.Background()
+	rdb := newRedisClient(ctx)
+
+	return *rootDir, minSizeBytes, excludeRegexps, rdb, ctx, *deleteDuplicates, *findDuplicates, *outputDuplicates, *maxDuplicates, nil
+}
+
+// walkFiles 遍历指定目录下的文件，并根据条件进行处理
+func walkFiles(rootDir string, minSizeBytes int64, fileChan chan<- string) error {
+	return godirwalk.Walk(rootDir, &godirwalk.Options{
+		Callback: func(osPathname string, de *godirwalk.Dirent) error {
+			if de.IsDir() {
+				return nil
+			}
+			info, err := os.Stat(osPathname)
+			if err != nil {
+				return fmt.Errorf("error getting file info for %s: %w", osPathname, err)
+			}
+			if info.Size() >= minSizeBytes {
+				fileChan <- osPathname
+			}
+			return nil
+		},
+		Unsorted: true,
+	})
+}
+
+func monitorProgress(ctx context.Context) {
+	ticker := time.NewTicker(5 * time.Second)
+	defer ticker.Stop()
+
+	for {
+		select {
+		case <-ctx.Done():
+			return
+		case <-ticker.C:
+			// You might want to implement a way to track progress
+			log.Println("Processing files...")
+		}
+	}
+}
diff --git a/redis_client.go b/redis_client.go
new file mode 100644
--- /dev/null
+++ ./redis_client.go
@@ -0,0 +1,172 @@
+// redis_client.go
+package main
+
+import (
+	"bytes"
+	"context"
+	"crypto/sha256"
+	"encoding/hex"
+	"fmt"
+	"github.com/go-redis/redis/v8"
+	"log"
+	"os"
+	"path/filepath" // 添加导入
+	"regexp"
+	"strings"
+)
+
+// Generate a SHA-256 hash for the given string
+func generateHash(s string) string {
+	hasher := sha256.New()
+	hasher.Write([]byte(s))
+	hash := hex.EncodeToString(hasher.Sum(nil))
+	return hash
+}
+
+// 使用给定的正则表达式匹配时间戳
+func extractTimestamp(filePath string) string {
+	// 匹配冒号或逗号后的 MM:SS 或 H:MM:SS 格式的时间戳
+	re := regexp.MustCompile(`[:,/](\d{1,2}:\d{2}(?::\d{2})?)`)
+	match := re.FindStringSubmatch(filePath)
+	if len(match) > 1 {
+		return match[1]
+	}
+	return ""
+}
+
+// 将重复文件的信息存储到 Redis
+func saveDuplicateFileInfoToRedis(rdb *redis.Client, ctx context.Context, fullHash string, info fileInfo) error {
+	// 提取文件路径中的时间戳
+	timestamp := extractTimestamp(info.path)
+	timestampLength := len(timestamp)
+	fileNameLength := len(filepath.Base(info.path))
+
+	// 计算综合分数
+	// 使用负值排序，确保时间戳长度优先，文件名长度其次
+	combinedScore := float64(-(timestampLength*1000000 + fileNameLength))
+
+	// 使用管道批量处理 Redis 命令
+	pipe := rdb.Pipeline()
+
+	// 将路径添加到有序集合 duplicateFiles:<fullHash> 中，并使用综合分数
+	pipe.ZAdd(ctx, "duplicateFiles:"+fullHash, &redis.Z{
+		Score:  combinedScore, // 综合分数
+		Member: info.path,
+	})
+
+	if _, err := pipe.Exec(ctx); err != nil {
+		return fmt.Errorf("error executing pipeline for duplicate file: %s: %w", info.path, err)
+	}
+
+	return nil
+}
+
+func saveFileInfoToRedis(rdb *redis.Client, ctx context.Context, hashedKey string, path string, buf bytes.Buffer, fileHash string, fullHash string) error {
+	// 规范化路径
+	normalizedPath := filepath.Clean(path)
+
+	// 使用管道批量处理Redis命令
+	pipe := rdb.Pipeline()
+
+	// 这里我们添加命令到管道，但不立即检查错误
+	pipe.Set(ctx, "fileInfo:"+hashedKey, buf.Bytes(), 0)
+	pipe.Set(ctx, "hashedKeyToPath:"+hashedKey, normalizedPath, 0)
+	pipe.SAdd(ctx, "fileHashToPathSet:"+fileHash, normalizedPath) // 将文件路径存储为集合
+	if fullHash != "" {
+		pipe.Set(ctx, "hashedKeyToFullHash:"+hashedKey, fullHash, 0) // 存储完整文件哈希值
+	}
+	// 存储从路径到hashedKey的映射
+	pipe.Set(ctx, "pathToHashedKey:"+normalizedPath, hashedKey, 0)
+	// 存储hashedKey到fileHash的映射
+	pipe.Set(ctx, "hashedKeyToFileHash:"+hashedKey, fileHash, 0)
+
+	if _, err := pipe.Exec(ctx); err != nil {
+		return fmt.Errorf("error executing pipeline for file: %s: %w", path, err)
+	}
+
+	// 添加日志
+	return nil
+}
+
+func cleanUpOldRecords(rdb *redis.Client, ctx context.Context) error {
+	log.Println("Starting to clean up old records")
+	iter := rdb.Scan(ctx, 0, "pathToHashedKey:*", 0).Iterator()
+	for iter.Next(ctx) {
+		pathToHashKey := iter.Val()
+
+		// 解析出文件路径
+		filePath := strings.TrimPrefix(pathToHashKey, "pathToHashedKey:")
+
+		// 检查文件是否存在
+		if _, err := os.Stat(filePath); os.IsNotExist(err) {
+			err := cleanUpRecordsByFilePath(rdb, ctx, filePath)
+			if err != nil {
+				log.Printf("Error cleaning up records for file %s: %s\n", filePath, err)
+			}
+		}
+	}
+
+	if err := iter.Err(); err != nil {
+		log.Printf("Error during iteration: %s\n", err)
+		return err
+	}
+
+	return nil
+}
+
+func cleanUpRecordsByFilePath(rdb *redis.Client, ctx context.Context, filePath string) error {
+	// 获取 hashedKey
+	hashedKey, err := rdb.Get(ctx, "pathToHashedKey:"+filePath).Result()
+	if err != nil && err != redis.Nil {
+		return fmt.Errorf("error retrieving hashedKey for path %s: %v", filePath, err)
+	}
+
+	// 获取 fileHash
+	fileHash, err := rdb.Get(ctx, "hashedKeyToFileHash:"+hashedKey).Result()
+	if err != nil {
+		return fmt.Errorf("error retrieving fileHash for key %s: %v", hashedKey, err)
+	}
+
+	// 获取 fullHash
+	fullHash, err := rdb.Get(ctx, "hashedKeyToFullHash:"+hashedKey).Result()
+	if err != nil && err != redis.Nil {
+		return fmt.Errorf("error retrieving fullHash for key %s: %v", hashedKey, err)
+	}
+
+	// 删除记录
+	pipe := rdb.TxPipeline()
+	pipe.Del(ctx, "fileInfo:"+hashedKey)            // 删除 fileInfo 相关数据
+	pipe.Del(ctx, "hashedKeyToPath:"+hashedKey)     // 删除 path 相关数据
+	pipe.Del(ctx, "pathToHashedKey:"+filePath)      // 删除从路径到 hashedKey 的映射
+	pipe.Del(ctx, "hashedKeyToFileHash:"+hashedKey) // 删除 hashedKey 到 fileHash 的映射
+	if fullHash != "" {
+		pipe.Del(ctx, "hashedKeyToFullHash:"+hashedKey)      // 删除完整文件哈希相关数据
+		pipe.ZRem(ctx, "duplicateFiles:"+fullHash, filePath) // 从 duplicateFiles 有序集合中移除路径
+	}
+	pipe.SRem(ctx, "fileHashToPathSet:"+fileHash, filePath) // 从 hash 集合中移除文件路径
+
+	_, err = pipe.Exec(ctx)
+	if err != nil {
+		return fmt.Errorf("error deleting keys for outdated record %s: %v", hashedKey, err)
+	}
+
+	log.Printf("Deleted outdated record: path=%s\n", filePath)
+	return nil
+}
+
+func cleanUpHashKeys(rdb *redis.Client, ctx context.Context, fullHash, duplicateFilesKey string) error {
+	fileHashKey := "fileHashToPathSet:" + fullHash
+
+	// 使用管道批量删除 Redis 键
+	pipe := rdb.TxPipeline()
+	pipe.Del(ctx, duplicateFilesKey)
+	pipe.Del(ctx, fileHashKey)
+
+	_, err := pipe.Exec(ctx)
+	if err != nil {
+		return fmt.Errorf("error executing pipeline for cleaning up hash keys: %w", err)
+	}
+
+	log.Printf("Cleaned up Redis keys: %s and %s\n", duplicateFilesKey, fileHashKey)
+	return nil
+}
diff --git a/utils.go b/utils.go
new file mode 100644
--- /dev/null
+++ ./utils.go
@@ -0,0 +1,590 @@
+// utils.go
+package main
+
+import (
+	"bufio"
+	"bytes"
+	"context"
+	"encoding/gob"
+	"fmt"
+	"github.com/go-redis/redis/v8"
+	"log"
+	"os"
+	"path/filepath"
+	"regexp"
+	"sort"
+	"strings"
+	"sync"
+	"time"
+)
+
+var mu sync.Mutex
+
+func loadExcludePatterns(filename string) ([]string, error) {
+	file, err := os.Open(filename)
+	if err != nil {
+		return nil, err
+	}
+	defer file.Close()
+
+	var patterns []string
+	scanner := bufio.NewScanner(file)
+	for scanner.Scan() {
+		pattern := scanner.Text()
+		patterns = append(patterns, pattern)
+	}
+	return patterns, scanner.Err()
+}
+
+func sortKeys(keys []string, data map[string]FileInfo, sortByModTime bool) {
+	if sortByModTime {
+		sort.Slice(keys, func(i, j int) bool {
+			return data[keys[i]].ModTime.After(data[keys[j]].ModTime)
+		})
+	} else {
+		sort.Slice(keys, func(i, j int) bool {
+			return data[keys[i]].Size > data[keys[j]].Size
+		})
+	}
+}
+
+func compileExcludePatterns(filename string) ([]*regexp.Regexp, error) {
+	excludePatterns, err := loadExcludePatterns(filename)
+	if err != nil {
+		return nil, err
+	}
+
+	excludeRegexps := make([]*regexp.Regexp, len(excludePatterns))
+	for i, pattern := range excludePatterns {
+		regexPattern := strings.Replace(pattern, "*", ".*", -1)
+		excludeRegexps[i], err = regexp.Compile(regexPattern)
+		if err != nil {
+			return nil, fmt.Errorf("Invalid regex pattern '%s': %v", regexPattern, err)
+		}
+	}
+	return excludeRegexps, nil
+}
+
+func performSaveOperation(rootDir, filename string, sortByModTime bool, rdb *redis.Client, ctx context.Context) {
+	log.Printf("Starting save operation to %s\n", filepath.Join(rootDir, filename))
+	if err := saveToFile(rootDir, filename, sortByModTime, rdb, ctx); err != nil {
+		log.Printf("Error saving to %s: %s\n", filepath.Join(rootDir, filename), err)
+	} else {
+		log.Printf("Saved data to %s\n", filepath.Join(rootDir, filename))
+	}
+}
+
+func writeLinesToFile(filename string, lines []string) error {
+	file, err := os.Create(filename)
+	if err != nil {
+		return err
+	}
+	defer file.Close()
+
+	for _, line := range lines {
+		if _, err := fmt.Fprintln(file, line); err != nil {
+			return err
+		}
+	}
+	return nil
+}
+
+type fileInfo struct {
+	name      string
+	path      string
+	buf       bytes.Buffer
+	startTime int64
+	fileHash  string
+	fullHash  string
+	line      string
+	header    string
+	FileInfo  // 嵌入已有的FileInfo结构体
+}
+
+func processFileHash(rootDir string, fileHash string, filePaths []string, rdb *redis.Client, ctx context.Context, processedFullHashes map[string]bool) (int, error) {
+	fileCount := 0
+	hashes := make(map[string][]fileInfo) // 添加这行
+	for _, fullPath := range filePaths {
+		if !strings.HasPrefix(fullPath, rootDir) {
+			continue
+		}
+
+		if _, err := os.Stat(fullPath); os.IsNotExist(err) {
+			continue
+		}
+
+		semaphore <- struct{}{} // 获取一个信号量
+		relativePath, err := filepath.Rel(rootDir, fullPath)
+		if err != nil {
+			<-semaphore // 释放信号量
+			continue
+		}
+		fileName := filepath.Base(relativePath)
+
+		// 获取或计算完整文件的SHA-512哈希值
+		fullHash, err := getFullFileHash(fullPath, rdb, ctx)
+		if err != nil {
+			<-semaphore // 释放信号量
+			continue
+		}
+
+		// 计算文件的SHA-512哈希值（只读取前4KB）
+		fileHash, err := getFileHash(fullPath, rdb, ctx)
+		if err != nil {
+			<-semaphore // 释放信号量
+			continue
+		}
+
+		// 获取文件信息并编码
+		info, err := os.Stat(fullPath)
+		if err != nil {
+			<-semaphore // 释放信号量
+			continue
+		}
+
+		var buf bytes.Buffer
+		enc := gob.NewEncoder(&buf)
+		if err := enc.Encode(FileInfo{Size: info.Size(), ModTime: info.ModTime()}); err != nil {
+			<-semaphore // 释放信号量
+			continue
+		}
+
+		// 调用saveFileInfoToRedis函数来保存文件信息到Redis
+		if err := saveFileInfoToRedis(rdb, ctx, generateHash(fullPath), fullPath, buf, fileHash, fullHash); err != nil {
+			<-semaphore // 释放信号量
+			continue
+		}
+
+		infoStruct := fileInfo{
+			name:      fileName,
+			path:      fullPath,
+			buf:       buf,
+			startTime: time.Now().Unix(),
+			fileHash:  fileHash,
+			fullHash:  fullHash,
+			line:      fmt.Sprintf("%d,\"./%s\"", info.Size(), relativePath),
+			// 删除 header 字段
+			FileInfo: FileInfo{Size: info.Size(), ModTime: info.ModTime()},
+		}
+		hashes[fullHash] = append(hashes[fullHash], infoStruct)
+		fileCount++
+		<-semaphore // 释放信号量
+	}
+
+	var saveErr error
+	for fullHash, infos := range hashes {
+		if len(infos) > 1 {
+			mu.Lock()
+			if !processedFullHashes[fullHash] {
+				for _, info := range infos {
+					log.Printf("Saving duplicate file info to Redis for file: %s", info.path)
+					err := saveDuplicateFileInfoToRedis(rdb, ctx, fullHash, info)
+					if err != nil {
+						log.Printf("Error saving duplicate file info to Redis for file: %s, error: %v", info.path, err)
+						saveErr = err
+					} else {
+						log.Printf("Successfully saved duplicate file info to Redis for file: %s", info.path)
+					}
+				}
+				processedFullHashes[fullHash] = true
+			}
+			mu.Unlock()
+		}
+	}
+
+	if saveErr != nil {
+		return fileCount, saveErr
+	}
+
+	return fileCount, nil
+}
+
+// 主函数
+func findAndLogDuplicates(rootDir string, rdb *redis.Client, ctx context.Context, maxDuplicates int) error {
+	log.Println("Scanning file hashes")
+	fileHashes, err := scanFileHashes(rdb, ctx)
+	if err != nil {
+		return err
+	}
+
+	fileCount := 0
+
+	// 用于存储已处理的文件哈希
+	processedFullHashes := make(map[string]bool)
+
+	workerCount := 500
+	var stopProcessing bool
+	taskQueue, poolWg, stopFunc, _ := NewWorkerPool(workerCount, &stopProcessing)
+
+	for fileHash, filePaths := range fileHashes {
+		if len(filePaths) > 1 {
+			fileCount += 1
+			if fileCount >= maxDuplicates {
+				stopProcessing = true
+				break
+			}
+
+			semaphore <- struct{}{} // 获取一个信号量
+
+			taskQueue <- func(fileHash string, filePaths []string) Task {
+				return func() {
+					defer func() {
+						<-semaphore // 释放信号量
+					}()
+
+					if stopProcessing {
+						return
+					}
+
+					log.Printf("Processing hash %s with %d files\n", fileHash, len(filePaths)) // 添加的日志
+
+					_, err := processFileHash(rootDir, fileHash, filePaths, rdb, ctx, processedFullHashes)
+					if err != nil {
+						log.Printf("Error processing file hash %s: %s\n", fileHash, err)
+						return
+					}
+				}
+			}(fileHash, filePaths)
+		}
+	}
+
+	stopFunc()
+	poolWg.Wait()
+
+	log.Printf("Total duplicates found: %d\n", fileCount)
+
+	if fileCount == 0 {
+		return nil
+	}
+
+	return nil
+}
+
+func scanFileHashes(rdb *redis.Client, ctx context.Context) (map[string][]string, error) {
+	iter := rdb.Scan(ctx, 0, "fileHashToPathSet:*", 0).Iterator()
+	fileHashes := make(map[string][]string)
+	for iter.Next(ctx) {
+		hashKey := iter.Val()
+		fileHash := strings.TrimPrefix(hashKey, "fileHashToPathSet:")
+		duplicateFiles, err := rdb.SMembers(ctx, hashKey).Result()
+		if err != nil {
+			return nil, fmt.Errorf("error retrieving duplicate files for key %s: %w", hashKey, err)
+		}
+		// 只添加包含多个文件路径的条目
+		if len(duplicateFiles) > 1 {
+			fileHashes[fileHash] = duplicateFiles
+		}
+	}
+	if err := iter.Err(); err != nil {
+		return nil, fmt.Errorf("error during iteration: %w", err)
+	}
+	return fileHashes, nil
+}
+
+func writeDuplicateFilesToFile(rootDir string, outputFile string, rdb *redis.Client, ctx context.Context) error {
+	file, err := os.Create(filepath.Join(rootDir, outputFile))
+	if err != nil {
+		return fmt.Errorf("Error creating output file: %s", err)
+	}
+	defer file.Close()
+
+	iter := rdb.Scan(ctx, 0, "duplicateFiles:*", 0).Iterator()
+	for iter.Next(ctx) {
+		duplicateFilesKey := iter.Val()
+
+		// 获取 fullHash
+		fullHash := strings.TrimPrefix(duplicateFilesKey, "duplicateFiles:")
+
+		// 获取重复文件列表，按文件名长度（score）排序
+		duplicateFiles, err := rdb.ZRange(ctx, duplicateFilesKey, 0, -1).Result()
+		if err != nil {
+			continue
+		}
+
+		if len(duplicateFiles) > 1 {
+			header := fmt.Sprintf("Duplicate files for fullHash %s:\n", fullHash)
+			if _, err := file.WriteString(header); err != nil {
+				continue
+			}
+			for i, duplicateFile := range duplicateFiles {
+				// 获取文件信息
+				hashedKey, err := getHashedKeyFromPath(rdb, ctx, duplicateFile)
+				if err != nil {
+					continue
+				}
+				fileInfoData, err := rdb.Get(ctx, "fileInfo:"+hashedKey).Bytes()
+				if err != nil {
+					continue
+				}
+
+				// 解码文件信息
+				var fileInfo FileInfo
+				buf := bytes.NewBuffer(fileInfoData)
+				dec := gob.NewDecoder(buf)
+				if err := dec.Decode(&fileInfo); err != nil {
+					continue
+				}
+
+				// 获取相对路径
+				relativePath, err := filepath.Rel(rootDir, duplicateFile)
+				if err != nil {
+					continue
+				}
+
+				// 使用 filepath.Clean 来规范化路径
+				cleanPath := filepath.Clean(relativePath)
+
+				// 根据文件是否为第一个，添加不同的前缀
+				var line string
+				if i == 0 {
+					line = fmt.Sprintf("[+] %d,\"./%s\"\n", fileInfo.Size, cleanPath)
+				} else {
+					line = fmt.Sprintf("[-] %d,\"./%s\"\n", fileInfo.Size, cleanPath)
+				}
+
+				if _, err := file.WriteString(line); err != nil {
+					continue
+				}
+			}
+			if _, err := file.WriteString("\n"); err != nil {
+				continue
+			}
+		}
+	}
+
+	if err := iter.Err(); err != nil {
+		return err
+	}
+
+	// 添加日志记录
+	log.Printf("Duplicate files have been written to %s", filepath.Join(rootDir, outputFile))
+
+	return nil
+}
+
+func getFileSizeFromRedis(rdb *redis.Client, ctx context.Context, fullPath string) (int64, error) {
+	// 首先从 fullPath 获取 hashedKey
+	hashedKey, err := getHashedKeyFromPath(rdb, ctx, fullPath)
+	if err != nil {
+		return 0, fmt.Errorf("error getting hashed key for %s: %w", fullPath, err)
+	}
+
+	// 然后使用 hashedKey 从 Redis 获取文件信息
+	fileInfoData, err := rdb.Get(ctx, "fileInfo:"+hashedKey).Bytes()
+	if err != nil {
+		return 0, fmt.Errorf("error getting file info for hashed key %s: %w", hashedKey, err)
+	}
+
+	var fileInfo FileInfo
+	buf := bytes.NewBuffer(fileInfoData)
+	dec := gob.NewDecoder(buf)
+	if err := dec.Decode(&fileInfo); err != nil {
+		return 0, fmt.Errorf("error decoding file info for hashed key %s: %w", hashedKey, err)
+	}
+
+	return fileInfo.Size, nil
+}
+
+func getHashedKeyFromPath(rdb *redis.Client, ctx context.Context, path string) (string, error) {
+	hashedKey, err := rdb.Get(ctx, "pathToHashedKey:"+filepath.Clean(path)).Result()
+	return hashedKey, err
+}
+
+// extractFileName extracts the file name from a given file path.
+func extractFileName(filePath string) string {
+	return strings.ToLower(filepath.Base(filePath))
+}
+
+var pattern = regexp.MustCompile(`\b(?:\d{2}\.\d{2}\.\d{2}|(?:\d+|[a-z]+(?:\d+[a-z]*)?))\b`)
+
+func extractKeywords(fileNames []string, stopProcessing *bool) []string {
+	workerCount := 100
+	// 创建自己的工作池
+	taskQueue, poolWg, stopFunc, _ := NewWorkerPool(workerCount, stopProcessing)
+
+	keywordsCh := make(chan string, len(fileNames)*10) // 假设每个文件名大约有10个关键词
+
+	for _, fileName := range fileNames {
+		poolWg.Add(1) // 确保在任务开始前递增计数
+		taskQueue <- func(name string) Task {
+			return func() {
+				defer func() {
+					poolWg.Done() // 任务结束时递减计数
+				}()
+
+				nameWithoutExt := strings.TrimSuffix(name, filepath.Ext(name))
+				matches := pattern.FindAllString(nameWithoutExt, -1)
+				for _, match := range matches {
+					keywordsCh <- match
+				}
+			}
+		}(fileName)
+	}
+
+	stopFunc() // 使用停止函数来关闭任务队列
+
+	// 关闭通道的逻辑保持不变
+	go func() {
+		poolWg.Wait()
+		close(keywordsCh)
+	}()
+
+	// 收集关键词
+	keywordsMap := make(map[string]struct{})
+	for keyword := range keywordsCh {
+		keywordsMap[keyword] = struct{}{}
+	}
+
+	// 将map转换为slice
+	var keywords []string
+	for keyword := range keywordsMap {
+		keywords = append(keywords, keyword)
+	}
+
+	return keywords
+}
+
+func findCloseFiles(fileNames, filePaths, keywords []string) map[string][]string {
+	closeFiles := make(map[string][]string)
+
+	for _, kw := range keywords {
+		for i, fileName := range fileNames {
+			if strings.Contains(strings.ToLower(fileName), strings.ToLower(kw)) {
+				closeFiles[kw] = append(closeFiles[kw], filePaths[i])
+			}
+		}
+	}
+
+	return closeFiles
+}
+
+func deleteDuplicateFiles(rootDir string, rdb *redis.Client, ctx context.Context) error {
+	iter := rdb.Scan(ctx, 0, "duplicateFiles:*", 0).Iterator()
+	for iter.Next(ctx) {
+		duplicateFilesKey := iter.Val()
+
+		// 获取 fullHash
+		fullHash := strings.TrimPrefix(duplicateFilesKey, "duplicateFiles:")
+
+		// 获取重复文件列表
+		duplicateFiles, err := rdb.ZRange(ctx, duplicateFilesKey, 0, -1).Result()
+		if err != nil {
+			continue
+		}
+
+		if len(duplicateFiles) > 1 {
+			// 保留第一个文件（你可以根据自己的需求修改保留策略）
+			fileToKeep := duplicateFiles[0]
+
+			// 检查第一个文件是否存在
+			if _, err := os.Stat(fileToKeep); os.IsNotExist(err) {
+				continue
+			}
+
+			filesToDelete := duplicateFiles[1:]
+
+			for _, duplicateFile := range filesToDelete {
+				// 先从 Redis 中删除相关记录
+				err := cleanUpRecordsByFilePath(rdb, ctx, duplicateFile)
+				if err != nil {
+					continue
+				}
+
+				// 然后删除文件
+				err = os.Remove(duplicateFile)
+				if err != nil {
+					continue
+				}
+			}
+
+			// 清理 Redis 键
+			err = cleanUpHashKeys(rdb, ctx, fullHash, duplicateFilesKey)
+			if err != nil {
+			}
+		}
+	}
+
+	if err := iter.Err(); err != nil {
+		return err
+	}
+
+	return nil
+}
+
+func shouldStopDuplicateFileSearch(duplicateCount int, maxDuplicateFiles int) bool {
+	return duplicateCount >= maxDuplicateFiles
+}
+
+func saveToFile(rootDir, filename string, sortByModTime bool, rdb *redis.Client, ctx context.Context) error {
+	iter := rdb.Scan(ctx, 0, "fileInfo:*", 0).Iterator()
+	data := make(map[string]FileInfo)
+
+	for iter.Next(ctx) {
+		hashedKey := strings.TrimPrefix(iter.Val(), "fileInfo:")
+
+		originalPath, err := rdb.Get(ctx, "hashedKeyToPath:"+hashedKey).Result()
+		if err != nil {
+			log.Printf("Error getting original path for key %s: %v", hashedKey, err)
+			continue
+		}
+
+		fileInfoData, err := rdb.Get(ctx, "fileInfo:"+hashedKey).Bytes()
+		if err != nil {
+			log.Printf("Error getting file info for key %s: %v", hashedKey, err)
+			continue
+		}
+
+		var fileInfo FileInfo
+		buf := bytes.NewBuffer(fileInfoData)
+		dec := gob.NewDecoder(buf)
+		if err := dec.Decode(&fileInfo); err != nil {
+			log.Printf("Error decoding file info for key %s: %v", hashedKey, err)
+			continue
+		}
+
+		data[originalPath] = fileInfo
+	}
+
+	if err := iter.Err(); err != nil {
+		return fmt.Errorf("error iterating over Redis keys: %w", err)
+	}
+
+	if len(data) == 0 {
+		return nil
+	}
+
+	return writeDataToFile(rootDir, filename, data, sortByModTime)
+}
+
+func writeDataToFile(rootDir, filename string, data map[string]FileInfo, sortByModTime bool) error {
+	var lines []string
+	keys := make([]string, 0, len(data))
+	for k := range data {
+		keys = append(keys, k)
+	}
+
+	sortKeys(keys, data, sortByModTime)
+
+	for _, k := range keys {
+		relativePath, err := filepath.Rel(rootDir, k)
+		if err != nil {
+			log.Printf("Error getting relative path for %s: %v", k, err)
+			continue
+		}
+		line := formatFileInfoLine(data[k], relativePath, sortByModTime)
+		lines = append(lines, line)
+	}
+
+	return writeLinesToFile(filepath.Join(rootDir, filename), lines)
+}
+
+func formatFileInfoLine(fileInfo FileInfo, relativePath string, sortByModTime bool) string {
+	if sortByModTime {
+		return fmt.Sprintf("\"./%s\"", relativePath)
+	}
+	return fmt.Sprintf("%d,\"./%s\"", fileInfo.Size, relativePath)
+}
+
+func decodeGob(data []byte, v interface{}) error {
+	return gob.NewDecoder(bytes.NewReader(data)).Decode(v)
+}
diff --git a/worker_pool.go b/worker_pool.go
new file mode 100644
--- /dev/null
+++ ./worker_pool.go
@@ -0,0 +1,40 @@
+// worker_pool.go
+package main
+
+import (
+	"sync"
+)
+
+// Task 定义了工作池中的任务类型
+type Task func()
+
+func NewWorkerPool(workerCount int, stopProcessing *bool) (chan<- Task, *sync.WaitGroup, func(), chan struct{}) {
+	var wg sync.WaitGroup
+	taskQueue := make(chan Task)
+	stopSignal := make(chan struct{})
+
+	for i := 0; i < workerCount; i++ {
+		wg.Add(1)
+		go func() {
+			defer wg.Done()
+			for {
+				select {
+				case <-stopSignal:
+					return
+				case task, ok := <-taskQueue:
+					if !ok || *stopProcessing {
+						return
+					}
+					task()
+				}
+			}
+		}()
+	}
+
+	stopFunc := func() {
+		close(stopSignal) // 发送停止信号
+		close(taskQueue)  // 关闭任务队列
+	}
+
+	return taskQueue, &wg, stopFunc, stopSignal
+}
