diff --git a/data_serialization.go b/data_serialization.go
new file mode 100644
--- /dev/null
+++ ./data_serialization.go
@@ -0,0 +1,3 @@
+package main
+
+import ()
diff --git a/file_processing.go b/file_processing.go
new file mode 100644
--- /dev/null
+++ ./file_processing.go
@@ -0,0 +1,523 @@
+// file_processing.go
+package main
+
+import (
+	"bytes"
+	"context"
+	"crypto/sha512"
+	"encoding/gob"
+	"fmt"
+	"github.com/go-redis/redis/v8"
+	"github.com/spf13/afero"
+	"io"
+	"log"
+	"os"
+	"path/filepath"
+	"regexp"
+	"sort"
+	"strconv"
+	"strings"
+	"time"
+)
+
+func FormatTimestamp(timestamp string) string {
+	parts := strings.Split(timestamp, ":")
+	formattedParts := make([]string, len(parts))
+	for i, part := range parts {
+		num, _ := strconv.Atoi(part)
+		formattedParts[i] = fmt.Sprintf("%02d", num)
+	}
+	return strings.Join(formattedParts, ":")
+}
+
+func TimestampToSeconds(timestamp string) int {
+	parts := strings.Split(timestamp, ":")
+	var totalSeconds int
+	if len(parts) == 2 {
+		minutes, _ := strconv.Atoi(parts[0])
+		seconds, _ := strconv.Atoi(parts[1])
+		totalSeconds = minutes*60 + seconds
+	} else if len(parts) == 3 {
+		hours, _ := strconv.Atoi(parts[0])
+		minutes, _ := strconv.Atoi(parts[1])
+		seconds, _ := strconv.Atoi(parts[2])
+		totalSeconds = hours*3600 + minutes*60 + seconds
+	} else {
+		fmt.Printf("无效的时间戳格式: %s\n", timestamp)
+		return 0
+	}
+
+	return totalSeconds
+}
+
+func ExtractTimestamps(filePath string) []string {
+	pattern := regexp.MustCompile(`[:,/](\d{1,2}(?::\d{1,2}){1,2})`)
+	matches := pattern.FindAllStringSubmatch(filePath, -1)
+
+	timestamps := make([]string, 0, len(matches))
+	for _, match := range matches {
+		if len(match) > 1 {
+			timestamps = append(timestamps, FormatTimestamp(match[1]))
+		}
+	}
+
+	uniqueTimestamps := make([]string, 0, len(timestamps))
+	seen := make(map[string]bool)
+	for _, ts := range timestamps {
+		if !seen[ts] {
+			seen[ts] = true
+			uniqueTimestamps = append(uniqueTimestamps, ts)
+		}
+	}
+
+	sort.Slice(uniqueTimestamps, func(i, j int) bool {
+		return TimestampToSeconds(uniqueTimestamps[i]) < TimestampToSeconds(uniqueTimestamps[j])
+	})
+
+	return uniqueTimestamps
+}
+
+type timestamp struct {
+	hour, minute, second int
+}
+
+func parseTimestamp(ts string) timestamp {
+	parts := strings.Split(ts, ":")
+	hour, _ := strconv.Atoi(parts[0])
+	minute, _ := strconv.Atoi(parts[1])
+	second := 0
+	if len(parts) > 2 {
+		second, _ = strconv.Atoi(parts[2])
+	}
+	return timestamp{hour, minute, second}
+}
+
+const (
+	ReadLimit       = 100 * 1024 // 100KB
+	FullFileReadCmd = -1
+)
+
+type FileInfo struct {
+	Size    int64
+	ModTime time.Time
+}
+
+type FileProcessor struct {
+	Rdb                           *redis.Client
+	Ctx                           context.Context
+	generateHashFunc              func(string) string
+	calculateFileHashFunc         func(path string, limit int64) (string, error)
+	fs                            afero.Fs
+	saveToFileFunc                func(dir, filename string, sortByModTime bool) error
+	writeDuplicateFilesToFileFunc func(rootDir string, outputFile string, rdb *redis.Client, ctx context.Context) error
+}
+
+func NewFileProcessor(Rdb *redis.Client, Ctx context.Context) *FileProcessor {
+	fp := &FileProcessor{
+		Rdb:              Rdb,
+		Ctx:              Ctx,
+		generateHashFunc: generateHash,
+		fs:               afero.NewOsFs(),
+	}
+	fp.saveToFileFunc = fp.saveToFile
+	fp.writeDuplicateFilesToFileFunc = writeDuplicateFilesToFile
+	fp.calculateFileHashFunc = fp.calculateFileHash
+	return fp
+}
+
+func (fp *FileProcessor) WriteDuplicateFilesToFile(rootDir, outputFile string, rdb *redis.Client, ctx context.Context) error {
+	file, err := fp.fs.Create(filepath.Join(rootDir, outputFile))
+	if err != nil {
+		return fmt.Errorf("Error creating output file: %s", err)
+	}
+	defer file.Close()
+
+	iter := rdb.Scan(ctx, 0, "duplicateFiles:*", 0).Iterator()
+	for iter.Next(ctx) {
+		duplicateFilesKey := iter.Val()
+		fullHash := strings.TrimPrefix(duplicateFilesKey, "duplicateFiles:")
+
+		duplicateFiles, err := rdb.ZRange(ctx, duplicateFilesKey, 0, -1).Result()
+		if err != nil {
+			log.Printf("Error getting duplicate files for key %s: %v", duplicateFilesKey, err)
+			continue
+		}
+
+		if len(duplicateFiles) > 1 {
+			header := fmt.Sprintf("Duplicate files for fullHash %s:\n", fullHash)
+			if _, err := file.WriteString(header); err != nil {
+				log.Printf("Error writing header: %v", err)
+				continue
+			}
+			for i, duplicateFile := range duplicateFiles {
+				hashedKey, err := fp.getHashedKeyFromPath(duplicateFile)
+				if err != nil {
+					log.Printf("Error getting hashed key for path %s: %v", duplicateFile, err)
+					continue
+				}
+				fileInfo, err := fp.getFileInfoFromRedis(hashedKey)
+				if err != nil {
+					log.Printf("Error getting file info for key %s: %v", hashedKey, err)
+					continue
+				}
+
+				cleanedPath := cleanRelativePath(rootDir, duplicateFile)
+				var line string
+				if i == 0 {
+					line = fmt.Sprintf("[+] %d,\"%s\"\n", fileInfo.Size, cleanedPath)
+				} else {
+					line = fmt.Sprintf("[-] %d,\"%s\"\n", fileInfo.Size, cleanedPath)
+				}
+				if _, err := file.WriteString(line); err != nil {
+					log.Printf("Error writing line: %v", err)
+					continue
+				}
+			}
+			// 在每组重复文件的末尾添加一个制表符
+			if _, err := file.WriteString("\t"); err != nil {
+				log.Printf("Error writing separator: %v", err)
+			}
+		}
+	}
+
+	return nil
+}
+
+func (fp *FileProcessor) getFileInfoFromRedis(hashedKey string) (FileInfo, error) {
+	var fileInfo FileInfo
+	value, err := fp.Rdb.Get(fp.Ctx, "fileInfo:"+hashedKey).Bytes()
+	if err != nil {
+		return fileInfo, fmt.Errorf("error getting file info from Redis: %w", err)
+	}
+
+	buf := bytes.NewBuffer(value)
+	dec := gob.NewDecoder(buf)
+	err = dec.Decode(&fileInfo)
+	if err != nil {
+		return fileInfo, fmt.Errorf("error decoding file info: %w", err)
+	}
+	return fileInfo, nil
+}
+
+func (fp *FileProcessor) saveToFile(dir, filename string, sortByModTime bool) error {
+	iter := fp.Rdb.Scan(fp.Ctx, 0, "fileInfo:*", 0).Iterator()
+	data := make(map[string]FileInfo)
+
+	for iter.Next(fp.Ctx) {
+		hashedKey := strings.TrimPrefix(iter.Val(), "fileInfo:")
+
+		originalPath, err := fp.Rdb.Get(fp.Ctx, "hashedKeyToPath:"+hashedKey).Result()
+		if err != nil {
+			log.Printf("Error getting original path for key %s: %v", hashedKey, err)
+			continue
+		}
+
+		fileInfo, err := fp.getFileInfoFromRedis(hashedKey)
+		if err != nil {
+			log.Printf("Error getting file info for key %s: %v", hashedKey, err)
+			continue
+		}
+
+		data[originalPath] = fileInfo
+	}
+
+	if err := iter.Err(); err != nil {
+		return fmt.Errorf("error iterating over Redis keys: %w", err)
+	}
+
+	if len(data) == 0 {
+		return nil
+	}
+
+	file, err := fp.fs.Create(filepath.Join(dir, filename))
+	if err != nil {
+		return fmt.Errorf("error creating file: %w", err)
+	}
+	defer file.Close()
+
+	keys := make([]string, 0, len(data))
+	for k := range data {
+		keys = append(keys, k)
+	}
+
+	sortKeys(keys, data, sortByModTime)
+
+	for _, k := range keys {
+		relativePath, err := filepath.Rel(dir, k)
+		if err != nil {
+			log.Printf("Error getting relative path for %s: %v", k, err)
+			continue
+		}
+		line := formatFileInfoLine(data[k], relativePath, sortByModTime)
+		if _, err := fmt.Fprint(file, line); err != nil {
+			return fmt.Errorf("error writing to file: %w", err)
+		}
+	}
+
+	return nil
+}
+
+func (fp *FileProcessor) ProcessFile(path string) error {
+	info, err := fp.fs.Stat(path)
+	if err != nil {
+		return fmt.Errorf("error getting file info: %w", err)
+	}
+
+	fileHash, err := fp.calculateFileHashFunc(path, ReadLimit)
+	if err != nil {
+		return fmt.Errorf("error calculating file hash: %w", err)
+	}
+
+	fullHash, err := fp.calculateFileHashFunc(path, FullFileReadCmd)
+	if err != nil {
+		return fmt.Errorf("error calculating full file hash: %w", err)
+	}
+
+	fileInfo := FileInfo{
+		Size:    info.Size(),
+		ModTime: info.ModTime(),
+	}
+
+	err = fp.saveFileInfoToRedis(path, fileInfo, fileHash, fullHash)
+	if err != nil {
+		return fmt.Errorf("error saving file info to Redis: %w", err)
+	}
+
+	return nil
+}
+
+func (fp *FileProcessor) calculateFileHash(path string, limit int64) (string, error) {
+	f, err := fp.fs.Open(path)
+	if err != nil {
+		return "", fmt.Errorf("error opening file: %w", err)
+	}
+	defer f.Close()
+
+	h := sha512.New()
+	if limit == FullFileReadCmd {
+		if _, err := io.Copy(h, f); err != nil {
+			return "", fmt.Errorf("error reading full file: %w", err)
+		}
+	} else {
+		if _, err := io.CopyN(h, f, limit); err != nil && err != io.EOF {
+			return "", fmt.Errorf("error reading file: %w", err)
+		}
+	}
+
+	return fmt.Sprintf("%x", h.Sum(nil)), nil
+}
+
+func (fp *FileProcessor) saveFileInfoToRedis(path string, info FileInfo, fileHash, fullHash string) error {
+	hashedKey := fp.generateHashFunc(path)
+
+	var buf bytes.Buffer
+	enc := gob.NewEncoder(&buf)
+	if err := enc.Encode(info); err != nil {
+		return fmt.Errorf("error encoding file info: %w", err)
+	}
+
+	pipe := fp.Rdb.Pipeline()
+	pipe.SetNX(fp.Ctx, "fileInfo:"+hashedKey, buf.Bytes(), 0)
+	pipe.Set(fp.Ctx, "hashedKeyToPath:"+hashedKey, path, 0)
+	pipe.SAdd(fp.Ctx, "fileHashToPathSet:"+fileHash, path)
+	pipe.Set(fp.Ctx, "hashedKeyToFullHash:"+hashedKey, fullHash, 0)
+	pipe.Set(fp.Ctx, "pathToHashedKey:"+path, hashedKey, 0)
+	pipe.Set(fp.Ctx, "hashedKeyToFileHash:"+hashedKey, fileHash, 0)
+
+	_, err := pipe.Exec(fp.Ctx)
+	if err != nil {
+		return fmt.Errorf("error executing Redis pipeline: %w", err)
+	}
+	return nil
+}
+
+const readLimit = 100 * 1024 // 100KB
+
+// 处理目录
+func processDirectory(path string) {
+}
+
+// 处理符号链接
+func processSymlink(path string) {
+}
+
+// 处理关键词
+func processKeyword(keyword string, keywordFiles []string, Rdb *redis.Client, Ctx context.Context, rootDir string) {
+	// 对 keywordFiles 进行排序
+	sort.Slice(keywordFiles, func(i, j int) bool {
+		sizeI, _ := getFileSize(Rdb, Ctx, filepath.Join(rootDir, cleanPath(keywordFiles[i])))
+		sizeJ, _ := getFileSize(Rdb, Ctx, filepath.Join(rootDir, cleanPath(keywordFiles[j])))
+		return sizeI > sizeJ
+	})
+
+	// 准备要写入的数据
+	var outputData strings.Builder
+	outputData.WriteString(keyword + "\n")
+	for _, filePath := range keywordFiles {
+		fileSize, _ := getFileSize(Rdb, Ctx, filepath.Join(rootDir, cleanPath(filePath)))
+		outputData.WriteString(fmt.Sprintf("%d,%s\n", fileSize, filePath))
+	}
+
+	// 创建并写入文件
+	outputFilePath := filepath.Join(rootDir, keyword+".txt")
+	outputFile, err := os.Create(outputFilePath)
+	if err != nil {
+		return
+	}
+	defer outputFile.Close()
+
+	_, err = outputFile.WriteString(outputData.String())
+	if err != nil {
+	}
+}
+
+// 清理和标准化路径
+func cleanPath(path string) string {
+	path = strings.Trim(path, `"`)
+	if strings.HasPrefix(path, "./") {
+		path = strings.TrimPrefix(path, "./")
+	}
+	return path
+}
+
+// 获取文件大小
+func getFileSize(Rdb *redis.Client, Ctx context.Context, fullPath string) (int64, error) {
+	size, err := getFileSizeFromRedis(Rdb, Ctx, fullPath)
+	if err != nil {
+		return 0, err
+	}
+	return size, nil
+}
+
+// 获取文件哈希值
+func getHash(path string, Rdb *redis.Client, Ctx context.Context, keyPrefix string, limit int64) (string, error) {
+	hashedKey := generateHash(path) // 使用全局的 generateHash 函数
+	hashKey := keyPrefix + hashedKey
+
+	// 尝试从Redis获取哈希值
+	hash, err := Rdb.Get(Ctx, hashKey).Result()
+	if err == nil && hash != "" {
+		return hash, nil
+	}
+
+	if err != nil && err != redis.Nil {
+		return "", err
+	}
+
+	// 计算哈希值
+	file, err := os.Open(path)
+	if err != nil {
+		return "", err
+	}
+	defer file.Close()
+
+	hasher := sha512.New()
+
+	if limit > 0 {
+		// 只读取前 limit 字节
+		reader := io.LimitReader(file, limit)
+		if _, err := io.Copy(hasher, reader); err != nil {
+			return "", err
+		}
+	} else {
+		// 读取整个文件
+		buf := make([]byte, 4*1024*1024) // 每次读取 4MB 数据
+		for {
+			n, err := file.Read(buf)
+			if err != nil && err != io.EOF {
+				return "", err
+			}
+			if n == 0 {
+				break
+			}
+			if _, err := hasher.Write(buf[:n]); err != nil {
+				return "", err
+			}
+		}
+	}
+
+	hashBytes := hasher.Sum(nil)
+	hash = fmt.Sprintf("%x", hashBytes)
+
+	// 将计算出的哈希值保存到Redis
+	err = Rdb.Set(Ctx, hashKey, hash, 0).Err()
+	if err != nil {
+		return "", err
+	}
+	return hash, nil
+}
+
+// 获取文件哈希
+func getFileHash(path string, Rdb *redis.Client, Ctx context.Context) (string, error) {
+	const readLimit = 100 * 1024 // 100KB
+	return getHash(path, Rdb, Ctx, "hashedKeyToFileHash:", readLimit)
+}
+
+// 获取完整文件哈希
+func getFullFileHash(path string, Rdb *redis.Client, Ctx context.Context) (string, error) {
+	const noLimit = -1 // No limit for full file hash
+	hash, err := getHash(path, Rdb, Ctx, "hashedKeyToFullHash:", noLimit)
+	if err != nil {
+		log.Printf("Error calculating full hash for file %s: %v", path, err)
+	} else {
+		log.Printf("Full hash for file %s: %s", path, hash)
+	}
+	return hash, err
+}
+
+func getFileInfo(rdb *redis.Client, ctx context.Context, filePath string) (FileInfo, error) {
+	hashedKey, err := rdb.Get(ctx, "pathToHashedKey:"+filePath).Result()
+	if err != nil {
+		return FileInfo{}, err
+	}
+
+	fileInfoData, err := rdb.Get(ctx, "fileInfo:"+hashedKey).Bytes()
+	if err != nil {
+		return FileInfo{}, err
+	}
+
+	var fileInfo FileInfo
+	buf := bytes.NewBuffer(fileInfoData)
+	dec := gob.NewDecoder(buf)
+	if err := dec.Decode(&fileInfo); err != nil {
+		return FileInfo{}, err
+	}
+
+	return fileInfo, nil
+}
+
+const timestampWeight = 1000000000 // 使用一个非常大的数字
+
+func CalculateScore(timestamps []string, fileNameLength int) float64 {
+	timestampCount := len(timestamps)
+	return float64(-(timestampCount*timestampWeight + fileNameLength))
+}
+
+func cleanRelativePath(rootDir, fullPath string) string {
+	rel, err := filepath.Rel(rootDir, fullPath)
+	if err != nil {
+		return fullPath
+	}
+	return "./" + rel
+}
+
+func (fp *FileProcessor) SaveDuplicateFileInfoToRedis(fullHash string, info FileInfo, filePath string) error {
+	timestamps := ExtractTimestamps(filePath)
+	fileNameLength := len(filepath.Base(filePath))
+	score := CalculateScore(timestamps, fileNameLength)
+
+	_, err := fp.Rdb.ZAdd(fp.Ctx, "duplicateFiles:"+fullHash, &redis.Z{
+		Score:  score,
+		Member: filePath,
+	}).Result()
+
+	if err != nil {
+		return fmt.Errorf("error adding duplicate file to Redis: %w", err)
+	}
+
+	return nil
+}
+
+func (fp *FileProcessor) getHashedKeyFromPath(path string) (string, error) {
+	return fp.Rdb.Get(fp.Ctx, "pathToHashedKey:"+filepath.Clean(path)).Result()
+}
diff --git a/file_processing_test.go b/file_processing_test.go
new file mode 100644
--- /dev/null
+++ ./file_processing_test.go
@@ -0,0 +1,804 @@
+// file_processing_test.go
+
+package main
+
+import (
+	"bytes"
+	"context"
+	"encoding/gob"
+	"fmt"
+	"github.com/alicebob/miniredis/v2"
+	"github.com/go-redis/redis/v8"
+	"github.com/go-redis/redismock/v8"
+	"github.com/spf13/afero"
+	"github.com/stretchr/testify/assert"
+	"io/ioutil"
+	"os"
+	"path/filepath" // 添加这行
+	"testing"
+	"time"
+)
+
+func TestFormatTimestamp(t *testing.T) {
+	tests := []struct {
+		input    string
+		expected string
+	}{
+		{"1:2:3", "01:02:03"},
+		{"10:20:30", "10:20:30"},
+		{"1:2", "01:02"},
+		{"10:20", "10:20"},
+	}
+
+	for _, test := range tests {
+		result := FormatTimestamp(test.input)
+		assert.Equal(t, test.expected, result)
+	}
+}
+
+func TestCalculateScore(t *testing.T) {
+	t.Run("Same timestamp length, different file name length", func(t *testing.T) {
+		score1 := CalculateScore([]string{"12:34:56", "01:23:45"}, 15)
+		score2 := CalculateScore([]string{"12:34:56", "01:23:45"}, 10)
+		assert.Less(t, score1, score2, "Score with longer file name should be less (sorted first)")
+	})
+
+	t.Run("Different timestamp length, same file name length", func(t *testing.T) {
+		score1 := CalculateScore([]string{"12:34:56", "01:23:45", "00:11:22"}, 10)
+		score2 := CalculateScore([]string{"12:34:56", "01:23:45"}, 10)
+		assert.Less(t, score1, score2, "Score with more timestamps should be less (sorted first)")
+	})
+
+	t.Run("Different timestamp length and file name length", func(t *testing.T) {
+		score1 := CalculateScore([]string{"12:34:56", "01:23:45", "00:11:22"}, 10)
+		score2 := CalculateScore([]string{"12:34:56", "01:23:45"}, 11155515)
+		assert.Less(t, score1, score2, "Score with more timestamps should be less, even if file name is shorter")
+	})
+}
+
+func TestTimestampToSeconds(t *testing.T) {
+	tests := []struct {
+		input    string
+		expected int
+	}{
+		{"1:2:3", 3723},
+		{"10:20:30", 37230},
+		{"1:2", 62},
+		{"10:20", 620},
+		{"invalid", 0},
+	}
+
+	for _, test := range tests {
+		result := TimestampToSeconds(test.input)
+		assert.Equal(t, test.expected, result)
+	}
+}
+
+func TestFileProcessor_SaveDuplicateFileInfoToRedis(t *testing.T) {
+	mr, err := miniredis.Run()
+	if err != nil {
+		t.Fatal(err)
+	}
+	defer mr.Close()
+
+	rdb := redis.NewClient(&redis.Options{
+		Addr: mr.Addr(),
+	})
+	ctx := context.Background()
+
+	fp := NewFileProcessor(rdb, ctx)
+
+	fullHash := "testhash"
+	info := FileInfo{
+		Size:    1000,
+		ModTime: time.Now(),
+	}
+
+	// Test case 1: File with one timestamp
+	filePath1 := "/path/to/file:12:34:56.mp4"
+	err = fp.SaveDuplicateFileInfoToRedis(fullHash, info, filePath1)
+	assert.NoError(t, err)
+
+	// Test case 2: File with two timestamps
+	filePath2 := "/path/to/file:12:34:56,01:23:45.mp4"
+	err = fp.SaveDuplicateFileInfoToRedis(fullHash, info, filePath2)
+	assert.NoError(t, err)
+
+	// Test case 3: File with no timestamp
+	filePath3 := "/path/to/file.mp4"
+	err = fp.SaveDuplicateFileInfoToRedis(fullHash, info, filePath3)
+	assert.NoError(t, err)
+
+	// Verify the data was saved correctly and in the right order
+	members, err := rdb.ZRange(ctx, "duplicateFiles:"+fullHash, 0, -1).Result()
+	assert.NoError(t, err)
+	assert.Equal(t, 3, len(members))
+	assert.Equal(t, filePath2, members[0]) // Should be first due to more timestamps
+	assert.Equal(t, filePath1, members[1])
+	assert.Equal(t, filePath3, members[2]) // Should be last due to no timestamps
+}
+
+func TestFileProcessor_ProcessFile(t *testing.T) {
+	// Create temporary files
+	tmpfile1, err := ioutil.TempFile("", "example1")
+	if err != nil {
+		t.Fatal(err)
+	}
+	defer os.Remove(tmpfile1.Name())
+
+	tmpfile2, err := ioutil.TempFile("", "example2")
+	if err != nil {
+		t.Fatal(err)
+	}
+	defer os.Remove(tmpfile2.Name())
+
+	// Write different data to the files
+	if _, err := tmpfile1.Write([]byte("hello world")); err != nil {
+		t.Fatal(err)
+	}
+	if _, err := tmpfile2.Write([]byte("hello universe")); err != nil {
+		t.Fatal(err)
+	}
+
+	if err := tmpfile1.Close(); err != nil {
+		t.Fatal(err)
+	}
+	if err := tmpfile2.Close(); err != nil {
+		t.Fatal(err)
+	}
+
+	mr, err := miniredis.Run()
+	if err != nil {
+		t.Fatal(err)
+	}
+	defer mr.Close()
+
+	rdb := redis.NewClient(&redis.Options{
+		Addr: mr.Addr(),
+	})
+	ctx := context.Background()
+
+	fp := NewFileProcessor(rdb, ctx)
+
+	// Mock the hash calculation functions
+	fp.calculateFileHashFunc = func(path string, limit int64) (string, error) {
+		if path == tmpfile1.Name() {
+			return "mockhash1", nil
+		}
+		return "mockhash2", nil
+	}
+
+	// Process both files
+	err = fp.ProcessFile(tmpfile1.Name())
+	assert.NoError(t, err)
+	err = fp.ProcessFile(tmpfile2.Name())
+	assert.NoError(t, err)
+
+	// Verify the data was saved correctly for both files
+	hashedKey1 := generateHash(tmpfile1.Name())
+	hashedKey2 := generateHash(tmpfile2.Name())
+
+	// Check file info
+	fileInfoData1, err := rdb.Get(ctx, "fileInfo:"+hashedKey1).Bytes()
+	assert.NoError(t, err)
+	assert.NotNil(t, fileInfoData1)
+
+	fileInfoData2, err := rdb.Get(ctx, "fileInfo:"+hashedKey2).Bytes()
+	assert.NoError(t, err)
+	assert.NotNil(t, fileInfoData2)
+
+	// Check paths
+	path1, err := rdb.Get(ctx, "hashedKeyToPath:"+hashedKey1).Result()
+	assert.NoError(t, err)
+	assert.Equal(t, tmpfile1.Name(), path1)
+
+	path2, err := rdb.Get(ctx, "hashedKeyToPath:"+hashedKey2).Result()
+	assert.NoError(t, err)
+	assert.Equal(t, tmpfile2.Name(), path2)
+
+	// Check file hashes
+	fileHash1, err := rdb.Get(ctx, "hashedKeyToFileHash:"+hashedKey1).Result()
+	assert.NoError(t, err)
+	assert.Equal(t, "mockhash1", fileHash1)
+
+	fileHash2, err := rdb.Get(ctx, "hashedKeyToFileHash:"+hashedKey2).Result()
+	assert.NoError(t, err)
+	assert.Equal(t, "mockhash2", fileHash2)
+}
+
+func TestProcessFile(t *testing.T) {
+	// 创建一个临时文件
+	tmpfile, err := ioutil.TempFile("", "example")
+	if err != nil {
+		t.Fatal(err)
+	}
+	defer os.Remove(tmpfile.Name()) // 清理
+
+	// 写入一些数据
+	if _, err := tmpfile.Write([]byte("hello world")); err != nil {
+		t.Fatal(err)
+	}
+	if err := tmpfile.Close(); err != nil {
+		t.Fatal(err)
+	}
+
+	// 创建一个 miniredis 实例
+	mr, err := miniredis.Run()
+	if err != nil {
+		t.Fatal(err)
+	}
+	defer mr.Close()
+
+	// 创建一个真实的 Redis 客户端，连接到 miniredis
+	rdb := redis.NewClient(&redis.Options{
+		Addr: mr.Addr(),
+	})
+	ctx := context.Background()
+
+	// 创建一个 FileProcessor
+	fp := NewFileProcessor(rdb, ctx)
+
+	// 保存原始的 generateHash 函数
+	originalGenerateHash := fp.generateHashFunc
+
+	// 创建一个新的 generateHash 函数
+	fp.generateHashFunc = func(s string) string {
+		return "mockedHash"
+	}
+
+	// 在测试结束时恢复原始函数
+	defer func() { fp.generateHashFunc = originalGenerateHash }()
+
+	// 模拟 calculateFileHash 方法
+	calculateFileHashCalls := 0
+	fp.calculateFileHashFunc = func(path string, limit int64) (string, error) {
+		calculateFileHashCalls++
+		if calculateFileHashCalls == 1 {
+			return "file_hash", nil
+		}
+		return "full_hash", nil
+	}
+
+	// 处理文件
+	err = fp.ProcessFile(tmpfile.Name())
+	if err != nil {
+		t.Errorf("ProcessFile() error = %v", err)
+	}
+
+	// 验证 Redis 中的数据
+	fileInfoData, err := rdb.Get(ctx, "fileInfo:mockedHash").Bytes()
+	assert.NoError(t, err)
+
+	var storedFileInfo FileInfo
+	err = decodeGob(fileInfoData, &storedFileInfo)
+	assert.NoError(t, err)
+
+	assert.Equal(t, int64(11), storedFileInfo.Size) // "hello world" 的长度
+
+	// 验证其他 Redis 键
+	assert.Equal(t, tmpfile.Name(), rdb.Get(ctx, "hashedKeyToPath:mockedHash").Val())
+	assert.True(t, rdb.SIsMember(ctx, "fileHashToPathSet:file_hash", tmpfile.Name()).Val())
+	assert.Equal(t, "full_hash", rdb.Get(ctx, "hashedKeyToFullHash:mockedHash").Val())
+	assert.Equal(t, "mockedHash", rdb.Get(ctx, "pathToHashedKey:"+tmpfile.Name()).Val())
+	assert.Equal(t, "file_hash", rdb.Get(ctx, "hashedKeyToFileHash:mockedHash").Val())
+
+	// 验证 calculateFileHash 被调用了两次
+	assert.Equal(t, 2, calculateFileHashCalls)
+}
+
+func TestExtractTimestamps(t *testing.T) {
+	tests := []struct {
+		name     string
+		filePath string
+		want     []string
+	}{
+		{
+			"Multiple timestamps",
+			"/Users/huangyingw/mini/media/usb_backup_crypt_8T_1/cartoon/dragonball/第一部/龙珠 第一部 日语配音/七龙珠146.mp4:24:30,:1:11:27,1:40:56,:02:35:52",
+			[]string{"24:30", "01:11:27", "01:40:56", "02:35:52"},
+		},
+		{
+			"Timestamps with different formats",
+			"/Users/huangyingw/mini/media/usb_backup_crypt_8T_1/cartoon/dragonball/第一部/龙珠 第一部 日语配音/七龙珠146.rmvb:24:30,1:11:27,:02:35:52",
+			[]string{"24:30", "01:11:27", "02:35:52"},
+		},
+		{
+			"Short timestamps",
+			"/Users/huangyingw/mini/media/usb_backup_crypt_8T_1/cartoon/dragonball/第一部/龙珠 第一部 日语配音/七龙珠146.mp4:02:43,07:34,10:26",
+			[]string{"02:43", "07:34", "10:26"},
+		},
+		{
+			"Many timestamps",
+			"/Users/huangyingw/mini/media/usb_backup_crypt_8T_1/cartoon/dragonball/第一部/龙珠 第一部 日语配音/七龙珠146.mp4:24:30,:1:11:27,1:40:56,:02:35:52,:02:36:03,:2:39:25,:2:43:06,:2:48:24,:2:53:16,:3:08:41,:3:58:08,:4:00:38,5:12:14,5:24:58,5:36:54,5:41:01,:6:16:21,:6:20:03",
+			[]string{"24:30", "01:11:27", "01:40:56", "02:35:52", "02:36:03", "02:39:25", "02:43:06", "02:48:24", "02:53:16", "03:08:41", "03:58:08", "04:00:38", "05:12:14", "05:24:58", "05:36:54", "05:41:01", "06:16:21", "06:20:03"},
+		},
+		{
+			"Timestamps in folder names",
+			"/Users/huangyingw/mini/media/usb_backup_crypt_8T_1/cartoon/dragonball/第一部:24:30,/龙珠 第一部 日语配音:1:11:27,/七龙珠146.mp4:1:40:56,/更多文件:02:35:52",
+			[]string{"24:30", "01:11:27", "01:40:56", "02:35:52"},
+		},
+		{
+			"Mixed format timestamps in path",
+			"/path/to/video/24:30,15:24,/subfolder:1:11:27,3:45,1:7/anotherfolder:02:35:52,/finalfile.mp4:03:45",
+			[]string{"01:07", "03:45", "15:24", "24:30", "01:11:27", "02:35:52"},
+		},
+	}
+
+	for _, tt := range tests {
+		t.Run(tt.name, func(t *testing.T) {
+			got := ExtractTimestamps(tt.filePath)
+			assert.Equal(t, tt.want, got)
+		})
+	}
+}
+
+func TestSaveDuplicateFileInfoToRedis(t *testing.T) {
+	rdb, mock := redismock.NewClientMock()
+	ctx := context.Background()
+	fp := NewFileProcessor(rdb, ctx)
+
+	fullHash := "testhash"
+	info := FileInfo{Size: 1000, ModTime: time.Unix(1620000000, 0)}
+	filePath := "/path/to/file_12:34:56.txt"
+
+	expectedScore := float64(-(1*timestampWeight + len(filepath.Base(filePath))))
+
+	mock.ExpectZAdd("duplicateFiles:"+fullHash, &redis.Z{
+		Score:  expectedScore,
+		Member: filePath,
+	}).SetVal(1)
+
+	err := fp.SaveDuplicateFileInfoToRedis(fullHash, info, filePath)
+	assert.NoError(t, err)
+
+	assert.NoError(t, mock.ExpectationsWereMet())
+}
+
+// Add more tests for other functions...
+
+func TestParseTimestamp(t *testing.T) {
+	tests := []struct {
+		input    string
+		expected timestamp
+	}{
+		{"12:34", timestamp{12, 34, 0}},
+		{"01:23:45", timestamp{1, 23, 45}},
+		{"00:00:01", timestamp{0, 0, 1}},
+	}
+
+	for _, test := range tests {
+		result := parseTimestamp(test.input)
+		assert.Equal(t, test.expected, result)
+	}
+}
+
+func TestFileProcessor_GetFileInfoFromRedis(t *testing.T) {
+	mr, err := miniredis.Run()
+	if err != nil {
+		t.Fatal(err)
+	}
+	defer mr.Close()
+
+	rdb := redis.NewClient(&redis.Options{
+		Addr: mr.Addr(),
+	})
+	ctx := context.Background()
+
+	fp := NewFileProcessor(rdb, ctx)
+
+	// Prepare test data
+	testFileInfo := FileInfo{
+		Size:    1000,
+		ModTime: time.Now(),
+	}
+	var buf bytes.Buffer
+	enc := gob.NewEncoder(&buf)
+	err = enc.Encode(testFileInfo)
+	assert.NoError(t, err)
+
+	hashedKey := "testkey"
+	err = rdb.Set(ctx, "fileInfo:"+hashedKey, buf.Bytes(), 0).Err()
+	assert.NoError(t, err)
+
+	// Test getFileInfoFromRedis
+	result, err := fp.getFileInfoFromRedis(hashedKey)
+	assert.Equal(t, testFileInfo.Size, result.Size)
+	assert.WithinDuration(t, testFileInfo.ModTime, result.ModTime, time.Second)
+}
+
+func TestFileProcessor_SaveToFile(t *testing.T) {
+	mr, err := miniredis.Run()
+	if err != nil {
+		t.Fatal(err)
+	}
+	defer mr.Close()
+
+	rdb := redis.NewClient(&redis.Options{
+		Addr: mr.Addr(),
+	})
+	ctx := context.Background()
+
+	fp := NewFileProcessor(rdb, ctx)
+
+	// Prepare test data
+	testData := map[string]FileInfo{
+		"/path/to/file1": {Size: 1000, ModTime: time.Now().Add(-1 * time.Hour)},
+		"/path/to/file2": {Size: 2000, ModTime: time.Now()},
+	}
+
+	for path, info := range testData {
+		hashedKey := generateHash(path)
+		var buf bytes.Buffer
+		enc := gob.NewEncoder(&buf)
+		err = enc.Encode(info)
+		assert.NoError(t, err)
+
+		err = rdb.Set(ctx, "fileInfo:"+hashedKey, buf.Bytes(), 0).Err()
+		assert.NoError(t, err)
+		err = rdb.Set(ctx, "hashedKeyToPath:"+hashedKey, path, 0).Err()
+		assert.NoError(t, err)
+	}
+
+	// Create a temporary directory for the test
+	tempDir, err := ioutil.TempDir("", "testdir")
+	if err != nil {
+		t.Fatal(err)
+	}
+	defer os.RemoveAll(tempDir)
+
+	// Test saveToFile
+	err = fp.saveToFile(tempDir, "testfile.txt", false)
+	assert.NoError(t, err)
+
+	// Verify the file contents
+	content, err := ioutil.ReadFile(filepath.Join(tempDir, "testfile.txt"))
+	assert.NoError(t, err)
+	assert.Contains(t, string(content), "2000,\"")
+	assert.Contains(t, string(content), "1000,\"")
+	assert.Contains(t, string(content), "file2")
+	assert.Contains(t, string(content), "file1")
+}
+
+func TestFileContentVerification(t *testing.T) {
+	// 设置测试环境
+	fs := afero.NewMemMapFs()
+	tempDir, err := afero.TempDir(fs, "", "testdir")
+	assert.NoError(t, err)
+	defer fs.RemoveAll(tempDir)
+
+	mr, err := miniredis.Run()
+	if err != nil {
+		t.Fatal(err)
+	}
+	defer mr.Close()
+
+	rdb := redis.NewClient(&redis.Options{
+		Addr: mr.Addr(),
+	})
+	ctx := context.Background()
+
+	fp := NewFileProcessor(rdb, ctx)
+	fp.fs = fs
+
+	// 准备测试数据
+	fullHash := "793bf43bc5719d3deb836a2a8d38eeada28d457c48153b1e7d5af7ed5f38be98632dbad7d64f0f83d58619c6ef49d7565622d7b20119e7d2cb2540ece11ce119"
+
+	testData := []struct {
+		path     string
+		size     int64
+		modTime  time.Time
+		fullHash string
+	}{
+		{filepath.Join(tempDir, "path", "to", "file1_01:23:45.mp4"), 209720828, time.Now().Add(-1 * time.Hour), "unique_hash_1"},
+		{filepath.Join(tempDir, "path", "to", "file2_02:34:56_03:45:67.mp4"), 2172777224, time.Now(), fullHash},
+		{filepath.Join(tempDir, "path", "to", "file3.mp4"), 2172777224, time.Now().Add(-2 * time.Hour), fullHash},
+	}
+
+	// 设置 Redis 数据和创建模拟文件
+	for _, data := range testData {
+		info := FileInfo{Size: data.size, ModTime: data.modTime}
+		err = fp.SaveDuplicateFileInfoToRedis(data.fullHash, info, data.path)
+		assert.NoError(t, err)
+
+		hashedKey := fp.generateHashFunc(data.path)
+		var buf bytes.Buffer
+		enc := gob.NewEncoder(&buf)
+		err = enc.Encode(info)
+		assert.NoError(t, err)
+
+		err = rdb.Set(ctx, "fileInfo:"+hashedKey, buf.Bytes(), 0).Err()
+		assert.NoError(t, err)
+		err = rdb.Set(ctx, "hashedKeyToPath:"+hashedKey, data.path, 0).Err()
+		assert.NoError(t, err)
+		err = rdb.Set(ctx, "pathToHashedKey:"+data.path, hashedKey, 0).Err()
+		assert.NoError(t, err)
+		err = rdb.Set(ctx, "hashedKeyToFullHash:"+hashedKey, data.fullHash, 0).Err()
+		assert.NoError(t, err)
+
+		// 创建模拟文件
+		err = afero.WriteFile(fs, data.path, []byte("test content"), 0644)
+		assert.NoError(t, err)
+	}
+
+	// 测试 saveToFile 方法
+	err = fp.saveToFile(tempDir, "fav.log", false)
+	assert.NoError(t, err)
+
+	content, err := afero.ReadFile(fs, filepath.Join(tempDir, "fav.log"))
+	assert.NoError(t, err)
+
+	expectedContent := `2172777224,"./path/to/file2_02:34:56_03:45:67.mp4"
+2172777224,"./path/to/file3.mp4"
+209720828,"./path/to/file1_01:23:45.mp4"
+`
+	assert.Equal(t, expectedContent, string(content), "fav.log content does not match expected")
+
+	// Test fav.log.sort
+	err = fp.saveToFile(tempDir, "fav.log.sort", true)
+	assert.NoError(t, err)
+
+	content, err = afero.ReadFile(fs, filepath.Join(tempDir, "fav.log.sort"))
+	assert.NoError(t, err)
+
+	expectedSortedContent := `"./path/to/file2_02:34:56_03:45:67.mp4"
+"./path/to/file1_01:23:45.mp4"
+"./path/to/file3.mp4"
+`
+	assert.Equal(t, expectedSortedContent, string(content), "fav.log.sort content does not match expected")
+
+	// Test WriteDuplicateFilesToFile method
+	err = fp.WriteDuplicateFilesToFile(tempDir, "fav.log.dup", rdb, ctx)
+	assert.NoError(t, err)
+
+	content, err = afero.ReadFile(fs, filepath.Join(tempDir, "fav.log.dup"))
+	assert.NoError(t, err)
+
+	expectedDupContent := fmt.Sprintf(`Duplicate files for fullHash %s:
+[+] 2172777224,"./path/to/file2_02:34:56_03:45:67.mp4"
+[-] 2172777224,"./path/to/file3.mp4"
+	`, fullHash)
+
+	assert.Equal(t, expectedDupContent, string(content), "fav.log.dup content does not match expected")
+}
+
+func setupTestData(rdb *redis.Client, ctx context.Context, rootDir string) error {
+	testData := []struct {
+		path     string
+		size     int64
+		modTime  time.Time
+		fullHash string
+	}{
+		{"/path/to/file1_01:23:45.mp4", 1000, time.Now().Add(-1 * time.Hour), "hash1"},
+		{"/path/to/file2_02:34:56_03:45:67.mp4", 2000, time.Now(), "hash1"},
+		{"/path/to/file3.mp4", 3000, time.Now().Add(-2 * time.Hour), "hash1"},
+	}
+
+	for _, data := range testData {
+		hashedKey := generateHash(data.path)
+		fileInfo := FileInfo{Size: data.size, ModTime: data.modTime}
+
+		var buf bytes.Buffer
+		enc := gob.NewEncoder(&buf)
+		if err := enc.Encode(fileInfo); err != nil {
+			return err
+		}
+
+		pipe := rdb.Pipeline()
+		pipe.Set(ctx, "fileInfo:"+hashedKey, buf.Bytes(), 0)
+		pipe.Set(ctx, "hashedKeyToPath:"+hashedKey, data.path, 0)
+		pipe.Set(ctx, "pathToHashedKey:"+data.path, hashedKey, 0)
+		pipe.Set(ctx, "hashedKeyToFullHash:"+hashedKey, data.fullHash, 0)
+		pipe.ZAdd(ctx, "duplicateFiles:"+data.fullHash, &redis.Z{Score: float64(-data.size), Member: data.path})
+
+		_, err := pipe.Exec(ctx)
+		if err != nil {
+			return err
+		}
+	}
+
+	return nil
+}
+
+func TestGetFileInfoFromRedis(t *testing.T) {
+	mr, err := miniredis.Run()
+	if err != nil {
+		t.Fatal(err)
+	}
+	defer mr.Close()
+
+	rdb := redis.NewClient(&redis.Options{
+		Addr: mr.Addr(),
+	})
+	ctx := context.Background()
+
+	fp := NewFileProcessor(rdb, ctx)
+
+	// Prepare test data
+	testPath := "/path/to/testfile.txt"
+	testInfo := FileInfo{
+		Size:    1024,
+		ModTime: time.Now(),
+	}
+
+	hashedKey := generateHash(testPath)
+	var buf bytes.Buffer
+	enc := gob.NewEncoder(&buf)
+	err = enc.Encode(testInfo)
+	assert.NoError(t, err)
+
+	err = rdb.Set(ctx, "fileInfo:"+hashedKey, buf.Bytes(), 0).Err()
+	assert.NoError(t, err)
+	err = rdb.Set(ctx, "hashedKeyToPath:"+hashedKey, testPath, 0).Err()
+	assert.NoError(t, err)
+	err = rdb.Set(ctx, "pathToHashedKey:"+testPath, hashedKey, 0).Err()
+	assert.NoError(t, err)
+
+	// Test getFileInfoFromRedis
+	retrievedInfo, err := fp.getFileInfoFromRedis(hashedKey)
+	assert.NoError(t, err)
+	assert.Equal(t, testInfo.Size, retrievedInfo.Size)
+	assert.Equal(t, testInfo.ModTime.Unix(), retrievedInfo.ModTime.Unix())
+}
+
+func TestCleanUpOldRecords(t *testing.T) {
+	mr, err := miniredis.Run()
+	if err != nil {
+		t.Fatal(err)
+	}
+	defer mr.Close()
+
+	rdb := redis.NewClient(&redis.Options{
+		Addr: mr.Addr(),
+	})
+	ctx := context.Background()
+
+	// 创建临时目录
+	tempDir, err := ioutil.TempDir("", "testcleanup")
+	if err != nil {
+		t.Fatal(err)
+	}
+	defer os.RemoveAll(tempDir)
+
+	// 准备测试数据
+	existingFile := filepath.Join(tempDir, "existing.txt")
+	nonExistingFile := filepath.Join(tempDir, "non_existing.txt")
+
+	// 创建存在的文件
+	_, err = os.Create(existingFile)
+	if err != nil {
+		t.Fatal(err)
+	}
+
+	for _, path := range []string{existingFile, nonExistingFile} {
+		hashedKey := generateHash(path)
+		err = rdb.Set(ctx, "pathToHashedKey:"+path, hashedKey, 0).Err()
+		assert.NoError(t, err)
+		err = rdb.Set(ctx, "hashedKeyToPath:"+hashedKey, path, 0).Err()
+		assert.NoError(t, err)
+		err = rdb.Set(ctx, "fileInfo:"+hashedKey, "dummy_data", 0).Err()
+		assert.NoError(t, err)
+		err = rdb.Set(ctx, "hashedKeyToFileHash:"+hashedKey, "dummy_hash", 0).Err()
+		assert.NoError(t, err)
+	}
+
+	// 运行 cleanUpOldRecords
+	err = cleanUpOldRecords(rdb, ctx)
+	assert.NoError(t, err)
+
+	// 检查不存在文件的记录是否被删除
+	_, err = rdb.Get(ctx, "pathToHashedKey:"+nonExistingFile).Result()
+	assert.Error(t, err)
+	assert.Equal(t, redis.Nil, err)
+
+	// 检查存在文件的记录是否被保留
+	val, err := rdb.Get(ctx, "pathToHashedKey:"+existingFile).Result()
+	assert.NoError(t, err)
+	assert.NotEmpty(t, val)
+}
+
+func mockSaveFileInfoToRedis(fp *FileProcessor, path string, info FileInfo, fileHash, fullHash string) error {
+	hashedKey := fp.generateHashFunc(path)
+
+	var buf bytes.Buffer
+	enc := gob.NewEncoder(&buf)
+	if err := enc.Encode(info); err != nil {
+		return fmt.Errorf("error encoding file info: %w", err)
+	}
+
+	ctx := fp.Ctx
+	pipe := fp.Rdb.Pipeline()
+	pipe.SetNX(ctx, "fileInfo:"+hashedKey, buf.Bytes(), 0)
+	pipe.Set(ctx, "hashedKeyToPath:"+hashedKey, path, 0)
+	pipe.SAdd(ctx, "fileHashToPathSet:"+fileHash, path)
+	pipe.Set(ctx, "hashedKeyToFullHash:"+hashedKey, fullHash, 0)
+	pipe.Set(ctx, "pathToHashedKey:"+path, hashedKey, 0)
+	pipe.Set(ctx, "hashedKeyToFileHash:"+hashedKey, fileHash, 0)
+
+	_, err := pipe.Exec(ctx)
+	if err != nil {
+		return fmt.Errorf("error executing Redis pipeline: %w", err)
+	}
+	return nil
+}
+
+func TestProcessFileBoundary(t *testing.T) {
+	mr, err := miniredis.Run()
+	if err != nil {
+		t.Fatal(err)
+	}
+	defer mr.Close()
+
+	rdb := redis.NewClient(&redis.Options{
+		Addr: mr.Addr(),
+	})
+	ctx := context.Background()
+
+	fs := afero.NewMemMapFs()
+	fp := NewFileProcessor(rdb, ctx)
+	fp.fs = fs
+
+	// 确保所有必要的函数都被初始化
+	fp.generateHashFunc = generateHash
+	fp.calculateFileHashFunc = func(path string, limit int64) (string, error) {
+		if limit == FullFileReadCmd {
+			return "full_hash_large_file", nil
+		}
+		return "partial_hash_large_file", nil
+	}
+
+	// 创建一个新的方法来模拟 ProcessFile 的行为
+	mockProcessFile := func(path string) error {
+		info, err := fs.Stat(path)
+		if err != nil {
+			return fmt.Errorf("error getting file info: %w", err)
+		}
+
+		fileHash, err := fp.calculateFileHashFunc(path, ReadLimit)
+		if err != nil {
+			return fmt.Errorf("error calculating file hash: %w", err)
+		}
+
+		fullHash, err := fp.calculateFileHashFunc(path, FullFileReadCmd)
+		if err != nil {
+			return fmt.Errorf("error calculating full file hash: %w", err)
+		}
+
+		fileInfo := FileInfo{
+			Size:    info.Size(),
+			ModTime: info.ModTime(),
+		}
+
+		err = mockSaveFileInfoToRedis(fp, path, fileInfo, fileHash, fullHash)
+		if err != nil {
+			return fmt.Errorf("error saving file info to Redis: %w", err)
+		}
+
+		return nil
+	}
+
+	// 测试空文件
+	emptyFilePath := "/path/to/empty_file.txt"
+	_, err = fs.Create(emptyFilePath)
+	assert.NoError(t, err)
+
+	err = mockProcessFile(emptyFilePath)
+	assert.NoError(t, err)
+
+	// 测试大文件（模拟）
+	largeFilePath := "/path/to/large_file.bin"
+	err = afero.WriteFile(fs, largeFilePath, []byte("large file content"), 0644)
+	assert.NoError(t, err)
+
+	err = mockProcessFile(largeFilePath)
+	assert.NoError(t, err)
+
+	// 验证大文件是否被正确处理
+	hashedKey := generateHash(largeFilePath)
+	fileHash, err := rdb.Get(ctx, "hashedKeyToFileHash:"+hashedKey).Result()
+	assert.NoError(t, err)
+	assert.Equal(t, "partial_hash_large_file", fileHash)
+
+	fullHash, err := rdb.Get(ctx, "hashedKeyToFullHash:"+hashedKey).Result()
+	assert.NoError(t, err)
+	assert.Equal(t, "full_hash_large_file", fullHash)
+}
diff --git a/main.go b/main.go
new file mode 100644
--- /dev/null
+++ ./main.go
@@ -0,0 +1,258 @@
+// main.go
+// 此文件是Go程序的主要入口点，包含了文件处理程序的核心逻辑和初始化代码。
+
+package main
+
+import (
+	"bufio"
+	"context"
+	"flag"
+	"fmt"
+	"github.com/go-redis/redis/v8"
+	"github.com/karrick/godirwalk"
+	"log"
+	"os"
+	"path/filepath"
+	"regexp"
+	"runtime"
+	"sort"
+	"sync"
+	"time"
+)
+
+var (
+	rootDir          string
+	redisAddr        string
+	workerCount      int
+	minSizeBytes     int64
+	deleteDuplicates bool
+	findDuplicates   bool
+	outputDuplicates bool
+	maxDuplicates    int
+	semaphore        chan struct{}
+)
+
+func init() {
+	flag.StringVar(&rootDir, "rootDir", "", "Root directory to start the search")
+	flag.StringVar(&redisAddr, "redisAddr", "localhost:6379", "Redis server address")
+	flag.IntVar(&workerCount, "workers", runtime.NumCPU(), "Number of worker goroutines")
+	flag.Int64Var(&minSizeBytes, "minSize", 200*1024*1024, "Minimum file size in bytes")
+	flag.BoolVar(&deleteDuplicates, "delete-duplicates", false, "Delete duplicate files")
+	flag.BoolVar(&findDuplicates, "find-duplicates", false, "Find duplicate files")
+	flag.BoolVar(&outputDuplicates, "output-duplicates", false, "Output duplicate files")
+	flag.IntVar(&maxDuplicates, "max-duplicates", 50, "Maximum number of duplicates to process")
+}
+
+func main() {
+	flag.Parse()
+
+	if rootDir == "" {
+		log.Fatal("rootDir must be specified")
+	}
+
+	ctx, cancel := context.WithCancel(context.Background())
+	defer cancel()
+
+	rdb := redis.NewClient(&redis.Options{
+		Addr: redisAddr,
+	})
+	defer rdb.Close()
+
+	semaphore = make(chan struct{}, runtime.NumCPU())
+
+	fp := NewFileProcessor(rdb, ctx)
+
+	if err := cleanUpOldRecords(rdb, ctx); err != nil {
+		log.Printf("Error cleaning up old records: %v", err)
+	}
+
+	if findDuplicates {
+		if err := findAndLogDuplicates(rootDir, rdb, ctx, maxDuplicates); err != nil {
+			log.Fatalf("Error finding duplicates: %v", err)
+		}
+		return
+	}
+
+	if outputDuplicates {
+		if err := fp.WriteDuplicateFilesToFile(rootDir, "fav.log.dup", rdb, ctx); err != nil {
+			log.Fatalf("Error writing duplicates to file: %v", err)
+		}
+		return
+	}
+
+	if deleteDuplicates {
+		if err := deleteDuplicateFiles(rootDir, rdb, ctx); err != nil {
+			log.Fatalf("Error deleting duplicate files: %v", err)
+		}
+		return
+	}
+
+	fileChan := make(chan string, workerCount)
+	var wg sync.WaitGroup
+
+	// Start worker goroutines
+	for i := 0; i < workerCount; i++ {
+		wg.Add(1)
+		go func() {
+			defer wg.Done()
+			for filePath := range fileChan {
+				if err := fp.ProcessFile(filePath); err != nil {
+					log.Printf("Error processing file %s: %v", filePath, err)
+				}
+			}
+		}()
+	}
+
+	// Start progress monitoring
+	go monitorProgress(ctx)
+
+	// Walk through files
+	err := walkFiles(rootDir, minSizeBytes, fileChan)
+	close(fileChan)
+	if err != nil {
+		log.Printf("Error walking files: %v", err)
+	}
+
+	wg.Wait()
+
+	// Save results
+	if err := fp.saveToFile(rootDir, "fav.log", false); err != nil {
+		log.Printf("Error saving to fav.log: %v", err)
+	}
+	if err := fp.saveToFile(rootDir, "fav.log.sort", true); err != nil {
+		log.Printf("Error saving to fav.log.sort: %v", err)
+	}
+
+	log.Println("Processing complete")
+}
+
+func processFavLog(filePath string, rootDir string, rdb *redis.Client, ctx context.Context) {
+	file, err := os.Open(filePath)
+	if err != nil {
+		log.Println("Error opening file:", err)
+		return
+	}
+	defer file.Close()
+
+	var fileNames, filePaths []string
+	scanner := bufio.NewScanner(file)
+	for scanner.Scan() {
+		line := scanner.Text()
+		line = regexp.MustCompile(`^\d+,`).ReplaceAllString(line, "")
+		filePaths = append(filePaths, line)
+		fileNames = append(fileNames, extractFileName(line))
+	}
+
+	// 确定工作池的大小并调用 extractKeywords
+	var stopProcessing bool
+	keywords := extractKeywords(fileNames, &stopProcessing)
+
+	closeFiles := findCloseFiles(fileNames, filePaths, keywords)
+
+	// 排序关键词
+	sort.Slice(keywords, func(i, j int) bool {
+		return len(closeFiles[keywords[i]]) > len(closeFiles[keywords[j]])
+	})
+
+	workerCount := 500
+	taskQueue, poolWg, stopFunc, _ := NewWorkerPool(workerCount, &stopProcessing)
+
+	for i, keyword := range keywords {
+		keywordFiles := closeFiles[keyword]
+		if len(keywordFiles) >= 2 {
+			poolWg.Add(1) // 在将任务发送到队列之前增加计数
+			taskQueue <- func(kw string, kf []string, idx int) Task {
+				return func() {
+					defer poolWg.Done()
+					log.Printf("Processing keyword %d of %d: %s\n", idx+1, len(keywords), kw)
+					processKeyword(kw, kf, rdb, ctx, rootDir)
+				}
+			}(keyword, keywordFiles, i)
+		}
+	}
+
+	stopFunc() // 使用停止函数来关闭任务队列
+	poolWg.Wait()
+}
+
+// 初始化Redis客户端
+func newRedisClient(ctx context.Context) *redis.Client {
+	rdb := redis.NewClient(&redis.Options{
+		Addr: "localhost:6379", // 该地址应从配置中获取
+	})
+	_, err := rdb.Ping(ctx).Result()
+	if err != nil {
+		log.Println("Error connecting to Redis:", err)
+		os.Exit(1)
+	}
+	return rdb
+}
+
+func initializeApp() (string, int64, []*regexp.Regexp, *redis.Client, context.Context, bool, bool, bool, int, error) {
+	rootDir := flag.String("rootDir", "", "Root directory to start the search")
+	deleteDuplicates := flag.Bool("delete-duplicates", false, "Delete duplicate files")
+	findDuplicates := flag.Bool("find-duplicates", false, "Find duplicate files")
+	outputDuplicates := flag.Bool("output-duplicates", false, "Output duplicate files")
+	maxDuplicates := flag.Int("max-duplicates", 50, "Maximum number of duplicates to process")
+	flag.Parse()
+
+	if *rootDir == "" {
+		return "", 0, nil, nil, nil, false, false, false, 0, fmt.Errorf("rootDir must be specified")
+	}
+
+	// Minimum file size in bytes
+	minSize := 200 // Default size is 200MB
+	minSizeBytes := int64(minSize * 1024 * 1024)
+
+	// 获取当前运行目录
+	currentDir, err := os.Getwd()
+	if err != nil {
+		log.Fatalf("Failed to get current directory: %v", err)
+	}
+
+	// 拼接当前目录和文件名
+	excludePatternsFilePath := filepath.Join(currentDir, "exclude_patterns.txt")
+
+	excludeRegexps, _ := compileExcludePatterns(excludePatternsFilePath)
+
+	// 创建 Redis 客户端
+	ctx := context.Background()
+	rdb := newRedisClient(ctx)
+
+	return *rootDir, minSizeBytes, excludeRegexps, rdb, ctx, *deleteDuplicates, *findDuplicates, *outputDuplicates, *maxDuplicates, nil
+}
+
+// walkFiles 遍历指定目录下的文件，并根据条件进行处理
+func walkFiles(rootDir string, minSizeBytes int64, fileChan chan<- string) error {
+	return godirwalk.Walk(rootDir, &godirwalk.Options{
+		Callback: func(osPathname string, de *godirwalk.Dirent) error {
+			if de.IsDir() {
+				return nil
+			}
+			info, err := os.Stat(osPathname)
+			if err != nil {
+				return fmt.Errorf("error getting file info for %s: %w", osPathname, err)
+			}
+			if info.Size() >= minSizeBytes {
+				fileChan <- osPathname
+			}
+			return nil
+		},
+		Unsorted: true,
+	})
+}
+
+func monitorProgress(ctx context.Context) {
+	ticker := time.NewTicker(5 * time.Second)
+	defer ticker.Stop()
+
+	for {
+		select {
+		case <-ctx.Done():
+			return
+		case <-ticker.C:
+			// You might want to implement a way to track progress
+			log.Println("Processing files...")
+		}
+	}
+}
diff --git a/redis_client.go b/redis_client.go
new file mode 100644
--- /dev/null
+++ ./redis_client.go
@@ -0,0 +1,172 @@
+// redis_client.go
+package main
+
+import (
+	"bytes"
+	"context"
+	"crypto/sha256"
+	"encoding/hex"
+	"fmt"
+	"github.com/go-redis/redis/v8"
+	"log"
+	"os"
+	"path/filepath" // 添加导入
+	"regexp"
+	"strings"
+)
+
+// Generate a SHA-256 hash for the given string
+func generateHash(s string) string {
+	hasher := sha256.New()
+	hasher.Write([]byte(s))
+	hash := hex.EncodeToString(hasher.Sum(nil))
+	return hash
+}
+
+// 使用给定的正则表达式匹配时间戳
+func extractTimestamp(filePath string) string {
+	// 匹配冒号或逗号后的 MM:SS 或 H:MM:SS 格式的时间戳
+	re := regexp.MustCompile(`[:,/](\d{1,2}:\d{2}(?::\d{2})?)`)
+	match := re.FindStringSubmatch(filePath)
+	if len(match) > 1 {
+		return match[1]
+	}
+	return ""
+}
+
+// 将重复文件的信息存储到 Redis
+func saveDuplicateFileInfoToRedis(rdb *redis.Client, ctx context.Context, fullHash string, info fileInfo) error {
+	// 提取文件路径中的时间戳
+	timestamp := extractTimestamp(info.path)
+	timestampLength := len(timestamp)
+	fileNameLength := len(filepath.Base(info.path))
+
+	// 计算综合分数
+	// 使用负值排序，确保时间戳长度优先，文件名长度其次
+	combinedScore := float64(-(timestampLength*1000000 + fileNameLength))
+
+	// 使用管道批量处理 Redis 命令
+	pipe := rdb.Pipeline()
+
+	// 将路径添加到有序集合 duplicateFiles:<fullHash> 中，并使用综合分数
+	pipe.ZAdd(ctx, "duplicateFiles:"+fullHash, &redis.Z{
+		Score:  combinedScore, // 综合分数
+		Member: info.path,
+	})
+
+	if _, err := pipe.Exec(ctx); err != nil {
+		return fmt.Errorf("error executing pipeline for duplicate file: %s: %w", info.path, err)
+	}
+
+	return nil
+}
+
+func saveFileInfoToRedis(rdb *redis.Client, ctx context.Context, hashedKey string, path string, buf bytes.Buffer, fileHash string, fullHash string) error {
+	// 规范化路径
+	normalizedPath := filepath.Clean(path)
+
+	// 使用管道批量处理Redis命令
+	pipe := rdb.Pipeline()
+
+	// 这里我们添加命令到管道，但不立即检查错误
+	pipe.Set(ctx, "fileInfo:"+hashedKey, buf.Bytes(), 0)
+	pipe.Set(ctx, "hashedKeyToPath:"+hashedKey, normalizedPath, 0)
+	pipe.SAdd(ctx, "fileHashToPathSet:"+fileHash, normalizedPath) // 将文件路径存储为集合
+	if fullHash != "" {
+		pipe.Set(ctx, "hashedKeyToFullHash:"+hashedKey, fullHash, 0) // 存储完整文件哈希值
+	}
+	// 存储从路径到hashedKey的映射
+	pipe.Set(ctx, "pathToHashedKey:"+normalizedPath, hashedKey, 0)
+	// 存储hashedKey到fileHash的映射
+	pipe.Set(ctx, "hashedKeyToFileHash:"+hashedKey, fileHash, 0)
+
+	if _, err := pipe.Exec(ctx); err != nil {
+		return fmt.Errorf("error executing pipeline for file: %s: %w", path, err)
+	}
+
+	// 添加日志
+	return nil
+}
+
+func cleanUpOldRecords(rdb *redis.Client, ctx context.Context) error {
+	log.Println("Starting to clean up old records")
+	iter := rdb.Scan(ctx, 0, "pathToHashedKey:*", 0).Iterator()
+	for iter.Next(ctx) {
+		pathToHashKey := iter.Val()
+
+		// 解析出文件路径
+		filePath := strings.TrimPrefix(pathToHashKey, "pathToHashedKey:")
+
+		// 检查文件是否存在
+		if _, err := os.Stat(filePath); os.IsNotExist(err) {
+			err := cleanUpRecordsByFilePath(rdb, ctx, filePath)
+			if err != nil {
+				log.Printf("Error cleaning up records for file %s: %s\n", filePath, err)
+			}
+		}
+	}
+
+	if err := iter.Err(); err != nil {
+		log.Printf("Error during iteration: %s\n", err)
+		return err
+	}
+
+	return nil
+}
+
+func cleanUpRecordsByFilePath(rdb *redis.Client, ctx context.Context, filePath string) error {
+	// 获取 hashedKey
+	hashedKey, err := rdb.Get(ctx, "pathToHashedKey:"+filePath).Result()
+	if err != nil && err != redis.Nil {
+		return fmt.Errorf("error retrieving hashedKey for path %s: %v", filePath, err)
+	}
+
+	// 获取 fileHash
+	fileHash, err := rdb.Get(ctx, "hashedKeyToFileHash:"+hashedKey).Result()
+	if err != nil {
+		return fmt.Errorf("error retrieving fileHash for key %s: %v", hashedKey, err)
+	}
+
+	// 获取 fullHash
+	fullHash, err := rdb.Get(ctx, "hashedKeyToFullHash:"+hashedKey).Result()
+	if err != nil && err != redis.Nil {
+		return fmt.Errorf("error retrieving fullHash for key %s: %v", hashedKey, err)
+	}
+
+	// 删除记录
+	pipe := rdb.TxPipeline()
+	pipe.Del(ctx, "fileInfo:"+hashedKey)            // 删除 fileInfo 相关数据
+	pipe.Del(ctx, "hashedKeyToPath:"+hashedKey)     // 删除 path 相关数据
+	pipe.Del(ctx, "pathToHashedKey:"+filePath)      // 删除从路径到 hashedKey 的映射
+	pipe.Del(ctx, "hashedKeyToFileHash:"+hashedKey) // 删除 hashedKey 到 fileHash 的映射
+	if fullHash != "" {
+		pipe.Del(ctx, "hashedKeyToFullHash:"+hashedKey)      // 删除完整文件哈希相关数据
+		pipe.ZRem(ctx, "duplicateFiles:"+fullHash, filePath) // 从 duplicateFiles 有序集合中移除路径
+	}
+	pipe.SRem(ctx, "fileHashToPathSet:"+fileHash, filePath) // 从 hash 集合中移除文件路径
+
+	_, err = pipe.Exec(ctx)
+	if err != nil {
+		return fmt.Errorf("error deleting keys for outdated record %s: %v", hashedKey, err)
+	}
+
+	log.Printf("Deleted outdated record: path=%s\n", filePath)
+	return nil
+}
+
+func cleanUpHashKeys(rdb *redis.Client, ctx context.Context, fullHash, duplicateFilesKey string) error {
+	fileHashKey := "fileHashToPathSet:" + fullHash
+
+	// 使用管道批量删除 Redis 键
+	pipe := rdb.TxPipeline()
+	pipe.Del(ctx, duplicateFilesKey)
+	pipe.Del(ctx, fileHashKey)
+
+	_, err := pipe.Exec(ctx)
+	if err != nil {
+		return fmt.Errorf("error executing pipeline for cleaning up hash keys: %w", err)
+	}
+
+	log.Printf("Cleaned up Redis keys: %s and %s\n", duplicateFilesKey, fileHashKey)
+	return nil
+}
diff --git a/utils.go b/utils.go
new file mode 100644
--- /dev/null
+++ ./utils.go
@@ -0,0 +1,577 @@
+// utils.go
+package main
+
+import (
+	"bufio"
+	"bytes"
+	"context"
+	"encoding/gob"
+	"fmt"
+	"github.com/go-redis/redis/v8"
+	"log"
+	"os"
+	"path/filepath"
+	"regexp"
+	"sort"
+	"strings"
+	"sync"
+	"time"
+)
+
+var mu sync.Mutex
+
+func loadExcludePatterns(filename string) ([]string, error) {
+	file, err := os.Open(filename)
+	if err != nil {
+		return nil, err
+	}
+	defer file.Close()
+
+	var patterns []string
+	scanner := bufio.NewScanner(file)
+	for scanner.Scan() {
+		pattern := scanner.Text()
+		patterns = append(patterns, pattern)
+	}
+	return patterns, scanner.Err()
+}
+
+func sortKeys(keys []string, data map[string]FileInfo, sortByModTime bool) {
+	if sortByModTime {
+		sort.Slice(keys, func(i, j int) bool {
+			return data[keys[i]].ModTime.After(data[keys[j]].ModTime)
+		})
+	} else {
+		sort.Slice(keys, func(i, j int) bool {
+			if data[keys[i]].Size == data[keys[j]].Size {
+				return keys[i] < keys[j]
+			}
+			return data[keys[i]].Size > data[keys[j]].Size
+		})
+	}
+}
+
+func compileExcludePatterns(filename string) ([]*regexp.Regexp, error) {
+	excludePatterns, err := loadExcludePatterns(filename)
+	if err != nil {
+		return nil, err
+	}
+
+	excludeRegexps := make([]*regexp.Regexp, len(excludePatterns))
+	for i, pattern := range excludePatterns {
+		regexPattern := strings.Replace(pattern, "*", ".*", -1)
+		excludeRegexps[i], err = regexp.Compile(regexPattern)
+		if err != nil {
+			return nil, fmt.Errorf("Invalid regex pattern '%s': %v", regexPattern, err)
+		}
+	}
+	return excludeRegexps, nil
+}
+
+func performSaveOperation(rootDir, filename string, sortByModTime bool, rdb *redis.Client, ctx context.Context) {
+	log.Printf("Starting save operation to %s\n", filepath.Join(rootDir, filename))
+	if err := saveToFile(rootDir, filename, sortByModTime, rdb, ctx); err != nil {
+		log.Printf("Error saving to %s: %s\n", filepath.Join(rootDir, filename), err)
+	} else {
+		log.Printf("Saved data to %s\n", filepath.Join(rootDir, filename))
+	}
+}
+
+func writeLinesToFile(filename string, lines []string) error {
+	file, err := os.Create(filename)
+	if err != nil {
+		return err
+	}
+	defer file.Close()
+
+	for _, line := range lines {
+		if _, err := fmt.Fprintln(file, line); err != nil {
+			return err
+		}
+	}
+	return nil
+}
+
+type fileInfo struct {
+	name      string
+	path      string
+	buf       bytes.Buffer
+	startTime int64
+	fileHash  string
+	fullHash  string
+	line      string
+	header    string
+	FileInfo  // 嵌入已有的FileInfo结构体
+}
+
+func processFileHash(rootDir string, fileHash string, filePaths []string, rdb *redis.Client, ctx context.Context, processedFullHashes map[string]bool) (int, error) {
+	fileCount := 0
+	hashes := make(map[string][]fileInfo) // 添加这行
+	for _, fullPath := range filePaths {
+		if !strings.HasPrefix(fullPath, rootDir) {
+			continue
+		}
+
+		if _, err := os.Stat(fullPath); os.IsNotExist(err) {
+			continue
+		}
+
+		semaphore <- struct{}{} // 获取一个信号量
+		relativePath, err := filepath.Rel(rootDir, fullPath)
+		if err != nil {
+			<-semaphore // 释放信号量
+			continue
+		}
+		fileName := filepath.Base(relativePath)
+
+		// 获取或计算完整文件的SHA-512哈希值
+		fullHash, err := getFullFileHash(fullPath, rdb, ctx)
+		if err != nil {
+			<-semaphore // 释放信号量
+			continue
+		}
+
+		// 计算文件的SHA-512哈希值（只读取前4KB）
+		fileHash, err := getFileHash(fullPath, rdb, ctx)
+		if err != nil {
+			<-semaphore // 释放信号量
+			continue
+		}
+
+		// 获取文件信息并编码
+		info, err := os.Stat(fullPath)
+		if err != nil {
+			<-semaphore // 释放信号量
+			continue
+		}
+
+		var buf bytes.Buffer
+		enc := gob.NewEncoder(&buf)
+		if err := enc.Encode(FileInfo{Size: info.Size(), ModTime: info.ModTime()}); err != nil {
+			<-semaphore // 释放信号量
+			continue
+		}
+
+		// 调用saveFileInfoToRedis函数来保存文件信息到Redis
+		if err := saveFileInfoToRedis(rdb, ctx, generateHash(fullPath), fullPath, buf, fileHash, fullHash); err != nil {
+			<-semaphore // 释放信号量
+			continue
+		}
+
+		infoStruct := fileInfo{
+			name:      fileName,
+			path:      fullPath,
+			buf:       buf,
+			startTime: time.Now().Unix(),
+			fileHash:  fileHash,
+			fullHash:  fullHash,
+			line:      fmt.Sprintf("%d,\"./%s\"", info.Size(), relativePath),
+			// 删除 header 字段
+			FileInfo: FileInfo{Size: info.Size(), ModTime: info.ModTime()},
+		}
+		hashes[fullHash] = append(hashes[fullHash], infoStruct)
+		fileCount++
+		<-semaphore // 释放信号量
+	}
+
+	var saveErr error
+	for fullHash, infos := range hashes {
+		if len(infos) > 1 {
+			mu.Lock()
+			if !processedFullHashes[fullHash] {
+				for _, info := range infos {
+					log.Printf("Saving duplicate file info to Redis for file: %s", info.path)
+					err := saveDuplicateFileInfoToRedis(rdb, ctx, fullHash, info)
+					if err != nil {
+						log.Printf("Error saving duplicate file info to Redis for file: %s, error: %v", info.path, err)
+						saveErr = err
+					} else {
+						log.Printf("Successfully saved duplicate file info to Redis for file: %s", info.path)
+					}
+				}
+				processedFullHashes[fullHash] = true
+			}
+			mu.Unlock()
+		}
+	}
+
+	if saveErr != nil {
+		return fileCount, saveErr
+	}
+
+	return fileCount, nil
+}
+
+// 主函数
+func findAndLogDuplicates(rootDir string, rdb *redis.Client, ctx context.Context, maxDuplicates int) error {
+	log.Println("Scanning file hashes")
+	fileHashes, err := scanFileHashes(rdb, ctx)
+	if err != nil {
+		return err
+	}
+
+	fileCount := 0
+
+	// 用于存储已处理的文件哈希
+	processedFullHashes := make(map[string]bool)
+
+	workerCount := 500
+	var stopProcessing bool
+	taskQueue, poolWg, stopFunc, _ := NewWorkerPool(workerCount, &stopProcessing)
+
+	for fileHash, filePaths := range fileHashes {
+		if len(filePaths) > 1 {
+			fileCount += 1
+			if fileCount >= maxDuplicates {
+				stopProcessing = true
+				break
+			}
+
+			semaphore <- struct{}{} // 获取一个信号量
+
+			taskQueue <- func(fileHash string, filePaths []string) Task {
+				return func() {
+					defer func() {
+						<-semaphore // 释放信号量
+					}()
+
+					if stopProcessing {
+						return
+					}
+
+					log.Printf("Processing hash %s with %d files\n", fileHash, len(filePaths)) // 添加的日志
+
+					_, err := processFileHash(rootDir, fileHash, filePaths, rdb, ctx, processedFullHashes)
+					if err != nil {
+						log.Printf("Error processing file hash %s: %s\n", fileHash, err)
+						return
+					}
+				}
+			}(fileHash, filePaths)
+		}
+	}
+
+	stopFunc()
+	poolWg.Wait()
+
+	log.Printf("Total duplicates found: %d\n", fileCount)
+
+	if fileCount == 0 {
+		return nil
+	}
+
+	return nil
+}
+
+func scanFileHashes(rdb *redis.Client, ctx context.Context) (map[string][]string, error) {
+	iter := rdb.Scan(ctx, 0, "fileHashToPathSet:*", 0).Iterator()
+	fileHashes := make(map[string][]string)
+	for iter.Next(ctx) {
+		hashKey := iter.Val()
+		fileHash := strings.TrimPrefix(hashKey, "fileHashToPathSet:")
+		duplicateFiles, err := rdb.SMembers(ctx, hashKey).Result()
+		if err != nil {
+			return nil, fmt.Errorf("error retrieving duplicate files for key %s: %w", hashKey, err)
+		}
+		// 只添加包含多个文件路径的条目
+		if len(duplicateFiles) > 1 {
+			fileHashes[fileHash] = duplicateFiles
+		}
+	}
+	if err := iter.Err(); err != nil {
+		return nil, fmt.Errorf("error during iteration: %w", err)
+	}
+	return fileHashes, nil
+}
+
+func writeDuplicateFilesToFile(rootDir string, outputFile string, rdb *redis.Client, ctx context.Context) error {
+	file, err := os.Create(filepath.Join(rootDir, outputFile))
+	if err != nil {
+		return fmt.Errorf("Error creating output file: %s", err)
+	}
+	defer file.Close()
+
+	iter := rdb.Scan(ctx, 0, "duplicateFiles:*", 0).Iterator()
+	for iter.Next(ctx) {
+		duplicateFilesKey := iter.Val()
+		fullHash := strings.TrimPrefix(duplicateFilesKey, "duplicateFiles:")
+
+		duplicateFiles, err := rdb.ZRange(ctx, duplicateFilesKey, 0, -1).Result()
+		if err != nil {
+			log.Printf("Error getting duplicate files for key %s: %v", duplicateFilesKey, err)
+			continue
+		}
+
+		if len(duplicateFiles) > 1 {
+			header := fmt.Sprintf("Duplicate files for fullHash %s:\n", fullHash)
+			if _, err := file.WriteString(header); err != nil {
+				log.Printf("Error writing header: %v", err)
+				continue
+			}
+			for i, duplicateFile := range duplicateFiles {
+				fp := NewFileProcessor(rdb, ctx)
+				hashedKey, err := fp.getHashedKeyFromPath(duplicateFile)
+				if err != nil {
+					log.Printf("Error getting hashed key for path %s: %v", duplicateFile, err)
+					continue
+				}
+				fileInfoData, err := rdb.Get(ctx, "fileInfo:"+hashedKey).Bytes()
+				if err != nil {
+					log.Printf("Error getting file info for key %s: %v", hashedKey, err)
+					continue
+				}
+
+				var fileInfo FileInfo
+				buf := bytes.NewBuffer(fileInfoData)
+				dec := gob.NewDecoder(buf)
+				if err := dec.Decode(&fileInfo); err != nil {
+					log.Printf("Error decoding file info: %v", err)
+					continue
+				}
+
+				cleanedPath := cleanRelativePath(rootDir, duplicateFile)
+				var line string
+				if i == 0 {
+					line = fmt.Sprintf("\t[+] %d,\"%s\"\n", fileInfo.Size, cleanedPath)
+				} else {
+					line = fmt.Sprintf("\t[-] %d,\"%s\"\n", fileInfo.Size, cleanedPath)
+				}
+				if _, err := file.WriteString(line); err != nil {
+					log.Printf("Error writing line: %v", err)
+					continue
+				}
+			}
+		} else {
+			log.Printf("No duplicates found for hash %s", fullHash)
+		}
+	}
+
+	if err := iter.Err(); err != nil {
+		return fmt.Errorf("error during iteration: %w", err)
+	}
+
+	return nil
+}
+
+func getFileSizeFromRedis(rdb *redis.Client, ctx context.Context, fullPath string) (int64, error) {
+	fp := NewFileProcessor(rdb, ctx)
+	hashedKey, err := fp.getHashedKeyFromPath(fullPath)
+	if err != nil {
+		return 0, fmt.Errorf("error getting hashed key for %s: %w", fullPath, err)
+	}
+
+	// 然后使用 hashedKey 从 Redis 获取文件信息
+	fileInfoData, err := rdb.Get(ctx, "fileInfo:"+hashedKey).Bytes()
+	if err != nil {
+		return 0, fmt.Errorf("error getting file info for hashed key %s: %w", hashedKey, err)
+	}
+
+	var fileInfo FileInfo
+	buf := bytes.NewBuffer(fileInfoData)
+	dec := gob.NewDecoder(buf)
+	if err := dec.Decode(&fileInfo); err != nil {
+		return 0, fmt.Errorf("error decoding file info for hashed key %s: %w", hashedKey, err)
+	}
+
+	return fileInfo.Size, nil
+}
+
+// extractFileName extracts the file name from a given file path.
+func extractFileName(filePath string) string {
+	return strings.ToLower(filepath.Base(filePath))
+}
+
+var pattern = regexp.MustCompile(`\b(?:\d{2}\.\d{2}\.\d{2}|(?:\d+|[a-z]+(?:\d+[a-z]*)?))\b`)
+
+func extractKeywords(fileNames []string, stopProcessing *bool) []string {
+	workerCount := 100
+	// 创建自己的工作池
+	taskQueue, poolWg, stopFunc, _ := NewWorkerPool(workerCount, stopProcessing)
+
+	keywordsCh := make(chan string, len(fileNames)*10) // 假设每个文件名大约有10个关键词
+
+	for _, fileName := range fileNames {
+		poolWg.Add(1) // 确保在任务开始前递增计数
+		taskQueue <- func(name string) Task {
+			return func() {
+				defer func() {
+					poolWg.Done() // 任务结束时递减计数
+				}()
+
+				nameWithoutExt := strings.TrimSuffix(name, filepath.Ext(name))
+				matches := pattern.FindAllString(nameWithoutExt, -1)
+				for _, match := range matches {
+					keywordsCh <- match
+				}
+			}
+		}(fileName)
+	}
+
+	stopFunc() // 使用停止函数来关闭任务队列
+
+	// 关闭通道的逻辑保持不变
+	go func() {
+		poolWg.Wait()
+		close(keywordsCh)
+	}()
+
+	// 收集关键词
+	keywordsMap := make(map[string]struct{})
+	for keyword := range keywordsCh {
+		keywordsMap[keyword] = struct{}{}
+	}
+
+	// 将map转换为slice
+	var keywords []string
+	for keyword := range keywordsMap {
+		keywords = append(keywords, keyword)
+	}
+
+	return keywords
+}
+
+func findCloseFiles(fileNames, filePaths, keywords []string) map[string][]string {
+	closeFiles := make(map[string][]string)
+
+	for _, kw := range keywords {
+		for i, fileName := range fileNames {
+			if strings.Contains(strings.ToLower(fileName), strings.ToLower(kw)) {
+				closeFiles[kw] = append(closeFiles[kw], filePaths[i])
+			}
+		}
+	}
+
+	return closeFiles
+}
+
+func deleteDuplicateFiles(rootDir string, rdb *redis.Client, ctx context.Context) error {
+	iter := rdb.Scan(ctx, 0, "duplicateFiles:*", 0).Iterator()
+	for iter.Next(ctx) {
+		duplicateFilesKey := iter.Val()
+
+		// 获取 fullHash
+		fullHash := strings.TrimPrefix(duplicateFilesKey, "duplicateFiles:")
+
+		// 获取重复文件列表
+		duplicateFiles, err := rdb.ZRange(ctx, duplicateFilesKey, 0, -1).Result()
+		if err != nil {
+			continue
+		}
+
+		if len(duplicateFiles) > 1 {
+			// 保留第一个文件（你可以根据自己的需求修改保留策略）
+			fileToKeep := duplicateFiles[0]
+
+			// 检查第一个文件是否存在
+			if _, err := os.Stat(fileToKeep); os.IsNotExist(err) {
+				continue
+			}
+
+			filesToDelete := duplicateFiles[1:]
+
+			for _, duplicateFile := range filesToDelete {
+				// 先从 Redis 中删除相关记录
+				err := cleanUpRecordsByFilePath(rdb, ctx, duplicateFile)
+				if err != nil {
+					continue
+				}
+
+				// 然后删除文件
+				err = os.Remove(duplicateFile)
+				if err != nil {
+					continue
+				}
+			}
+
+			// 清理 Redis 键
+			err = cleanUpHashKeys(rdb, ctx, fullHash, duplicateFilesKey)
+			if err != nil {
+			}
+		}
+	}
+
+	if err := iter.Err(); err != nil {
+		return err
+	}
+
+	return nil
+}
+
+func shouldStopDuplicateFileSearch(duplicateCount int, maxDuplicateFiles int) bool {
+	return duplicateCount >= maxDuplicateFiles
+}
+
+func saveToFile(rootDir, filename string, sortByModTime bool, rdb *redis.Client, ctx context.Context) error {
+	iter := rdb.Scan(ctx, 0, "fileInfo:*", 0).Iterator()
+	data := make(map[string]FileInfo)
+
+	for iter.Next(ctx) {
+		hashedKey := strings.TrimPrefix(iter.Val(), "fileInfo:")
+
+		originalPath, err := rdb.Get(ctx, "hashedKeyToPath:"+hashedKey).Result()
+		if err != nil {
+			log.Printf("Error getting original path for key %s: %v", hashedKey, err)
+			continue
+		}
+
+		fileInfoData, err := rdb.Get(ctx, "fileInfo:"+hashedKey).Bytes()
+		if err != nil {
+			log.Printf("Error getting file info for key %s: %v", hashedKey, err)
+			continue
+		}
+
+		var fileInfo FileInfo
+		buf := bytes.NewBuffer(fileInfoData)
+		dec := gob.NewDecoder(buf)
+		if err := dec.Decode(&fileInfo); err != nil {
+			log.Printf("Error decoding file info for key %s: %v", hashedKey, err)
+			continue
+		}
+
+		data[originalPath] = fileInfo
+	}
+
+	if err := iter.Err(); err != nil {
+		return fmt.Errorf("error iterating over Redis keys: %w", err)
+	}
+
+	if len(data) == 0 {
+		return nil
+	}
+
+	return writeDataToFile(rootDir, filename, data, sortByModTime)
+}
+
+func writeDataToFile(rootDir, filename string, data map[string]FileInfo, sortByModTime bool) error {
+	var lines []string
+	keys := make([]string, 0, len(data))
+	for k := range data {
+		keys = append(keys, k)
+	}
+
+	sortKeys(keys, data, sortByModTime)
+
+	for _, k := range keys {
+		relativePath, err := filepath.Rel(rootDir, k)
+		if err != nil {
+			log.Printf("Error getting relative path for %s: %v", k, err)
+			continue
+		}
+		line := formatFileInfoLine(data[k], relativePath, sortByModTime)
+		lines = append(lines, line)
+	}
+
+	return writeLinesToFile(filepath.Join(rootDir, filename), lines)
+}
+
+func formatFileInfoLine(fileInfo FileInfo, relativePath string, sortByModTime bool) string {
+	if sortByModTime {
+		return fmt.Sprintf("\"./%s\"\n", relativePath) // 添加换行符
+	}
+	return fmt.Sprintf("%d,\"./%s\"\n", fileInfo.Size, relativePath) // 添加换行符
+}
+
+// decodeGob decodes gob-encoded data into the provided interface
+func decodeGob(data []byte, v interface{}) error {
+	return gob.NewDecoder(bytes.NewReader(data)).Decode(v)
+}
diff --git a/worker_pool.go b/worker_pool.go
new file mode 100644
--- /dev/null
+++ ./worker_pool.go
@@ -0,0 +1,40 @@
+// worker_pool.go
+package main
+
+import (
+	"sync"
+)
+
+// Task 定义了工作池中的任务类型
+type Task func()
+
+func NewWorkerPool(workerCount int, stopProcessing *bool) (chan<- Task, *sync.WaitGroup, func(), chan struct{}) {
+	var wg sync.WaitGroup
+	taskQueue := make(chan Task)
+	stopSignal := make(chan struct{})
+
+	for i := 0; i < workerCount; i++ {
+		wg.Add(1)
+		go func() {
+			defer wg.Done()
+			for {
+				select {
+				case <-stopSignal:
+					return
+				case task, ok := <-taskQueue:
+					if !ok || *stopProcessing {
+						return
+					}
+					task()
+				}
+			}
+		}()
+	}
+
+	stopFunc := func() {
+		close(stopSignal) // 发送停止信号
+		close(taskQueue)  // 关闭任务队列
+	}
+
+	return taskQueue, &wg, stopFunc, stopSignal
+}
