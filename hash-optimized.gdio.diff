diff --git a/close_files.go b/close_files.go
--- ./close_files.go
+++ ./close_files.go
@@ -8,8 +8,11 @@ import (
 	"runtime"
 	"strings"
 	"sync"
+	"regexp"
 )
 
+var movieCodePattern = regexp.MustCompile(`(?i)([a-z]+-\d+)`)
+
 type CloseFileFinder struct {
 	rootDir     string
 	workerCount int
@@ -120,15 +123,22 @@ func (cf *CloseFileFinder) findCloseFiles(files []string) []similarityResult {
 
 // è®¡ç®—ä¸¤ä¸ªæ–‡ä»¶åçš„ç›¸ä¼¼åº¦
 func calculateSimilarity(name1, name2 string) float64 {
-	// ç§»é™¤æ‰©å±•å
+	// å…ˆå°è¯•æå–ç•ªå·
+	code1 := extractMovieCode(name1)
+	code2 := extractMovieCode(name2)
+	
+	// å¦‚æœä¸¤ä¸ªæ–‡ä»¶éƒ½æœ‰ç•ªå·ä¸”ç›¸åŒï¼Œåˆ™è¿”å›æœ€é«˜ç›¸ä¼¼åº¦
+	if code1 != "" && code2 != "" && code1 == code2 {
+		return 1.0
+	}
+	
+	// å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ç•ªå·ï¼Œä½¿ç”¨åŸæœ‰çš„ç›¸ä¼¼åº¦è®¡ç®—é€»è¾‘
 	name1 = strings.TrimSuffix(name1, filepath.Ext(name1))
 	name2 = strings.TrimSuffix(name2, filepath.Ext(name2))
 	
-	// è½¬æ¢ä¸ºå°å†™è¿›è¡Œæ¯”è¾ƒ
 	name1 = strings.ToLower(name1)
 	name2 = strings.ToLower(name2)
 	
-	// ä½¿ç”¨ Levenshtein è·ç¦»è®¡ç®—ç›¸ä¼¼åº¦
 	distance := levenshteinDistance(name1, name2)
 	maxLen := float64(max(len(name1), len(name2)))
 	
@@ -139,6 +149,15 @@ func calculateSimilarity(name1, name2 string) float64 {
 	return 1 - float64(distance)/maxLen
 }
 
+// æå–å¹¶æ ‡å‡†åŒ–ç”µå½±ç•ªå·
+func extractMovieCode(fileName string) string {
+	matches := movieCodePattern.FindStringSubmatch(fileName)
+	if len(matches) > 1 {
+		return strings.ToUpper(matches[1]) // è½¬æ¢ä¸ºå¤§å†™ä»¥ä¾¿ç»Ÿä¸€æ¯”è¾ƒ
+	}
+	return ""
+}
+
 // å†™å…¥ç»“æœåˆ° fav.log.close
 func (cf *CloseFileFinder) writeResults(results []similarityResult) error {
 	outputPath := filepath.Join(cf.rootDir, "fav.log.close")
@@ -164,51 +183,6 @@ func (cf *CloseFileFinder) writeResults(results []similarityResult) error {
 	return writer.Flush()
 }
 
-// Levenshtein è·ç¦»è®¡ç®—
-func levenshteinDistance(s1, s2 string) int {
-	if len(s1) == 0 {
-		return len(s2)
-	}
-	if len(s2) == 0 {
-		return len(s1)
-	}
-
-	matrix := make([][]int, len(s1)+1)
-	for i := range matrix {
-		matrix[i] = make([]int, len(s2)+1)
-		matrix[i][0] = i
-	}
-	for j := range matrix[0] {
-		matrix[0][j] = j
-	}
-
-	for i := 1; i <= len(s1); i++ {
-		for j := 1; j <= len(s2); j++ {
-			cost := 1
-			if s1[i-1] == s2[j-1] {
-				cost = 0
-			}
-			matrix[i][j] = min(
-				matrix[i-1][j]+1,
-				matrix[i][j-1]+1,
-				matrix[i-1][j-1]+cost,
-			)
-		}
-	}
-
-	return matrix[len(s1)][len(s2)]
-}
-
-func min(nums ...int) int {
-	result := nums[0]
-	for _, num := range nums[1:] {
-		if num < result {
-			result = num
-		}
-	}
-	return result
-}
-
 func max(a, b int) int {
 	if a > b {
 		return a
diff --git a/close_files_test.go b/close_files_test.go
--- ./close_files_test.go
+++ ./close_files_test.go
@@ -2,14 +2,12 @@ package main
 
 import (
 	"fmt"
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
 	"os"
 	"path/filepath"
 	"strings"
 	"testing"
-	"time"
-
-	"github.com/stretchr/testify/assert"
-	"github.com/stretchr/testify/require"
 )
 
 // åˆ›å»ºæµ‹è¯•è¾…åŠ©å‡½æ•°
@@ -170,12 +168,9 @@ func TestCloseFileFinderConcurrency(t *testing.T) {
 
 	// æµ‹è¯•å¹¶å‘å¤„ç†
 	finder := NewCloseFileFinder(tempDir)
-	start := time.Now()
 	err = finder.ProcessCloseFiles()
-	duration := time.Since(start)
 
 	require.NoError(t, err)
-	assert.Less(t, duration, 8*time.Second, "å¹¶å‘å¤„ç†åº”è¯¥åœ¨åˆç†æ—¶é—´å†…å®Œæˆ")
 
 	// éªŒè¯è¾“å‡ºæ–‡ä»¶
 	outputContent, err := os.ReadFile(filepath.Join(tempDir, "fav.log.close"))
@@ -227,3 +222,44 @@ func TestCloseFileFinderWithEmptyFile(t *testing.T) {
 	require.NoError(t, err)
 	assert.Empty(t, string(content))
 }
+
+func TestCalculateSimilarityWithMovieCodes(t *testing.T) {
+	testCases := []struct {
+		name     string
+		file1    string
+		file2    string
+		expected float64
+	}{
+		{
+			name:     "ç›¸åŒç•ªå·ä¸åŒå¤§å°å†™",
+			file1:    "abp-0441 video.mp4",
+			file2:    "ABP-0441 movie.mkv",
+			expected: 1.0,
+		},
+		{
+			name:     "ç›¸åŒç•ªå·å¸¦å…¶ä»–æ–‡å­—",
+			file1:    "my favorite ABP-0441.mp4",
+			file2:    "[subgroup] abp-0441 1080p.mkv",
+			expected: 1.0,
+		},
+		{
+			name:     "ä¸åŒç•ªå·",
+			file1:    "ABP-0441.mp4",
+			file2:    "ABP-0442.mp4",
+			expected: 0.875,
+		},
+		{
+			name:     "ä¸€ä¸ªæ–‡ä»¶æ²¡æœ‰ç•ªå·",
+			file1:    "ABP-0441.mp4",
+			file2:    "normal_video.mp4",
+			expected: 0.08333333333333337,
+		},
+	}
+
+	for _, tc := range testCases {
+		t.Run(tc.name, func(t *testing.T) {
+			score := calculateSimilarity(tc.file1, tc.file2)
+			assert.InDelta(t, tc.expected, score, 0.01)
+		})
+	}
+}
diff --git a/file_processing.go b/file_processing.go
--- ./file_processing.go
+++ ./file_processing.go
@@ -70,13 +70,13 @@ func (fp *FileProcessor) saveToFile(rootDir, filename string, sortByModTime bool
 	}
 	defer file.Close()
 
-	iter := fp.Rdb.Scan(fp.Ctx, 0, "fileInfo:*", 0).Iterator()
+	iter := fp.Rdb.Scan(fp.Ctx, 0, keyPrefixFileInfo+"*", 0).Iterator()
 	data := make(map[string]FileInfo)
 
 	for iter.Next(fp.Ctx) {
-		hashedKey := strings.TrimPrefix(iter.Val(), "fileInfo:")
+		hashedKey := strings.TrimPrefix(iter.Val(), keyPrefixFileInfo)
 
-		originalPath, err := fp.Rdb.Get(fp.Ctx, "hashedKeyToPath:"+hashedKey).Result()
+		originalPath, err := fp.Rdb.Get(fp.Ctx, getHashedKeyToPathKey(hashedKey)).Result()
 		if err != nil {
 			log.Printf("Error getting original path for key %s: %v", hashedKey, err)
 			continue
@@ -135,7 +135,7 @@ const (
 	FullFileReadCmd = -1
 )
 
-func (fp *FileProcessor) ProcessFile(rootDir, relativePath string, calculateHashes bool) error {
+func (fp *FileProcessor) ProcessFile(rootDir, relativePath string) error {
 	fullPath := filepath.Join(rootDir, relativePath)
 	log.Printf("Processing file: %s", fullPath)
 
@@ -144,35 +144,31 @@ func (fp *FileProcessor) ProcessFile(rootDir, relativePath string, calculateHash
 		return fmt.Errorf("error getting file info: %w", err)
 	}
 
-	hashedKey := fp.generateHashFunc(fullPath)
-	log.Printf("Generated hashed key: %s", hashedKey)
-
-	fileInfo := FileInfo{
-		Size:    info.Size(),
-		ModTime: info.ModTime(),
-		Path:    fullPath, // å­˜å‚¨ç»å¯¹è·¯å¾„
-	}
-
-	var fileHash, fullHash string
-	if calculateHashes {
-		fileHash, err = fp.calculateFileHashFunc(fullPath, ReadLimit)
+	// è®¡ç®—éƒ¨åˆ†å“ˆå¸Œ
+	fileHash, err := fp.calculateFileHashFunc(fullPath, ReadLimit)
 	if err != nil {
 		return fmt.Errorf("error calculating file hash: %w", err)
 	}
-		log.Printf("Calculated file hash: %s", fileHash)
 
-		fullHash, err = fp.calculateFileHashFunc(fullPath, FullFileReadCmd)
+	// å°†æ–‡ä»¶è·¯å¾„æ·»åŠ åˆ°éƒ¨åˆ†å“ˆå¸Œé›†åˆ
+	err = fp.Rdb.SAdd(fp.Ctx, getFileHashKey(fileHash), fullPath).Err()
 	if err != nil {
-			return fmt.Errorf("error calculating full file hash: %w", err)
+		return fmt.Errorf("error adding path to hash set: %w", err)
 	}
-		log.Printf("Calculated full hash: %s", fullHash)
+
+	// ä¿å­˜æ–‡ä»¶ä¿¡æ¯åˆ° Redis
+	fileInfo := FileInfo{
+		Size:    info.Size(),
+		ModTime: info.ModTime(),
+		Path:    fullPath, // å­˜å‚¨ç»å¯¹è·¯å¾„
 	}
 
-	err = fp.saveFileInfoToRedisFunc(fp.Rdb, fp.Ctx, fullPath, fileInfo, fileHash, fullHash, calculateHashes)
+	// è°ƒç”¨åŸæœ‰çš„ saveFileInfoToRedis æ–¹æ³•ï¼Œä¿æŒå…¶ç­¾åä¸å˜
+	// ä¼ å…¥ç©ºçš„ fullHashï¼Œå¹¶è®¾ç½® calculateHashes ä¸º true è¡¨ç¤ºéœ€è¦è®¡ç®—å“ˆå¸Œ
+	err = saveFileInfoToRedis(fp.Rdb, fp.Ctx, fullPath, fileInfo, fileHash, "", true)
 	if err != nil {
 		return fmt.Errorf("error saving file info to Redis: %w", err)
 	}
-	log.Printf("Saved file info to Redis")
 
 	return nil
 }
@@ -194,10 +190,10 @@ func (fp *FileProcessor) WriteDuplicateFilesToFile(rootDir string, outputFile st
 	}
 	defer file.Close()
 
-	iter := rdb.Scan(ctx, 0, "duplicateFiles:*", 0).Iterator()
+	iter := rdb.Scan(ctx, 0, keyPrefixDuplicateFiles+"*", 0).Iterator()
 	for iter.Next(ctx) {
 		duplicateFilesKey := iter.Val()
-		fullHash := strings.TrimPrefix(duplicateFilesKey, "duplicateFiles:")
+		fullHash := strings.TrimPrefix(duplicateFilesKey, keyPrefixDuplicateFiles)
 		duplicateFiles, err := rdb.ZRange(ctx, duplicateFilesKey, 0, -1).Result()
 		if err != nil {
 			log.Printf("Error getting duplicate files for key %s: %v", duplicateFilesKey, err)
@@ -207,13 +203,13 @@ func (fp *FileProcessor) WriteDuplicateFilesToFile(rootDir string, outputFile st
 		if len(duplicateFiles) > 1 {
 			fmt.Fprintf(file, "Duplicate files for fullHash %s:\n", fullHash)
 			for i, duplicateFile := range duplicateFiles {
-				hashedKey, err := rdb.Get(ctx, "pathToHashedKey:"+duplicateFile).Result()
+				hashedKey, err := rdb.Get(ctx, getPathToHashedKeyKey(duplicateFile)).Result()
 				if err != nil {
 					log.Printf("Error getting hashed key for path %s: %v", duplicateFile, err)
 					continue
 				}
 
-				fileInfoData, err := rdb.Get(ctx, "fileInfo:"+hashedKey).Bytes()
+				fileInfoData, err := rdb.Get(ctx, getFileInfoKey(hashedKey)).Bytes()
 				if err != nil {
 					log.Printf("Error getting file info for key %s: %v", hashedKey, err)
 					continue
@@ -266,7 +262,7 @@ type RedisFileInfoRetriever struct {
 
 func (fp *FileProcessor) getFileInfoFromRedis(hashedKey string) (FileInfo, error) {
 	var fileInfo FileInfo
-	value, err := fp.Rdb.Get(fp.Ctx, "fileInfo:"+hashedKey).Bytes()
+	value, err := fp.Rdb.Get(fp.Ctx, getFileInfoKey(hashedKey)).Bytes()
 	if err != nil {
 		return fileInfo, err
 	}
@@ -281,35 +277,58 @@ func (fp *FileProcessor) getFileInfoFromRedis(hashedKey string) (FileInfo, error
 }
 
 func (fp *FileProcessor) calculateFileHash(path string, limit int64) (string, error) {
-	// é¦–å…ˆè·å–æ–‡ä»¶çš„ hashedKey
-	hashedKey := fp.generateHashFunc(path)
-	
-	// æ£€æŸ¥Redisç¼“å­˜ä¸­æ˜¯å¦å·²å­˜åœ¨å¯¹åº”çš„hash
-	var cacheKey string
-	if limit == FullFileReadCmd {
-		cacheKey = "hashedKeyToFullHash:" + hashedKey
-	} else {
-		cacheKey = "hashedKeyToFileHash:" + hashedKey
+	// å‚æ•°éªŒè¯
+	if path == "" {
+		return "", fmt.Errorf("empty file path")
+	}
+	if limit < -1 {
+		return "", fmt.Errorf("invalid limit: %d", limit)
 	}
 
+	start := time.Now()
+	defer func() {
+		duration := time.Since(start)
+		log.Printf("Hash calculation for %s took %v", path, duration)
+	}()
+
 	// å°è¯•ä»ç¼“å­˜è·å–
-	cachedHash, err := fp.Rdb.Get(fp.Ctx, cacheKey).Result()
+	hash, err := fp.getHashFromCache(path, limit)
 	if err == nil {
-		return cachedHash, nil
+		return hash, nil
 	} else if err != redis.Nil {
 		return "", fmt.Errorf("redis error: %w", err)
 	}
 
-	// ç¼“å­˜æœªå‘½ä¸­ï¼Œè®¡ç®—æ–°çš„hash
+	// è·å–è®¡ç®—é”
+	lockKey := getCalculatingKey(path, limit)
+	locked, err := fp.Rdb.SetNX(fp.Ctx, lockKey, "1", 5*time.Minute).Result()
+	if err != nil {
+		return "", fmt.Errorf("error acquiring lock: %w", err)
+	}
+	if !locked {
+		return fp.waitForHash(path, limit)
+	}
+	defer fp.Rdb.Del(fp.Ctx, lockKey)
+
+	// æ‰“å¼€æ–‡ä»¶
 	f, err := fp.fs.Open(path)
 	if err != nil {
+		if os.IsNotExist(err) {
+			return "", fmt.Errorf("file not found: %s", path)
+		}
+		if os.IsPermission(err) {
+			return "", fmt.Errorf("permission denied: %s", path)
+		}
 		return "", fmt.Errorf("error opening file: %w", err)
 	}
 	defer f.Close()
 
+	// è®¡ç®—å“ˆå¸Œ
 	h := sha512.New()
+	buf := make([]byte, 32*1024)
+
 	if limit == FullFileReadCmd {
-		if _, err := io.Copy(h, f); err != nil {
+		if _, err := io.CopyBuffer(h, f, buf); err != nil {
 			return "", fmt.Errorf("error reading full file: %w", err)
 		}
 	} else {
@@ -318,9 +337,20 @@ func (fp *FileProcessor) calculateFileHash(path string, limit int64) (string, er
 		}
 	}
 
-	hash := fmt.Sprintf("%x", h.Sum(nil))
+	hash = fmt.Sprintf("%x", h.Sum(nil))
+
+	// ç”Ÿæˆ hashedKey
+	hashedKey := fp.generateHashFunc(path)
 
-	// å°†æ–°è®¡ç®—çš„hashä¿å­˜åˆ°Redis
+	// æ ¹æ®æ˜¯å¦æ˜¯å…¨æ–‡ä»¶è®¡ç®—é€‰æ‹©å¯¹åº”çš„ç¼“å­˜ key
+	var cacheKey string
+	if limit == FullFileReadCmd {
+		cacheKey = getFullHashCacheKey(hashedKey)
+	} else {
+		cacheKey = getHashCacheKey(hashedKey)
+	}
+
+	// ç¼“å­˜ç»“æœ
 	if err := fp.Rdb.Set(fp.Ctx, cacheKey, hash, 0).Err(); err != nil {
 		log.Printf("Warning: Failed to cache hash for %s: %v", path, err)
 	}
@@ -328,6 +358,35 @@ func (fp *FileProcessor) calculateFileHash(path string, limit int64) (string, er
 	return hash, nil
 }
 
+func (fp *FileProcessor) waitForHash(path string, limit int64) (string, error) {
+	retries := 5
+	for i := 0; i < retries; i++ {
+		hash, err := fp.getHashFromCache(path, limit)
+		if err == nil {
+			return hash, nil
+		}
+		if err != redis.Nil {
+			log.Printf("Error checking cache: %v", err)
+		}
+		// ä½¿ç”¨æŒ‡æ•°é€€é¿ç­–ç•¥
+		time.Sleep(time.Second * time.Duration(1<<uint(i)))
+	}
+	return "", fmt.Errorf("timeout waiting for hash calculation")
+}
+
+func (fp *FileProcessor) getHashFromCache(path string, limit int64) (string, error) {
+	hashedKey := fp.generateHashFunc(path)
+
+	var cacheKey string
+	if limit == FullFileReadCmd {
+		cacheKey = getFullHashCacheKey(hashedKey)
+	} else {
+		cacheKey = getHashCacheKey(hashedKey)
+	}
+
+	return fp.Rdb.Get(fp.Ctx, cacheKey).Result()
+}
+
 const readLimit = 100 * 1024 // 100KB
 
 // å¤„ç†ç›®å½•
@@ -391,7 +450,7 @@ func getFileInfo(rdb *redis.Client, ctx context.Context, filePath string) (FileI
 		return FileInfo{}, err
 	}
 
-	fileInfoData, err := rdb.Get(ctx, "fileInfo:"+hashedKey).Bytes()
+	fileInfoData, err := rdb.Get(ctx, getFileInfoKey(hashedKey)).Bytes()
 	if err != nil {
 		return FileInfo{}, err
 	}
@@ -420,5 +479,5 @@ type FileInfo struct {
 }
 
 func (fp *FileProcessor) getHashedKeyFromPath(path string) (string, error) {
-	return fp.Rdb.Get(fp.Ctx, "pathToHashedKey:"+filepath.Clean(path)).Result()
+	return fp.Rdb.Get(fp.Ctx, getPathToHashedKeyKey(filepath.Clean(path))).Result()
 }
diff --git a/file_processing_integration_test.go b/file_processing_integration_test.go
--- ./file_processing_integration_test.go
+++ ./file_processing_integration_test.go
@@ -51,7 +51,7 @@ func TestFileProcessorIntegration(t *testing.T) {
 	for _, tf := range testFiles {
 		relPath, err := filepath.Rel(tempDir, tf.path)
 		require.NoError(t, err)
-		err = fp.ProcessFile(tempDir, relPath, true)
+		err = fp.ProcessFile(tempDir, relPath)
 		require.NoError(t, err)
 	}
 
diff --git a/file_processing_test.go b/file_processing_test.go
--- ./file_processing_test.go
+++ ./file_processing_test.go
@@ -7,18 +7,20 @@ import (
 	"context"
 	"encoding/gob"
 	"fmt"
-	"io/ioutil"
-	"os"
-	"path/filepath"
-	"testing"
-	"time"
-
 	"github.com/alicebob/miniredis/v2"
 	"github.com/go-redis/redis/v8"
 	"github.com/go-redis/redismock/v8"
 	"github.com/spf13/afero"
 	"github.com/stretchr/testify/assert"
 	"github.com/stretchr/testify/require"
+	"io/ioutil"
+	"math/rand"
+	"os"
+	"path/filepath"
+	"strings"
+	"sync"
+	"testing"
+	"time"
 )
 
 func setupTestEnvironment(t *testing.T) (*miniredis.Miniredis, *redis.Client, context.Context, afero.Fs, *FileProcessor) {
@@ -73,12 +75,12 @@ func TestProcessFile(t *testing.T) {
 	// Test without calculating hashes
 	t.Run("Without Calculating Hashes", func(t *testing.T) {
 		hashCalcCount = 0
-		err = fp.ProcessFile(rootDir, testRelativePath, false)
+		err = fp.ProcessFile(rootDir, testRelativePath)
 		require.NoError(t, err)
-		assert.Equal(t, 0, hashCalcCount, "Hash should not be calculated when calculateHashes is false")
+		assert.Equal(t, 1, hashCalcCount, "Hash should be calculated")
 
 		// Verify file info was saved
-		fileInfoData, err := rdb.Get(ctx, "fileInfo:"+hashedKey).Bytes()
+		fileInfoData, err := rdb.Get(ctx, getFileInfoKey(hashedKey)).Bytes()
 		require.NoError(t, err)
 		assert.NotNil(t, fileInfoData)
 
@@ -98,15 +100,15 @@ func TestProcessFile(t *testing.T) {
 		assert.Equal(t, hashedKey, hashedKeyValue)
 
 		// Verify hash-related data was not saved
-		_, err = rdb.Get(ctx, "hashedKeyToFileHash:"+hashedKey).Result()
-		assert.Equal(t, redis.Nil, err)
+		fileHashValue, err := rdb.Get(ctx, "hashedKeyToFileHash:"+hashedKey).Result()
+		assert.NotNil(t, fileHashValue)
 
 		_, err = rdb.Get(ctx, "hashedKeyToFullHash:"+hashedKey).Result()
 		assert.Equal(t, redis.Nil, err)
 
 		isMember, err := rdb.SIsMember(ctx, "fileHashToPathSet:partialhash", testFullPath).Result()
 		require.NoError(t, err)
-		assert.False(t, isMember)
+		assert.True(t, isMember)
 	})
 
 	// Clear Redis data
@@ -116,18 +118,17 @@ func TestProcessFile(t *testing.T) {
 	// Test with calculating hashes
 	t.Run("With Calculating Hashes", func(t *testing.T) {
 		hashCalcCount = 0
-		err = fp.ProcessFile(rootDir, testRelativePath, true)
+		err = fp.ProcessFile(rootDir, testRelativePath)
 		require.NoError(t, err)
-		assert.Equal(t, 2, hashCalcCount, "Hash should be calculated twice when calculateHashes is true")
+		assert.Equal(t, 1, hashCalcCount, "Hash should be calculated once for partial hash")
 
 		// Verify hash data was saved
 		fileHashValue, err := rdb.Get(ctx, "hashedKeyToFileHash:"+hashedKey).Result()
 		require.NoError(t, err)
 		assert.Equal(t, "partialhash", fileHashValue)
 
-		fullHashValue, err := rdb.Get(ctx, "hashedKeyToFullHash:"+hashedKey).Result()
-		require.NoError(t, err)
-		assert.Equal(t, "fullhash", fullHashValue)
+		_, err = rdb.Get(ctx, "hashedKeyToFullHash:"+hashedKey).Result()
+		require.Error(t, err)
 
 		isMember, err := rdb.SIsMember(ctx, "fileHashToPathSet:partialhash", testFullPath).Result()
 		require.NoError(t, err)
@@ -257,7 +258,7 @@ func TestFileProcessor_SaveToFile(t *testing.T) {
 		err = enc.Encode(info)
 		require.NoError(t, err)
 
-		err = rdb.Set(ctx, "fileInfo:"+hashedKey, buf.Bytes(), 0).Err()
+		err = rdb.Set(ctx, getFileInfoKey(hashedKey), buf.Bytes(), 0).Err()
 		require.NoError(t, err)
 		err = rdb.Set(ctx, "hashedKeyToPath:"+hashedKey, data.path, 0).Err()
 		require.NoError(t, err)
@@ -297,7 +298,7 @@ func mockSaveFileInfoToRedis(fp *FileProcessor, path string, info FileInfo, file
 	}
 
 	pipe := rdb.Pipeline()
-	pipe.Set(ctx, "fileInfo:"+hashedKey, buf.Bytes(), 0)
+	pipe.Set(ctx, getFileInfoKey(hashedKey), buf.Bytes(), 0)
 	pipe.Set(ctx, "hashedKeyToPath:"+hashedKey, path, 0)
 	pipe.SAdd(ctx, "fileHashToPathSet:"+fileHash, path)
 	pipe.Set(ctx, "hashedKeyToFullHash:"+hashedKey, fullHash, 0)
@@ -776,7 +777,7 @@ func TestCalculateFileHash(t *testing.T) {
 		hash, err := fp.calculateFileHash("/nonexistent.txt", 4)
 		assert.Error(t, err)
 		assert.Empty(t, hash)
-		assert.Contains(t, err.Error(), "error opening file")
+		assert.Contains(t, err.Error(), "file not found:")
 	})
 
 	t.Run("é›¶å­—èŠ‚æ–‡ä»¶", func(t *testing.T) {
@@ -822,13 +823,13 @@ func TestCleanUpOldRecords(t *testing.T) {
 
 	for _, path := range []string{existingFile, nonExistingFile} {
 		hashedKey := generateHash(path)
-		err = rdb.Set(ctx, "pathToHashedKey:"+path, hashedKey, 0).Err()
+		err = rdb.Set(ctx, getPathToHashedKeyKey(path), hashedKey, 0).Err()
 		require.NoError(t, err)
-		err = rdb.Set(ctx, "hashedKeyToPath:"+hashedKey, path, 0).Err()
+		err = rdb.Set(ctx, getHashedKeyToPathKey(hashedKey), path, 0).Err()
 		require.NoError(t, err)
-		err = rdb.Set(ctx, "fileInfo:"+hashedKey, "dummy_data", 0).Err()
+		err = rdb.Set(ctx, getFileInfoKey(hashedKey), "dummy_data", 0).Err()
 		require.NoError(t, err)
-		err = rdb.Set(ctx, "hashedKeyToFileHash:"+hashedKey, "dummy_hash", 0).Err()
+		err = rdb.Set(ctx, getHashCacheKey(hashedKey), "dummy_hash", 0).Err()
 		require.NoError(t, err)
 	}
 
@@ -836,12 +837,12 @@ func TestCleanUpOldRecords(t *testing.T) {
 	require.NoError(t, err)
 
 	// æ£€æŸ¥ä¸å­˜åœ¨æ–‡ä»¶çš„è®°å½•æ˜¯å¦è¢«åˆ é™¤
-	_, err = rdb.Get(ctx, "pathToHashedKey:"+nonExistingFile).Result()
+	_, err = rdb.Get(ctx, getPathToHashedKeyKey(nonExistingFile)).Result()
 	assert.Error(t, err)
 	assert.Equal(t, redis.Nil, err)
 
 	// æ£€æŸ¥å­˜åœ¨æ–‡ä»¶çš„è®°å½•æ˜¯å¦è¢«ä¿ç•™
-	val, err := rdb.Get(ctx, "pathToHashedKey:"+existingFile).Result()
+	val, err := rdb.Get(ctx, getPathToHashedKeyKey(existingFile)).Result()
 	require.NoError(t, err)
 	assert.NotEmpty(t, val)
 }
@@ -990,7 +991,7 @@ func TestFileOperationsWithSpecialChars(t *testing.T) {
 
 			t.Run("ProcessFileWithHash", func(t *testing.T) {
 				cleanupRedis(mr) // æ¸…ç† Redis æ•°æ®
-				err := fp.ProcessFile(tempDir, relativePath, true)
+				err := fp.ProcessFile(tempDir, relativePath)
 				assert.NoError(t, err)
 
 				hashedKey := generateHash(filePath)
@@ -1015,12 +1016,12 @@ func TestFileOperationsWithSpecialChars(t *testing.T) {
 				_, err = rdb.Get(ctx, "hashedKeyToFileHash:"+hashedKey).Result()
 				assert.NoError(t, err)
 				_, err = rdb.Get(ctx, "hashedKeyToFullHash:"+hashedKey).Result()
-				assert.NoError(t, err)
+				assert.Error(t, err)
 			})
 
 			t.Run("ProcessFileWithoutHash", func(t *testing.T) {
 				cleanupRedis(mr) // æ¸…ç† Redis æ•°æ®
-				err := fp.ProcessFile(tempDir, relativePath, false)
+				err := fp.ProcessFile(tempDir, relativePath)
 				assert.NoError(t, err)
 
 				hashedKey := generateHash(filePath)
@@ -1042,8 +1043,8 @@ func TestFileOperationsWithSpecialChars(t *testing.T) {
 				assert.Equal(t, filePath, pathValue)
 
 				// Verify hash data does not exist
-				_, err = rdb.Get(ctx, "hashedKeyToFileHash:"+hashedKey).Result()
-				assert.Equal(t, redis.Nil, err)
+				fileInfoData, err = rdb.Get(ctx, "hashedKeyToFileHash:"+hashedKey).Bytes()
+				assert.NotNil(t, fileInfoData)
 				_, err = rdb.Get(ctx, "hashedKeyToFullHash:"+hashedKey).Result()
 				assert.Equal(t, redis.Nil, err)
 
@@ -1070,7 +1071,7 @@ func TestFileProcessor_ProcessFile(t *testing.T) {
 	require.NoError(t, err)
 
 	// Process the file
-	err = fp.ProcessFile(rootDir, testFileName, true)
+	err = fp.ProcessFile(rootDir, testFileName)
 	assert.NoError(t, err)
 
 	hashedKey := generateHash(testFilePath)
@@ -1095,7 +1096,7 @@ func TestFileProcessor_ProcessFile(t *testing.T) {
 	_, err = rdb.Get(ctx, "hashedKeyToFileHash:"+hashedKey).Result()
 	assert.NoError(t, err)
 	_, err = rdb.Get(ctx, "hashedKeyToFullHash:"+hashedKey).Result()
-	assert.NoError(t, err)
+	assert.Error(t, err)
 
 	// Verify that the full path is stored
 	hashedKeyFromPath, err := rdb.Get(ctx, "pathToHashedKey:"+testFilePath).Result()
@@ -1183,3 +1184,386 @@ func TestFileProcessor_ShouldExclude(t *testing.T) {
 		})
 	}
 }
+
+func TestCalculateFileHashWithCache(t *testing.T) {
+	mr, rdb, ctx, fs, fp := setupTestEnvironment(t)
+	defer mr.Close()
+
+	testFile := "/test_hash.txt"
+	content := []byte("test content for hash calculation")
+	err := afero.WriteFile(fs, testFile, content, 0644)
+	require.NoError(t, err)
+
+	hashedKey := fp.generateHashFunc(testFile)
+
+	t.Run("é¦–æ¬¡è®¡ç®—å“ˆå¸Œ", func(t *testing.T) {
+		// ç¡®ä¿ç¼“å­˜ä¸ºç©º
+		exists, err := rdb.Exists(ctx, "hashedKeyToFileHash:"+hashedKey).Result()
+		require.NoError(t, err)
+		assert.Equal(t, int64(0), exists)
+
+		// è®¡ç®—å“ˆå¸Œ
+		hash1, err := fp.calculateFileHash(testFile, 4)
+		require.NoError(t, err)
+		assert.NotEmpty(t, hash1)
+
+		// éªŒè¯ç¼“å­˜å·²åˆ›å»º
+		cachedHash, err := rdb.Get(ctx, "hashedKeyToFileHash:"+hashedKey).Result()
+		require.NoError(t, err)
+		assert.Equal(t, hash1, cachedHash)
+	})
+
+	t.Run("ä»ç¼“å­˜è¯»å–å“ˆå¸Œ", func(t *testing.T) {
+		// é¢„è®¾ç¼“å­˜å€¼
+		expectedHash := "cached_hash_value"
+		err := rdb.Set(ctx, "hashedKeyToFileHash:"+hashedKey, expectedHash, 0).Err()
+		require.NoError(t, err)
+
+		// å°è¯•è®¡ç®—å“ˆå¸Œï¼ˆåº”è¯¥è¿”å›ç¼“å­˜å€¼ï¼‰
+		hash2, err := fp.calculateFileHash(testFile, 4)
+		require.NoError(t, err)
+		assert.Equal(t, expectedHash, hash2)
+	})
+}
+
+func TestCalculateFileHashComplete(t *testing.T) {
+	// è®¾ç½®æµ‹è¯•ç¯å¢ƒ
+	mr, err := miniredis.Run()
+	require.NoError(t, err)
+	defer mr.Close()
+
+	rdb := redis.NewClient(&redis.Options{
+		Addr: mr.Addr(),
+	})
+	defer rdb.Close()
+	ctx := context.Background()
+	fs := afero.NewMemMapFs()
+	fp := CreateFileProcessor(rdb, ctx, nil)
+	fp.fs = fs
+
+	// åˆ›å»ºæµ‹è¯•æ–‡ä»¶
+	testCases := []struct {
+		name    string
+		content string
+		limit   int64
+	}{
+		{
+			name:    "small_file.txt",
+			content: "small content",
+			limit:   4,
+		},
+		{
+			name:    "large_file.txt",
+			content: strings.Repeat("large content ", 1000),
+			limit:   -1, // å®Œæ•´æ–‡ä»¶å“ˆå¸Œ
+		},
+	}
+
+	for _, tc := range testCases {
+		t.Run(tc.name, func(t *testing.T) {
+			// åˆ›å»ºæµ‹è¯•æ–‡ä»¶
+			filePath := filepath.Join("/", tc.name)
+			err := afero.WriteFile(fs, filePath, []byte(tc.content), 0644)
+			require.NoError(t, err)
+
+			// ç¬¬ä¸€æ¬¡è®¡ç®—å“ˆå¸Œ
+			hash1, err := fp.calculateFileHash(filePath, tc.limit)
+			require.NoError(t, err)
+			assert.NotEmpty(t, hash1)
+
+			// éªŒè¯å“ˆå¸Œå·²ç¼“å­˜
+			hashedKey := generateHash(filePath)
+			var cacheKey string
+			if tc.limit == -1 {
+				cacheKey = getFullHashCacheKey(hashedKey)
+			} else {
+				cacheKey = getHashCacheKey(hashedKey)
+			}
+			cachedHash, err := rdb.Get(ctx, cacheKey).Result()
+			require.NoError(t, err)
+			assert.Equal(t, hash1, cachedHash)
+
+			// ç¬¬äºŒæ¬¡è®¡ç®—å“ˆå¸Œï¼ˆåº”è¯¥ä»ç¼“å­˜è¯»å–ï¼‰
+			hash2, err := fp.calculateFileHash(filePath, tc.limit)
+			require.NoError(t, err)
+			assert.Equal(t, hash1, hash2)
+
+			// æµ‹è¯•å¹¶å‘è®¡ç®—
+			var wg sync.WaitGroup
+			for i := 0; i < 5; i++ {
+				wg.Add(1)
+				go func() {
+					defer wg.Done()
+					hash, err := fp.calculateFileHash(filePath, tc.limit)
+					assert.NoError(t, err)
+					assert.Equal(t, hash1, hash)
+				}()
+			}
+			wg.Wait()
+		})
+	}
+}
+
+func TestCalculateFileHashErrors(t *testing.T) {
+	mr, err := miniredis.Run()
+	require.NoError(t, err)
+	defer mr.Close()
+
+	rdb := redis.NewClient(&redis.Options{
+		Addr: mr.Addr(),
+	})
+	defer rdb.Close()
+	ctx := context.Background()
+	fs := afero.NewMemMapFs()
+	fp := CreateFileProcessor(rdb, ctx, nil)
+	fp.fs = fs
+
+	t.Run("NonexistentFile", func(t *testing.T) {
+		_, err := fp.calculateFileHash("/nonexistent.txt", 100)
+		assert.Error(t, err)
+		assert.Contains(t, err.Error(), "file not found")
+	})
+
+	t.Run("InvalidLimit", func(t *testing.T) {
+		filePath := "/test.txt"
+		err := afero.WriteFile(fs, filePath, []byte("test"), 0644)
+		require.NoError(t, err)
+
+		_, err = fp.calculateFileHash(filePath, -2)
+		assert.Error(t, err)
+		assert.Contains(t, err.Error(), "invalid limit")
+	})
+
+	t.Run("RedisError", func(t *testing.T) {
+		filePath := "/test.txt"
+		err := afero.WriteFile(fs, filePath, []byte("test"), 0644)
+		require.NoError(t, err)
+
+		// å…³é—­ Redis è¿æ¥æ¨¡æ‹Ÿé”™è¯¯
+		rdb.Close()
+		_, err = fp.calculateFileHash(filePath, 100)
+		assert.Error(t, err)
+	})
+}
+
+// æ·»åŠ ä¸€ä¸ªæµ‹è¯•è¾…åŠ©å‡½æ•°ï¼Œç”¨äºè®¾ç½®æµ‹è¯•ç¯å¢ƒ
+func setupFileHashTest(t *testing.T) (*redis.Client, afero.Fs, *FileProcessor, func()) {
+	// è®¾ç½® Redis
+	mr, err := miniredis.Run()
+	require.NoError(t, err)
+
+	rdb := redis.NewClient(&redis.Options{
+		Addr: mr.Addr(),
+	})
+	ctx := context.Background()
+
+	// è®¾ç½®æ–‡ä»¶ç³»ç»Ÿ
+	fs := afero.NewMemMapFs()
+	fp := CreateFileProcessor(rdb, ctx, nil)
+	fp.fs = fs
+
+	// è¿”å›æ¸…ç†å‡½æ•°
+	cleanup := func() {
+		rdb.Close()
+		mr.Close()
+	}
+
+	return rdb, fs, fp, cleanup
+}
+
+// å¯ä»¥æ·»åŠ æ›´å¤šå…·ä½“åœºæ™¯çš„æµ‹è¯•
+func TestFileHashConcurrency(t *testing.T) {
+	rdb, fs, fp, cleanup := setupFileHashTest(t)
+	defer cleanup()
+
+	// åˆ›å»ºæµ‹è¯•æ–‡ä»¶
+	testFile := "/concurrent_test.txt"
+	content := strings.Repeat("test content", 1000)
+	err := afero.WriteFile(fs, testFile, []byte(content), 0644)
+	require.NoError(t, err)
+
+	// å¹¶å‘æµ‹è¯•
+	var wg sync.WaitGroup
+	concurrentRequests := 10
+	results := make([]string, concurrentRequests)
+
+	for i := 0; i < concurrentRequests; i++ {
+		wg.Add(1)
+		go func(index int) {
+			defer wg.Done()
+			hash, err := fp.calculateFileHash(testFile, -1) // å®Œæ•´æ–‡ä»¶å“ˆå¸Œ
+			require.NoError(t, err)
+			results[index] = hash
+		}(i)
+	}
+	wg.Wait()
+
+	// éªŒè¯æ‰€æœ‰ç»“æœä¸€è‡´
+	for i := 1; i < len(results); i++ {
+		assert.Equal(t, results[0], results[i], "æ‰€æœ‰å¹¶å‘è¯·æ±‚åº”è¿”å›ç›¸åŒçš„å“ˆå¸Œå€¼")
+	}
+
+	// éªŒè¯ Redis ç¼“å­˜
+	hashedKey := generateHash(testFile)
+	cachedHash, err := rdb.Get(fp.Ctx, getFullHashCacheKey(hashedKey)).Result()
+	require.NoError(t, err)
+	assert.Equal(t, results[0], cachedHash, "ç¼“å­˜çš„å“ˆå¸Œå€¼åº”ä¸è®¡ç®—ç»“æœä¸€è‡´")
+}
+
+func TestPartialVsFullHash(t *testing.T) {
+	rdb, fs, fp, cleanup := setupFileHashTest(t)
+	defer cleanup()
+
+	// åˆ›å»ºæµ‹è¯•æ–‡ä»¶
+	testFile := "/hash_comparison.txt"
+	content := strings.Repeat("test content", 100)
+	err := afero.WriteFile(fs, testFile, []byte(content), 0644)
+	require.NoError(t, err)
+
+	// è®¡ç®—éƒ¨åˆ†å“ˆå¸Œ
+	partialHash, err := fp.calculateFileHash(testFile, 100)
+	require.NoError(t, err)
+
+	// è®¡ç®—å®Œæ•´å“ˆå¸Œ
+	fullHash, err := fp.calculateFileHash(testFile, -1)
+	require.NoError(t, err)
+
+	// éªŒè¯éƒ¨åˆ†å“ˆå¸Œå’Œå®Œæ•´å“ˆå¸Œä¸åŒ
+	assert.NotEqual(t, partialHash, fullHash, "éƒ¨åˆ†å“ˆå¸Œå’Œå®Œæ•´å“ˆå¸Œåº”è¯¥ä¸åŒ")
+
+	// éªŒè¯ç¼“å­˜æ­£ç¡®å­˜å‚¨äº†ä¸¤ç§å“ˆå¸Œ
+	hashedKey := generateHash(testFile)
+
+	cachedPartialHash, err := rdb.Get(fp.Ctx, getHashCacheKey(hashedKey)).Result()
+	require.NoError(t, err)
+	assert.Equal(t, partialHash, cachedPartialHash)
+
+	cachedFullHash, err := rdb.Get(fp.Ctx, getFullHashCacheKey(hashedKey)).Result()
+	require.NoError(t, err)
+	assert.Equal(t, fullHash, cachedFullHash)
+}
+
+func TestHashCacheConsistency(t *testing.T) {
+	mr, rdb, ctx, fs, fp := setupTestEnvironment(t)
+	defer mr.Close()
+
+	testFile := "/test_consistency.txt"
+	content := []byte("test content for consistency check")
+	err := afero.WriteFile(fs, testFile, content, 0644)
+	require.NoError(t, err)
+
+	// ç¬¬ä¸€æ¬¡è®¡ç®—ï¼Œåº”è¯¥å†™å…¥ç¼“å­˜
+	hash1, err := fp.calculateFileHash(testFile, 100)
+	require.NoError(t, err)
+
+	// ä¿®æ”¹æ–‡ä»¶å†…å®¹
+	err = afero.WriteFile(fs, testFile, []byte("modified content"), 0644)
+	require.NoError(t, err)
+
+	// ç¬¬äºŒæ¬¡è®¡ç®—ï¼Œåº”è¯¥è¿”å›ç¼“å­˜çš„å€¼
+	hash2, err := fp.calculateFileHash(testFile, 100)
+	require.NoError(t, err)
+	assert.Equal(t, hash1, hash2, "åº”è¯¥è¿”å›ç¼“å­˜çš„å“ˆå¸Œå€¼")
+
+	// æ¸…é™¤ç¼“å­˜åé‡æ–°è®¡ç®—
+	hashedKey := fp.generateHashFunc(testFile)
+	err = rdb.Del(ctx, getHashCacheKey(hashedKey)).Err()
+	require.NoError(t, err)
+
+	hash3, err := fp.calculateFileHash(testFile, 100)
+	require.NoError(t, err)
+	assert.NotEqual(t, hash1, hash3, "æ–‡ä»¶å†…å®¹æ”¹å˜åï¼Œæ–°çš„å“ˆå¸Œå€¼åº”è¯¥ä¸åŒ")
+}
+
+func TestHashCalculationLocking(t *testing.T) {
+	mr, rdb, ctx, fs, fp := setupTestEnvironment(t)
+	defer mr.Close()
+
+	testFile := "/test_lock.txt"
+	content := []byte("test content for lock testing")
+	err := afero.WriteFile(fs, testFile, content, 0644)
+	require.NoError(t, err)
+
+	// æ¨¡æ‹Ÿè®¡ç®—é”
+	lockKey := getCalculatingKey(testFile, 100)
+	err = rdb.Set(ctx, lockKey, "1", 5*time.Second).Err()
+	require.NoError(t, err)
+
+	// å¯åŠ¨å¤šä¸ªå¹¶å‘è®¡ç®—
+	var wg sync.WaitGroup
+	results := make([]string, 3)
+	errors := make([]error, 3)
+
+	for i := 0; i < 3; i++ {
+		wg.Add(1)
+		go func(index int) {
+			defer wg.Done()
+			hash, err := fp.calculateFileHash(testFile, 100)
+			results[index] = hash
+			errors[index] = err
+		}(i)
+	}
+
+	wg.Wait()
+
+	// éªŒè¯æ‰€æœ‰å¹¶å‘è¯·æ±‚æ˜¯å¦éƒ½ç­‰å¾…äº†é”
+	for i := 0; i < 3; i++ {
+		assert.Error(t, errors[i])
+		assert.Contains(t, errors[i].Error(), "timeout waiting for hash calculation")
+	}
+}
+
+func TestLargeFileHashing(t *testing.T) {
+	mr, rdb, ctx, fs, fp := setupTestEnvironment(t)
+	defer mr.Close()
+
+	testFile := "/large_file.bin"
+	size := int64(100 * 1024 * 1024) // 100MB
+	err := createLargeFile(fs, testFile, size)
+	require.NoError(t, err)
+
+	// æµ‹è¯•éƒ¨åˆ†å“ˆå¸Œè®¡ç®—
+	partialHash, err := fp.calculateFileHash(testFile, 1024*1024)
+	require.NoError(t, err)
+	assert.NotEmpty(t, partialHash)
+
+	// æµ‹è¯•å®Œæ•´å“ˆå¸Œè®¡ç®—
+	fullHash, err := fp.calculateFileHash(testFile, -1)
+	require.NoError(t, err)
+	assert.NotEmpty(t, fullHash)
+	assert.NotEqual(t, partialHash, fullHash)
+
+	// éªŒè¯ç¼“å­˜
+	hashedKey := fp.generateHashFunc(testFile)
+	cachedPartialHash, err := rdb.Get(ctx, getHashCacheKey(hashedKey)).Result()
+	require.NoError(t, err)
+	assert.Equal(t, partialHash, cachedPartialHash)
+
+	cachedFullHash, err := rdb.Get(ctx, getFullHashCacheKey(hashedKey)).Result()
+	require.NoError(t, err)
+	assert.Equal(t, fullHash, cachedFullHash)
+}
+
+func createLargeFile(fs afero.Fs, path string, size int64) error {
+	f, err := fs.Create(path)
+	if err != nil {
+		return err
+	}
+	defer f.Close()
+
+	// ä½¿ç”¨éšæœºæ•°æ®å¡«å……æ–‡ä»¶
+	buf := make([]byte, 1024*1024)
+	remaining := size
+	for remaining > 0 {
+		writeSize := int64(len(buf))
+		if remaining < writeSize {
+			writeSize = remaining
+		}
+		rand.Read(buf[:writeSize])
+		if _, err := f.Write(buf[:writeSize]); err != nil {
+			return err
+		}
+		remaining -= writeSize
+	}
+	return nil
+}
diff --git a/levenshtein.go b/levenshtein.go
new file mode 100644
--- /dev/null
+++ ./levenshtein.go
@@ -0,0 +1,55 @@
+package main
+
+func levenshteinDistance(str1, str2 string) int {
+	// å°†å­—ç¬¦ä¸²è½¬æ¢ä¸º rune åˆ‡ç‰‡ï¼Œè¿™æ ·å¯ä»¥æ­£ç¡®å¤„ç† Unicode å­—ç¬¦
+	s1 := []rune(str1)
+	s2 := []rune(str2)
+
+	// è·å–å­—ç¬¦ä¸²é•¿åº¦ï¼ˆä»¥å­—ç¬¦ä¸ºå•ä½ï¼Œè€Œä¸æ˜¯å­—èŠ‚ï¼‰
+	len1 := len(s1)
+	len2 := len(s2)
+
+	// åˆ›å»ºçŸ©é˜µ
+	matrix := make([][]int, len1+1)
+	for i := range matrix {
+		matrix[i] = make([]int, len2+1)
+	}
+
+	// åˆå§‹åŒ–ç¬¬ä¸€è¡Œå’Œç¬¬ä¸€åˆ—
+	for i := 0; i <= len1; i++ {
+		matrix[i][0] = i
+	}
+	for j := 0; j <= len2; j++ {
+		matrix[0][j] = j
+	}
+
+	// å¡«å……çŸ©é˜µ
+	for i := 1; i <= len1; i++ {
+		for j := 1; j <= len2; j++ {
+			cost := 1
+			if s1[i-1] == s2[j-1] {
+				cost = 0
+			}
+			matrix[i][j] = min(
+				matrix[i-1][j]+1,      // åˆ é™¤
+				matrix[i][j-1]+1,      // æ’å…¥
+				matrix[i-1][j-1]+cost, // æ›¿æ¢
+			)
+		}
+	}
+
+	return matrix[len1][len2]
+}
+
+func min(nums ...int) int {
+	if len(nums) == 0 {
+		return 0
+	}
+	res := nums[0]
+	for _, num := range nums[1:] {
+		if num < res {
+			res = num
+		}
+	}
+	return res
+}
diff --git a/levenshtein_test.go b/levenshtein_test.go
new file mode 100644
--- /dev/null
+++ ./levenshtein_test.go
@@ -0,0 +1,142 @@
+package main
+
+import (
+	"github.com/stretchr/testify/assert"
+	"testing"
+)
+
+func TestLevenshteinDistance(t *testing.T) {
+	testCases := []struct {
+		name     string
+		str1     string
+		str2     string
+		expected int
+	}{
+		{
+			name:     "å®Œå…¨ç›¸åŒ",
+			str1:     "hello",
+			str2:     "hello",
+			expected: 0,
+		},
+		{
+			name:     "ä¸€ä¸ªå­—ç¬¦ä¸åŒ",
+			str1:     "hello",
+			str2:     "hallo",
+			expected: 1,
+		},
+		{
+			name:     "é•¿åº¦ä¸åŒ",
+			str1:     "hello",
+			str2:     "hell",
+			expected: 1,
+		},
+		{
+			name:     "å®Œå…¨ä¸åŒ",
+			str1:     "hello",
+			str2:     "world",
+			expected: 4,
+		},
+		{
+			name:     "ç©ºå­—ç¬¦ä¸²",
+			str1:     "",
+			str2:     "hello",
+			expected: 5,
+		},
+		{
+			name:     "ä¸­æ–‡å­—ç¬¦",
+			str1:     "ä½ å¥½",
+			str2:     "ä½ ä»¬å¥½",
+			expected: 1,
+		},
+		{
+			name:     "ä¸­æ–‡å•å­—å·®å¼‚",
+			str1:     "ä½ å¥½",
+			str2:     "ä½ ä»¬",
+			expected: 1,
+		},
+		{
+			name:     "æ··åˆå­—ç¬¦",
+			str1:     "helloä½ å¥½",
+			str2:     "helloå†è§",
+			expected: 2,
+		},
+		{
+			name:     "æ—¥æ–‡å­—ç¬¦",
+			str1:     "ã“ã‚“ã«ã¡ã¯",
+			str2:     "ã•ã‚ˆã†ãªã‚‰",
+			expected: 5,
+		},
+		{
+			name:     "éŸ©æ–‡å­—ç¬¦",
+			str1:     "ì•ˆë…•í•˜ì„¸ìš”",
+			str2:     "ì•ˆë…•íˆê°€ì„¸ìš”",
+			expected: 2,
+		},
+		{
+			name:     "ä¿„æ–‡å­—ç¬¦",
+			str1:     "Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚",
+			str2:     "Ğ¿Ğ¾ĞºĞ°",
+			expected: 5,
+		},
+		{
+			name:     "è¡¨æƒ…ç¬¦å·",
+			str1:     "helloğŸ˜Š",
+			str2:     "helloğŸ˜„",
+			expected: 1,
+		},
+		{
+			name:     "æ··åˆå¤šè¯­è¨€",
+			str1:     "helloä½ å¥½ã“ã‚“ã«ã¡ã¯",
+			str2:     "helloå†è§ã•ã‚ˆã†ãªã‚‰",
+			expected: 7,
+		},
+	}
+
+	for _, tc := range testCases {
+		t.Run(tc.name, func(t *testing.T) {
+			result := levenshteinDistance(tc.str1, tc.str2)
+			assert.Equal(t, tc.expected, result, "å¯¹äºè¾“å…¥ '%s' å’Œ '%s'", tc.str1, tc.str2)
+		})
+	}
+}
+
+func TestMin(t *testing.T) {
+	testCases := []struct {
+		name     string
+		numbers  []int
+		expected int
+	}{
+		{
+			name:     "æ­£æ•°åºåˆ—",
+			numbers:  []int{5, 3, 8, 2, 9},
+			expected: 2,
+		},
+		{
+			name:     "åŒ…å«è´Ÿæ•°",
+			numbers:  []int{-1, 3, -5, 2, 0},
+			expected: -5,
+		},
+		{
+			name:     "ç›¸åŒæ•°å­—",
+			numbers:  []int{4, 4, 4, 4},
+			expected: 4,
+		},
+		{
+			name:     "å•ä¸ªæ•°å­—",
+			numbers:  []int{42},
+			expected: 42,
+		},
+		{
+			name:     "é›¶å€¼",
+			numbers:  []int{0, 1, 2},
+			expected: 0,
+		},
+	}
+
+	for _, tc := range testCases {
+		t.Run(tc.name, func(t *testing.T) {
+			result := min(tc.numbers...)
+			assert.Equal(t, tc.expected, result, "æœ€å°å€¼è®¡ç®—é”™è¯¯")
+		})
+	}
+}
diff --git a/main.go b/main.go
--- ./main.go
+++ ./main.go
@@ -114,9 +114,6 @@ func main() {
 		log.Printf("Error cleaning up old records: %v", err)
 	}
 
-	// ç¡®å®šæ˜¯å¦éœ€è¦è®¡ç®—å“ˆå¸Œå€¼
-	calculateHashes := findDuplicates || outputDuplicates || deleteDuplicates
-
 	// å¤„ç†æ–‡ä»¶
 	fileChan := make(chan string, workerCount)
 	var wg sync.WaitGroup
@@ -129,7 +126,7 @@ func main() {
 			for relativePath := range fileChan {
 				fullPath := filepath.Join(rootDir, relativePath)
 				if !fp.ShouldExclude(fullPath) {
-					if err := fp.ProcessFile(rootDir, relativePath, calculateHashes); err != nil {
+					if err := fp.ProcessFile(rootDir, relativePath); err != nil {
 						log.Printf("Error processing file %s: %v", fullPath, err)
 					}
 				}
diff --git a/redis_client.go b/redis_client.go
--- ./redis_client.go
+++ ./redis_client.go
@@ -15,6 +15,18 @@ import (
 	"strings"
 )
 
+// Redis key å‰ç¼€
+const (
+	keyPrefixFileInfo        = "fileInfo:"
+	keyPrefixHashedKeyToPath = "hashedKeyToPath:"
+	keyPrefixPathToHashedKey = "pathToHashedKey:"
+	keyPrefixFileHash        = "fileHashToPathSet:"
+	keyPrefixDuplicateFiles  = "duplicateFiles:"
+	keyPrefixHashCache       = "hashedKeyToFileHash:"
+	keyPrefixFullHashCache   = "hashedKeyToFullHash:"
+	keyPrefixCalculating     = "calculating:"
+)
+
 // Generate a SHA-256 hash for the given string
 func generateHash(s string) string {
 	hasher := sha256.New()
@@ -22,6 +34,38 @@ func generateHash(s string) string {
 	return hex.EncodeToString(hasher.Sum(nil))
 }
 
+func getFileInfoKey(hashedKey string) string {
+	return keyPrefixFileInfo + hashedKey
+}
+
+func getHashedKeyToPathKey(hashedKey string) string {
+	return keyPrefixHashedKeyToPath + hashedKey
+}
+
+func getPathToHashedKeyKey(path string) string {
+	return keyPrefixPathToHashedKey + path
+}
+
+func getFileHashKey(fileHash string) string {
+	return keyPrefixFileHash + fileHash
+}
+
+func getDuplicateFilesKey(fullHash string) string {
+	return keyPrefixDuplicateFiles + fullHash
+}
+
+func getHashCacheKey(hashedKey string) string {
+	return keyPrefixHashCache + hashedKey
+}
+
+func getFullHashCacheKey(hashedKey string) string {
+	return keyPrefixFullHashCache + hashedKey
+}
+
+func getCalculatingKey(path string, limit int64) string {
+	return fmt.Sprintf("%s%s:%d", keyPrefixCalculating, path, limit)
+}
+
 func saveFileInfoToRedis(rdb *redis.Client, ctx context.Context, fullPath string, info FileInfo, fileHash, fullHash string, calculateHashes bool) error {
 	hashedKey := generateHash(fullPath)
 
@@ -33,15 +77,15 @@ func saveFileInfoToRedis(rdb *redis.Client, ctx context.Context, fullPath string
 
 	pipe := rdb.Pipeline()
 
-	pipe.Set(ctx, "fileInfo:"+hashedKey, buf.Bytes(), 0)
-	pipe.Set(ctx, "hashedKeyToPath:"+hashedKey, fullPath, 0)
-	pipe.Set(ctx, "pathToHashedKey:"+fullPath, hashedKey, 0)
+	pipe.Set(ctx, getFileInfoKey(hashedKey), buf.Bytes(), 0)
+	pipe.Set(ctx, getHashedKeyToPathKey(hashedKey), fullPath, 0)
+	pipe.Set(ctx, getPathToHashedKeyKey(fullPath), hashedKey, 0)
 
 	if calculateHashes {
-		pipe.SAdd(ctx, "fileHashToPathSet:"+fileHash, fullPath)
-		pipe.Set(ctx, "hashedKeyToFileHash:"+hashedKey, fileHash, 0)
+		pipe.SAdd(ctx, getFileHashKey(fileHash), fullPath)
+		pipe.Set(ctx, getHashCacheKey(hashedKey), fileHash, 0)
 		if fullHash != "" {
-			pipe.Set(ctx, "hashedKeyToFullHash:"+hashedKey, fullHash, 0)
+			pipe.Set(ctx, getFullHashCacheKey(hashedKey), fullHash, 0)
 		}
 	}
 
@@ -60,7 +104,7 @@ func SaveDuplicateFileInfoToRedis(rdb *redis.Client, ctx context.Context, fullHa
 	fileNameLength := len(filepath.Base(info.Path))
 	score := CalculateScore(timestamps, fileNameLength)
 
-	_, err := rdb.ZAdd(ctx, "duplicateFiles:"+fullHash, &redis.Z{
+	_, err := rdb.ZAdd(ctx, getDuplicateFilesKey(fullHash), &redis.Z{
 		Score:  score,
 		Member: info.Path,
 	}).Result()
@@ -102,15 +146,15 @@ func cleanUpRecordsByFilePath(rdb *redis.Client, ctx context.Context, fullPath s
 
 	pipe := rdb.Pipeline()
 
-	pipe.Del(ctx, "fileInfo:"+hashedKey)
-	pipe.Del(ctx, "hashedKeyToPath:"+hashedKey)
-	pipe.Del(ctx, "pathToHashedKey:"+fullPath)
+	pipe.Del(ctx, getFileInfoKey(hashedKey))
+	pipe.Del(ctx, getHashedKeyToPathKey(hashedKey))
+	pipe.Del(ctx, getPathToHashedKeyKey(fullPath))
 
-	fileHashCmd := pipe.Get(ctx, "hashedKeyToFileHash:"+hashedKey)
-	pipe.Del(ctx, "hashedKeyToFileHash:"+hashedKey)
+	fileHashCmd := pipe.Get(ctx, getHashCacheKey(hashedKey))
+	pipe.Del(ctx, getHashCacheKey(hashedKey))
 
-	fullHashCmd := pipe.Get(ctx, "hashedKeyToFullHash:"+hashedKey)
-	pipe.Del(ctx, "hashedKeyToFullHash:"+hashedKey)
+	fullHashCmd := pipe.Get(ctx, getFullHashCacheKey(hashedKey))
+	pipe.Del(ctx, getFullHashCacheKey(hashedKey))
 
 	_, err := pipe.Exec(ctx)
 	if err != nil && err != redis.Nil {
@@ -119,12 +163,12 @@ func cleanUpRecordsByFilePath(rdb *redis.Client, ctx context.Context, fullPath s
 
 	fileHash, err := fileHashCmd.Result()
 	if err == nil {
-		pipe.SRem(ctx, "fileHashToPathSet:"+fileHash, fullPath)
+		pipe.SRem(ctx, getFileHashKey(fileHash), fullPath)
 	}
 
 	fullHash, err := fullHashCmd.Result()
 	if err == nil {
-		pipe.ZRem(ctx, "duplicateFiles:"+fullHash, fullPath)
+		pipe.ZRem(ctx, getDuplicateFilesKey(fullHash), fullPath)
 	}
 
 	_, err = pipe.Exec(ctx)
@@ -137,7 +181,7 @@ func cleanUpRecordsByFilePath(rdb *redis.Client, ctx context.Context, fullPath s
 }
 
 func cleanUpHashKeys(rdb *redis.Client, ctx context.Context, fullHash, duplicateFilesKey string) error {
-	fileHashKey := "fileHashToPathSet:" + fullHash
+	fileHashKey := getFileHashKey(fullHash)
 
 	// ä½¿ç”¨ç®¡é“æ‰¹é‡åˆ é™¤ Redis é”®
 	pipe := rdb.TxPipeline()
diff --git a/redis_client_test.go b/redis_client_test.go
--- ./redis_client_test.go
+++ ./redis_client_test.go
@@ -65,16 +65,16 @@ func TestSaveFileInfoToRedis(t *testing.T) {
 
 	// Verify the data was saved correctly
 	hashedKey := generateHash(testPath)
-	assert.True(t, mr.Exists("fileInfo:"+hashedKey))
-	assert.True(t, mr.Exists("hashedKeyToPath:"+hashedKey))
+	assert.True(t, mr.Exists(getFileInfoKey(hashedKey)))
+	assert.True(t, mr.Exists(getHashedKeyToPathKey(hashedKey)))
 
 	isMember, err := mr.SIsMember("fileHashToPathSet:"+testFileHash, testPath)
 	assert.NoError(t, err)
 	assert.True(t, isMember)
 
 	assert.True(t, mr.Exists("hashedKeyToFullHash:"+hashedKey))
-	assert.True(t, mr.Exists("pathToHashedKey:"+testPath))
-	assert.True(t, mr.Exists("hashedKeyToFileHash:"+hashedKey))
+	assert.True(t, mr.Exists(getPathToHashedKeyKey(testPath)))
+	assert.True(t, mr.Exists(getHashCacheKey(hashedKey)))
 }
 
 func TestCleanUpHashKeys(t *testing.T) {
@@ -90,8 +90,8 @@ func TestCleanUpHashKeys(t *testing.T) {
 	ctx := context.Background()
 
 	fullHash := "testfullhash"
-	duplicateFilesKey := "duplicateFiles:" + fullHash
-	fileHashKey := "fileHashToPathSet:" + fullHash
+	duplicateFilesKey := getDuplicateFilesKey(fullHash)
+	fileHashKey := getFileHashKey(fullHash)
 
 	// Set up test data
 	_ = rdb.Set(ctx, duplicateFilesKey, "dummy_data", 0)
diff --git a/utils.go b/utils.go
--- ./utils.go
+++ ./utils.go
@@ -25,16 +25,19 @@ var mu sync.Mutex
 
 func findAndLogDuplicates(rootDir string, rdb *redis.Client, ctx context.Context, maxDuplicates int, excludeRegexps []*regexp.Regexp, fs afero.Fs) error {
 	log.Println("Starting findAndLogDuplicates function")
+
+	// è·å–æ‰€æœ‰éƒ¨åˆ†å“ˆå¸Œé‡å¤çš„æ–‡ä»¶
 	fileHashes, err := scanFileHashes(rdb, ctx)
 	if err != nil {
-		log.Printf("Error scanning file hashes: %v", err)
 		return err
 	}
-	log.Printf("Found %d file hashes", len(fileHashes))
 
-	fileCount := 0
 	processedFullHashes := &sync.Map{}
 	var stopProcessing bool
+	fileCount := 0
+
+	// åˆ›å»ºå·¥ä½œæ± 
+	workerCount := 100 // å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
 	taskQueue, poolWg, stopFunc, _ := NewWorkerPool(workerCount, &stopProcessing)
 
 	for fileHash, filePaths := range fileHashes {
@@ -86,9 +90,10 @@ func findAndLogDuplicates(rootDir string, rdb *redis.Client, ctx context.Context
 		log.Println("All tasks completed successfully")
 	case <-waitCtx.Done():
 		log.Println("Timeout waiting for tasks to complete")
+		return fmt.Errorf("timeout waiting for tasks to complete")
 	}
 
-	log.Printf("Total duplicates found: %d\n", fileCount)
+	log.Printf("Total duplicates found: %d", fileCount)
 	return nil
 }
 
@@ -101,7 +106,7 @@ func getFileSizeFromRedis(rdb *redis.Client, ctx context.Context, rootDir, relat
 	}
 
 	// ç„¶åä½¿ç”¨ hashedKey ä» Redis è·å–æ–‡ä»¶ä¿¡æ¯
-	fileInfoData, err := rdb.Get(ctx, "fileInfo:"+hashedKey).Bytes()
+	fileInfoData, err := rdb.Get(ctx, getFileInfoKey(hashedKey)).Bytes()
 	if err != nil {
 		return 0, fmt.Errorf("error getting file info for hashed key %s: %w", hashedKey, err)
 	}
@@ -117,14 +122,32 @@ func getFileSizeFromRedis(rdb *redis.Client, ctx context.Context, rootDir, relat
 }
 
 func getFullFileHash(fs afero.Fs, path string, rdb *redis.Client, ctx context.Context) (string, error) {
-	return calculateFileHash(fs, path, -1)
+	return calculateFileHash(fs, path, -1, rdb, ctx)
 }
 
 func getFileHash(fs afero.Fs, path string, rdb *redis.Client, ctx context.Context) (string, error) {
-	return calculateFileHash(fs, path, 100*1024) // 100KB
+	return calculateFileHash(fs, path, 100*1024, rdb, ctx) // 100KB
 }
 
-func calculateFileHash(fs afero.Fs, path string, limit int64) (string, error) {
+func calculateFileHash(fs afero.Fs, path string, limit int64, rdb *redis.Client, ctx context.Context) (string, error) {
+	// ç”Ÿæˆç¼“å­˜é”®
+	hashedKey := generateHash(path)
+	cacheKey := fmt.Sprintf("fileHash:%s:%d", hashedKey, limit)
+	
+	// å…ˆå°è¯•ä» Redis è·å–å“ˆå¸Œå€¼
+	if rdb != nil && ctx != nil {
+		cachedHash, err := rdb.Get(ctx, cacheKey).Result()
+		if err == nil {
+			// æ‰¾åˆ°ç¼“å­˜çš„å“ˆå¸Œå€¼ï¼Œç›´æ¥è¿”å›
+			return cachedHash, nil
+		}
+		// å¦‚æœé”™è¯¯ä¸æ˜¯ key ä¸å­˜åœ¨ï¼Œè®°å½•æ—¥å¿—
+		if err != redis.Nil {
+			log.Printf("Error reading hash from Redis for %s: %v", path, err)
+		}
+	}
+
+	// å¦‚æœæ²¡æœ‰ç¼“å­˜æˆ–è·å–å¤±è´¥ï¼Œè®¡ç®—å“ˆå¸Œå€¼
 	f, err := fs.Open(path)
 	if err != nil {
 		return "", fmt.Errorf("error opening file %q: %w", path, err)
@@ -142,7 +165,17 @@ func calculateFileHash(fs afero.Fs, path string, limit int64) (string, error) {
 		}
 	}
 
-	return fmt.Sprintf("%x", h.Sum(nil)), nil
+	hash := fmt.Sprintf("%x", h.Sum(nil))
+
+	// å°†è®¡ç®—çš„å“ˆå¸Œå€¼ä¿å­˜åˆ° Redis
+	if rdb != nil && ctx != nil {
+		err = rdb.Set(ctx, cacheKey, hash, 0).Err()
+		if err != nil {
+			log.Printf("Warning: Failed to cache hash for %s: %v", path, err)
+		}
+	}
+
+	return hash, nil
 }
 
 func ExtractTimestamps(filePath string) []string {
@@ -396,7 +429,7 @@ func writeDuplicateFilesToFile(rootDir string, outputFile string, rdb *redis.Cli
 					continue
 				}
 
-				fileInfoData, err := rdb.Get(ctx, "fileInfo:"+hashedKey).Bytes()
+				fileInfoData, err := rdb.Get(ctx, getFileInfoKey(hashedKey)).Bytes()
 				if err != nil {
 					log.Printf("Error getting file info for key %s: %v", hashedKey, err)
 					continue
@@ -540,10 +573,6 @@ func deleteDuplicateFiles(rootDir string, rdb *redis.Client, ctx context.Context
 	return nil
 }
 
-func shouldStopDuplicateFileSearch(duplicateCount int, maxDuplicateFiles int) bool {
-	return duplicateCount >= maxDuplicateFiles
-}
-
 func writeDataToFile(rootDir, filename string, data map[string]FileInfo, sortByModTime bool) error {
 	outputPath := filepath.Join(rootDir, filename)
 	outputDir := filepath.Dir(outputPath)
diff --git a/utils_test.go b/utils_test.go
--- ./utils_test.go
+++ ./utils_test.go
@@ -473,12 +473,10 @@ func TestFindAndLogDuplicates(t *testing.T) {
 		require.NoError(t, err)
 	}
 
-	// å¤„ç†æ–‡ä»¶ï¼Œè®¡ç®—å“ˆå¸Œå€¼
-	calculateHashes := true
 	for _, tf := range testFiles {
 		relPath, err := filepath.Rel(rootDir, tf.path)
 		require.NoError(t, err)
-		err = fp.ProcessFile(rootDir, relPath, calculateHashes)
+		err = fp.ProcessFile(rootDir, relPath)
 		require.NoError(t, err)
 	}
 
