diff --git a/.cursorrules b/.cursorrules
--- ./.cursorrules
+++ ./.cursorrules
@@ -1,4 +1,36 @@
 用中文回答
+在添加新方法之前，先搜索现有代码库中是否已有类似实现
+检查是否可以通过组合现有方法来实现所需功能
+确保不创建重复的功能实现
+要检查一下之前的apply是否正确.
+
+# AI 工作指令
+
+## 核心原则
+1. 专注于用户明确要求的具体任务
+2. 不要主动添加额外的功能或优化
+3. 在修改代码时保持最小改动原则
+
+## 工作流程
+1. 仔细理解用户的具体需求
+2. 确认需要修改的关键部分
+3. 只修改必要的代码
+4. 保持其他代码不变
+
+## 代码修改原则
 1. 在添加新方法之前，先搜索现有代码库中是否已有类似实现
 2. 检查是否可以通过组合现有方法来实现所需功能
 3. 确保不创建重复的功能实现
+4. 只在用户明确要求时才进行重构或优化
+
+## 响应规范
+1. 用中文回答
+2. 清晰说明将要修改的内容
+3. 解释为什么需要这些修改
+4. 只展示必要的代码改动
+
+## 代码展示格式
+1. 使用 markdown 代码块
+2. 标注文件路径和修改位置
+3. 使用注释标明跳过的未修改代码
+4. 重点突出修改的部分
diff --git a/file_processing.go b/file_processing.go
--- ./file_processing.go
+++ ./file_processing.go
@@ -13,11 +13,11 @@ import (
 	"log"
 	"os"
 	"path/filepath"
-	"regexp"
+	"regexp"  // 添加
+	"strconv" // 添加
+	"strings" // 添加
+	"time"    // 添加
 	"sort"
-	"strconv"
-	"strings"
-	"time"
 )
 
 type FileProcessor struct {
@@ -38,7 +38,7 @@ func CreateFileProcessor(rdb *redis.Client, ctx context.Context, excludeRegexps
 		excludeRegexps: excludeRegexps,
 	}
 
-	// 设置默认值
+	// 设置默认
 	fp.generateHashFunc = generateHash
 	fp.calculateFileHashFunc = fp.calculateFileHash
 	fp.saveFileInfoToRedisFunc = saveFileInfoToRedis
@@ -137,20 +137,16 @@ const (
 
 func (fp *FileProcessor) ProcessFile(rootDir, relativePath string, calculateHashes bool) error {
 	fullPath := filepath.Join(rootDir, relativePath)
-	log.Printf("Processing file: %s", fullPath)
 
 	info, err := fp.fs.Stat(fullPath)
 	if err != nil {
 		return fmt.Errorf("error getting file info: %w", err)
 	}
 
-	hashedKey := fp.generateHashFunc(fullPath)
-	log.Printf("Generated hashed key: %s", hashedKey)
-
 	fileInfo := FileInfo{
 		Size:    info.Size(),
 		ModTime: info.ModTime(),
-		Path:    fullPath, // 存储绝对路径
+		Path:    fullPath,
 	}
 
 	var fileHash, fullHash string
@@ -159,24 +155,36 @@ func (fp *FileProcessor) ProcessFile(rootDir, relativePath string, calculateHash
 		if err != nil {
 			return fmt.Errorf("error calculating file hash: %w", err)
 		}
-		log.Printf("Calculated file hash: %s", fileHash)
 
+		hasMultiple, err := fp.hasMultipleFilesWithHash(fileHash)
+		if err != nil {
+			log.Printf("Warning: Failed to check partial hash: %v", err)
+		} else if hasMultiple {
 			fullHash, err = fp.calculateFileHashFunc(fullPath, FullFileReadCmd)
 			if err != nil {
 				return fmt.Errorf("error calculating full file hash: %w", err)
 			}
-		log.Printf("Calculated full hash: %s", fullHash)
+		}
 	}
 
 	err = fp.saveFileInfoToRedisFunc(fp.Rdb, fp.Ctx, fullPath, fileInfo, fileHash, fullHash, calculateHashes)
 	if err != nil {
 		return fmt.Errorf("error saving file info to Redis: %w", err)
 	}
-	log.Printf("Saved file info to Redis")
 
 	return nil
 }
 
+// 修改函数名和返回值以更好地表达意图
+func (fp *FileProcessor) hasMultipleFilesWithHash(partialHash string) (bool, error) {
+	count, err := fp.Rdb.SCard(fp.Ctx, "fileHashToPathSet:"+partialHash).Result()
+	if err != nil {
+		return false, err
+	}
+	// 只有当有多个文件具有相同的部分哈希时才返回 true
+	return count > 1, nil
+}
+
 type FileInfoRetriever interface {
 	getFileInfoFromRedis(hashedKey string) (FileInfo, error)
 }
@@ -281,10 +289,8 @@ func (fp *FileProcessor) getFileInfoFromRedis(hashedKey string) (FileInfo, error
 }
 
 func (fp *FileProcessor) calculateFileHash(path string, limit int64) (string, error) {
-	// 首先获取文件的 hashedKey
+	// 生成缓存键
 	hashedKey := fp.generateHashFunc(path)
-	
-	// 检查Redis缓存中是否已存在对应的hash
 	var cacheKey string
 	if limit == FullFileReadCmd {
 		cacheKey = "hashedKeyToFullHash:" + hashedKey
@@ -295,27 +301,27 @@ func (fp *FileProcessor) calculateFileHash(path string, limit int64) (string, er
 	// 尝试从缓存获取
 	cachedHash, err := fp.Rdb.Get(fp.Ctx, cacheKey).Result()
 	if err == nil {
+		log.Printf("Using cached hash for %s", path)
 		return cachedHash, nil
 	} else if err != redis.Nil {
 		return "", fmt.Errorf("redis error: %w", err)
 	}
 
 	// 缓存未命中，计算新的hash
-	f, err := fp.fs.Open(path)
+	file, err := fp.fs.Open(path)
 	if err != nil {
 		return "", fmt.Errorf("error opening file: %w", err)
 	}
-	defer f.Close()
+	defer file.Close()
 
 	h := sha512.New()
-	if limit == FullFileReadCmd {
-		if _, err := io.Copy(h, f); err != nil {
-			return "", fmt.Errorf("error reading full file: %w", err)
-		}
+	if limit > 0 {
+		_, err = io.CopyN(h, file, limit)
 	} else {
-		if _, err := io.CopyN(h, f, limit); err != nil && err != io.EOF {
-			return "", fmt.Errorf("error reading file: %w", err)
+		_, err = io.Copy(h, file)
 	}
+	if err != nil && err != io.EOF {
+		return "", fmt.Errorf("error reading file: %w", err)
 	}
 
 	hash := fmt.Sprintf("%x", h.Sum(nil))
@@ -323,6 +329,8 @@ func (fp *FileProcessor) calculateFileHash(path string, limit int64) (string, er
 	// 将新计算的hash保存到Redis
 	if err := fp.Rdb.Set(fp.Ctx, cacheKey, hash, 0).Err(); err != nil {
 		log.Printf("Warning: Failed to cache hash for %s: %v", path, err)
+	} else {
+		log.Printf("Cached new hash for %s", path)
 	}
 
 	return hash, nil
@@ -338,7 +346,7 @@ func processDirectory(path string) {
 func processSymlink(path string) {
 }
 
-// 处理关键词
+// ��理关键词
 func processKeyword(keyword string, keywordFiles []string, Rdb *redis.Client, Ctx context.Context, rootDir string, excludeRegexps []*regexp.Regexp) {
 	// 对 keywordFiles 进行排序
 	sort.Slice(keywordFiles, func(i, j int) bool {
diff --git a/file_processing_test.go b/file_processing_test.go
--- ./file_processing_test.go
+++ ./file_processing_test.go
@@ -7,18 +7,17 @@ import (
 	"context"
 	"encoding/gob"
 	"fmt"
-	"io/ioutil"
-	"os"
-	"path/filepath"
-	"testing"
-	"time"
-
 	"github.com/alicebob/miniredis/v2"
 	"github.com/go-redis/redis/v8"
 	"github.com/go-redis/redismock/v8"
 	"github.com/spf13/afero"
 	"github.com/stretchr/testify/assert"
 	"github.com/stretchr/testify/require"
+	"io/ioutil"
+	"os"
+	"path/filepath"
+	"testing"
+	"time"
 )
 
 func setupTestEnvironment(t *testing.T) (*miniredis.Miniredis, *redis.Client, context.Context, afero.Fs, *FileProcessor) {
@@ -375,7 +374,7 @@ func TestExtractTimestamps(t *testing.T) {
 		},
 		{
 			"Timestamps with different formats",
-			"/Users/huangyingw/mini/media/usb_backup_crypt_8T_1/cartoon/dragonball/第一部/龙珠 第一部 日语配音/七龙珠146.rmvb:24:30,1:11:27,:02:35:52",
+			"/Users/huangyingw/mini/media/usb_backup_crypt_8T_1/cartoon/dragonball/第一/龙珠 第一部 日语配音/七龙珠146.rmvb:24:30,1:11:27,:02:35:52",
 			[]string{"24:30", "01:11:27", "02:35:52"},
 		},
 		{
@@ -746,7 +745,7 @@ func TestCalculateFileHash(t *testing.T) {
 		// 验证两次计算结果相同
 		assert.Equal(t, hash1, hash2, "相同大小的读取应该产生相同的哈希值")
 
-		// 验证第二次计算使用了缓存
+		// 验证第二次计算使了缓存
 		hashedKey := fp.generateHashFunc(testFile)
 		cachedHash, err := rdb.Get(ctx, "hashedKeyToFileHash:"+hashedKey).Result()
 		require.NoError(t, err)
@@ -1183,3 +1182,125 @@ func TestFileProcessor_ShouldExclude(t *testing.T) {
 		})
 	}
 }
+
+func TestCalculateFileHashCaching(t *testing.T) {
+	mr, rdb, ctx, fs, fp := setupTestEnvironment(t)
+	defer mr.Close()
+
+	// 创建测试文件
+	testFile := "/test.txt"
+	content := []byte("test content")
+	err := afero.WriteFile(fs, testFile, content, 0644)
+	require.NoError(t, err)
+
+	// 第一次计算哈希
+	hash1, err := fp.calculateFileHash(testFile, ReadLimit)
+	require.NoError(t, err)
+
+	// 第二次计算哈希（应该从缓存获取）
+	hash2, err := fp.calculateFileHash(testFile, ReadLimit)
+	require.NoError(t, err)
+
+	// 验证结果
+	assert.Equal(t, hash1, hash2)
+
+	// 验证缓存
+	hashedKey := fp.generateHashFunc(testFile)
+	cachedHash, err := rdb.Get(ctx, "hashedKeyToFileHash:"+hashedKey).Result()
+	require.NoError(t, err)
+	assert.Equal(t, hash1, cachedHash)
+}
+
+func TestFileHashCalculation(t *testing.T) {
+	// 设置 Redis
+	mr, err := miniredis.Run()
+	require.NoError(t, err)
+	defer mr.Close()
+
+	rdb := redis.NewClient(&redis.Options{
+		Addr: mr.Addr(),
+	})
+	defer rdb.Close()
+
+	ctx := context.Background()
+	fs := afero.NewMemMapFs()
+
+	// 创建测试目录
+	tempDir := "/test"
+	err = fs.MkdirAll(tempDir, 0755)
+	require.NoError(t, err)
+
+	// 创建两个内容相同的文件和一个不同的文件
+	testFiles := []struct {
+		path    string
+		content string
+	}{
+		{filepath.Join(tempDir, "file1.txt"), "duplicate content"},
+		{filepath.Join(tempDir, "file2.txt"), "duplicate content"},
+		{filepath.Join(tempDir, "file3.txt"), "unique content"},
+	}
+
+	fp := CreateFileProcessor(rdb, ctx, nil, WithFs(fs))
+
+	// 测试场景
+	t.Run("Hash Calculation and Caching", func(t *testing.T) {
+		// 处理第一个文件
+		err = fp.ProcessFile(tempDir, "file1.txt", true)
+		require.NoError(t, err)
+
+		// 验证部分哈希已计算并缓存
+		hashedKey1 := fp.generateHashFunc(filepath.Join(tempDir, "file1.txt"))
+		partialHash1, err := rdb.Get(ctx, "hashedKeyToFileHash:"+hashedKey1).Result()
+		require.NoError(t, err)
+		assert.NotEmpty(t, partialHash1)
+
+		// 验证完整哈希尚未计算
+		_, err = rdb.Get(ctx, "hashedKeyToFullHash:"+hashedKey1).Result()
+		assert.Equal(t, redis.Nil, err, "完整哈希不应该被计算")
+
+		// 处理第二个相同内容的文件
+		err = fp.ProcessFile(tempDir, "file2.txt", true)
+		require.NoError(t, err)
+
+		// 验证第二个文件触发了完整哈希的计算
+		hashedKey2 := fp.generateHashFunc(filepath.Join(tempDir, "file2.txt"))
+		fullHash2, err := rdb.Get(ctx, "hashedKeyToFullHash:"+hashedKey2).Result()
+		require.NoError(t, err)
+		assert.NotEmpty(t, fullHash2)
+
+		// 验证第一个文件的完整哈希也被计算
+		fullHash1, err := rdb.Get(ctx, "hashedKeyToFullHash:"+hashedKey1).Result()
+		require.NoError(t, err)
+		assert.NotEmpty(t, fullHash1)
+
+		// 处理第三个不同内容的文件
+		err = fp.ProcessFile(tempDir, "file3.txt", true)
+		require.NoError(t, err)
+
+		// 验证第三个文件只计算了部分哈希
+		hashedKey3 := fp.generateHashFunc(filepath.Join(tempDir, "file3.txt"))
+		_, err = rdb.Get(ctx, "hashedKeyToFileHash:"+hashedKey3).Result()
+		require.NoError(t, err)
+
+		// 验证第三个文件没有计算完整哈希
+		_, err = rdb.Get(ctx, "hashedKeyToFullHash:"+hashedKey3).Result()
+		assert.Equal(t, redis.Nil, err, "不同内容的文件不应计算完整哈希")
+	})
+
+	// 测试哈希缓存
+	t.Run("Hash Cache Usage", func(t *testing.T) {
+		// 重新处理第一个文件
+		err = fp.ProcessFile(tempDir, "file1.txt", true)
+		require.NoError(t, err)
+
+		// 验证使用了缓存的哈希值
+		hashedKey := fp.generateHashFunc(filepath.Join(tempDir, "file1.txt"))
+		cachedPartialHash, err := rdb.Get(ctx, "hashedKeyToFileHash:"+hashedKey).Result()
+		require.NoError(t, err)
+		assert.Equal(t, partialHash1, cachedPartialHash, "应该使用缓存的部分哈希值")
+
+		cachedFullHash, err := rdb.Get(ctx, "hashedKeyToFullHash:"+hashedKey).Result()
+		require.NoError(t, err)
+		assert.Equal(t, fullHash1, cachedFullHash, "应该使用缓存的完整哈希值")
+	})
+}
diff --git a/utils_test.go b/utils_test.go
--- ./utils_test.go
+++ ./utils_test.go
@@ -73,46 +73,6 @@ func TestExtractKeywords(t *testing.T) {
 	assert.Equal(t, expectedKeywords, keywords, "Extracted keywords do not match expected keywords")
 }
 
-func TestFindCloseFiles(t *testing.T) {
-	// 创建临时目录
-	tempDir, err := os.MkdirTemp("", "closefiles_test")
-	require.NoError(t, err)
-	defer os.RemoveAll(tempDir)
-
-	// 创建测试用的 fav.log 文件
-	favLog := `100,"test_file_1.txt"
-200,"test_file_2.txt"
-300,"similar_name_1.mp4"
-400,"similar_name_2.mp4"
-500,"totally_different.txt"
-`
-	err = os.WriteFile(filepath.Join(tempDir, "fav.log"), []byte(favLog), 0644)
-	require.NoError(t, err)
-
-	// 创建 CloseFileFinder 实例并处理文件
-	finder := NewCloseFileFinder(tempDir)
-	err = finder.ProcessCloseFiles()
-	require.NoError(t, err)
-
-	// 验证输出文件存在
-	outputPath := filepath.Join(tempDir, "fav.log.close")
-	_, err = os.Stat(outputPath)
-	assert.NoError(t, err)
-
-	// 读取并验证输出内容
-	content, err := os.ReadFile(outputPath)
-	require.NoError(t, err)
-
-	// 验证相似文件被正确识别
-	contentStr := string(content)
-	assert.Contains(t, contentStr, "similar_name_1.mp4")
-	assert.Contains(t, contentStr, "similar_name_2.mp4")
-	assert.Contains(t, contentStr, "相似度:")
-
-	// 验证不相似的文件没有被错误匹配
-	assert.NotContains(t, contentStr, "totally_different.txt")
-}
-
 func TestWalkFiles(t *testing.T) {
 	tempDir, err := ioutil.TempDir("", "test_walk_files")
 	assert.NoError(t, err)
