diff --git a/data_serialization.go b/data_serialization.go
new file mode 100644
--- /dev/null
+++ ./data_serialization.go
@@ -0,0 +1,11 @@
+package main
+
+import (
+	"time"
+)
+
+// FileInfo holds file information
+type FileInfo struct {
+	Size    int64
+	ModTime time.Time
+}
diff --git a/file_processing.go b/file_processing.go
new file mode 100644
--- /dev/null
+++ ./file_processing.go
@@ -0,0 +1,150 @@
+// file_processing.go
+package main
+
+import (
+	"bytes"
+	"context"
+	"crypto/sha256"
+	"encoding/gob"
+	"fmt"
+	"github.com/go-redis/redis/v8"
+	"io"
+	"os"
+	"path/filepath"
+	"sync/atomic"
+)
+
+func getFileInfoFromRedis(rdb *redis.Client, ctx context.Context, hashedKey string) (FileInfo, error) {
+	var fileInfo FileInfo
+	value, err := rdb.Get(ctx, hashedKey).Bytes()
+	if err != nil {
+		return fileInfo, err
+	}
+
+	buf := bytes.NewBuffer(value)
+	dec := gob.NewDecoder(buf)
+	err = dec.Decode(&fileInfo)
+	return fileInfo, err
+}
+
+func saveToFile(dir, filename string, sortByModTime bool, rdb *redis.Client, ctx context.Context) error {
+	iter := rdb.Scan(ctx, 0, "*", 0).Iterator()
+	var data = make(map[string]FileInfo)
+	for iter.Next(ctx) {
+		hashedKey := iter.Val()
+		originalPath, err := rdb.Get(ctx, "path:"+hashedKey).Result()
+		if err != nil {
+			continue
+		}
+
+		fileInfo, err := getFileInfoFromRedis(rdb, ctx, hashedKey)
+		if err == nil {
+			data[originalPath] = fileInfo
+		}
+	}
+
+	var lines []string
+	var keys []string
+	for k := range data {
+		keys = append(keys, k)
+	}
+	sortKeys(keys, data, sortByModTime)
+	for _, k := range keys {
+		relativePath, _ := filepath.Rel(dir, k)
+		line := formatFileInfoLine(data[k], relativePath, sortByModTime)
+		lines = append(lines, line)
+	}
+
+	return writeLinesToFile(filepath.Join(dir, filename), lines)
+}
+
+func processFile(path string, typ os.FileMode, rdb *redis.Client, ctx context.Context, startTime int64) {
+	// Update progress counter atomically
+	atomic.AddInt32(&progressCounter, 1)
+
+	// 生成文件路径的哈希作为键
+	hashedKey := generateHash(path)
+
+	// 检查Redis中是否已存在该文件的信息
+	exists, err := rdb.Exists(ctx, hashedKey).Result()
+	if err != nil {
+		fmt.Printf("Error checking existence in Redis for file %s: %s\n", path, err)
+		return
+	}
+	if exists > 0 {
+		// 文件已存在于Redis中，更新其更新时间戳
+		_, err := rdb.Set(ctx, "updateTime:"+hashedKey, startTime, 0).Result()
+		if err != nil {
+			fmt.Printf("Error updating updateTime for file %s: %s\n", path, err)
+		}
+		return
+	}
+
+	info, err := os.Stat(path)
+	if err != nil {
+		fmt.Printf("Error stating file: %s, Error: %s\n", path, err)
+		return
+	}
+
+	var buf bytes.Buffer
+	enc := gob.NewEncoder(&buf)
+	if err := enc.Encode(FileInfo{Size: info.Size(), ModTime: info.ModTime()}); err != nil {
+		fmt.Printf("Error encoding: %s, File: %s\n", err, path)
+		return
+	}
+
+	// 计算文件的SHA-256哈希值
+	fileHash, err := calculateFileHash(path)
+	if err != nil {
+		fmt.Printf("Error calculating hash for file %s: %s\n", path, err)
+		return
+	}
+
+	// 使用管道批量处理Redis命令
+	pipe := rdb.Pipeline()
+
+	// 这里我们添加命令到管道，但不立即检查错误
+	pipe.Set(ctx, hashedKey, buf.Bytes(), 0)
+	pipe.Set(ctx, "path:"+hashedKey, path, 0)
+	pipe.Set(ctx, "updateTime:"+hashedKey, startTime, 0)
+	pipe.Set(ctx, "hash:"+hashedKey, fileHash, 0) // 存储文件哈希值
+
+	if _, err = pipe.Exec(ctx); err != nil {
+		fmt.Printf("Error executing pipeline for file: %s: %s\n", path, err)
+		return
+	}
+}
+
+func formatFileInfoLine(fileInfo FileInfo, relativePath string, sortByModTime bool) string {
+	if sortByModTime {
+		utcTimestamp := fileInfo.ModTime.UTC().Unix()
+		return fmt.Sprintf("%d,\"./%s\"", utcTimestamp, relativePath)
+	}
+	return fmt.Sprintf("%d,\"./%s\"", fileInfo.Size, relativePath)
+}
+
+// calculateFileHash 计算文件的SHA-256哈希值
+func calculateFileHash(filePath string) (string, error) {
+	file, err := os.Open(filePath)
+	if err != nil {
+		return "", err
+	}
+	defer file.Close()
+
+	hasher := sha256.New()
+	if _, err := io.Copy(hasher, file); err != nil {
+		return "", err
+	}
+	return fmt.Sprintf("%x", hasher.Sum(nil)), nil
+}
+
+func processDirectory(path string) {
+	// 处理目录的逻辑
+	fmt.Printf("Processing directory: %s\n", path)
+	// 可能的操作：遍历目录下的文件等
+}
+func processSymlink(path string) {
+	// 处理软链接的逻辑
+	fmt.Printf("Processing symlink: %s\n", path)
+	// 可能的操作：解析软链接，获取实际文件等
+}
diff --git a/find_large_files_with_cache.go b/find_large_files_with_cache.go
deleted file mode 100644
--- ./find_large_files_with_cache.go
+++ /dev/null
@@ -1,298 +0,0 @@
-package main
-
-import (
-	"bufio"
-	"bytes"
-	"context"
-	"crypto/sha256"
-	"encoding/gob"
-	"encoding/hex"
-	"fmt"
-	"github.com/go-redis/redis/v8"
-	"github.com/karrick/godirwalk"
-	"os"
-	"path/filepath"
-	"regexp"
-	"sort"
-	"strings"
-	"sync"
-	"sync/atomic"
-	"time"
-)
-
-var progressCounter int32 // Progress counter
-var rdb *redis.Client     // Redis client
-var ctx = context.Background()
-
-// FileInfo holds file information
-type FileInfo struct {
-	Size    int64
-	ModTime time.Time
-}
-
-// Task 定义了工作池中的任务类型
-type Task func()
-
-// NewWorkerPool 创建并返回一个工作池
-func NewWorkerPool(workerCount int) (chan<- Task, *sync.WaitGroup) {
-	var wg sync.WaitGroup
-	taskQueue := make(chan Task)
-
-	for i := 0; i < workerCount; i++ {
-		wg.Add(1)
-		go func() {
-			defer wg.Done()
-			for task := range taskQueue {
-				task()
-			}
-		}()
-	}
-
-	return taskQueue, &wg
-}
-
-var wg sync.WaitGroup
-var workerPool = make(chan struct{}, 20) // Limit concurrency to 20
-
-// Initialize Redis client
-func init() {
-	rdb = redis.NewClient(&redis.Options{
-		Addr: "localhost:6379",
-	})
-	_, err := rdb.Ping(ctx).Result()
-	if err != nil {
-		fmt.Println("Error connecting to Redis:", err)
-		os.Exit(1)
-	}
-}
-
-// Generate a SHA-256 hash for the given string
-func generateHash(s string) string {
-	hasher := sha256.New()
-	hasher.Write([]byte(s))
-	return hex.EncodeToString(hasher.Sum(nil))
-}
-
-func processDirectory(path string) {
-	// 处理目录的逻辑
-	fmt.Printf("Processing directory: %s\n", path)
-	// 可能的操作：遍历目录下的文件等
-}
-
-func processSymlink(path string) {
-	// 处理软链接的逻辑
-	fmt.Printf("Processing symlink: %s\n", path)
-	// 可能的操作：解析软链接，获取实际文件等
-}
-
-func loadExcludePatterns(filename string) ([]string, error) {
-	file, err := os.Open(filename)
-	if err != nil {
-		return nil, err
-	}
-	defer file.Close()
-
-	var patterns []string
-	scanner := bufio.NewScanner(file)
-	for scanner.Scan() {
-		pattern := scanner.Text()
-		fmt.Printf("Loaded exclude pattern: %s\n", pattern) // 打印每个加载的模式
-		patterns = append(patterns, pattern)
-	}
-	return patterns, scanner.Err()
-}
-
-func saveToFile(dir, filename string, sortByModTime bool) error {
-	file, err := os.Create(filepath.Join(dir, filename))
-	if err != nil {
-		return err
-	}
-	defer file.Close()
-
-	iter := rdb.Scan(ctx, 0, "*", 0).Iterator()
-	var data = make(map[string]FileInfo)
-	for iter.Next(ctx) {
-		hashedKey := iter.Val()
-		originalPath, err := rdb.Get(ctx, "path:"+hashedKey).Result()
-		if err != nil {
-			continue
-		}
-		value, err := rdb.Get(ctx, hashedKey).Bytes()
-		if err != nil {
-			continue
-		}
-		var fileInfo FileInfo
-		buf := bytes.NewBuffer(value)
-		dec := gob.NewDecoder(buf)
-		if err := dec.Decode(&fileInfo); err == nil {
-			data[originalPath] = fileInfo
-		}
-	}
-
-	var keys []string
-	for k := range data {
-		keys = append(keys, k)
-	}
-
-	sortKeys(keys, data, sortByModTime)
-
-	for _, k := range keys {
-		relativePath, _ := filepath.Rel(dir, k)
-		if sortByModTime {
-			utcTimestamp := data[k].ModTime.UTC().Unix()
-			fmt.Fprintf(file, "%d,\"./%s\"\n", utcTimestamp, relativePath)
-		} else {
-			fmt.Fprintf(file, "%d,\"./%s\"\n", data[k].Size, relativePath)
-		}
-	}
-	return nil
-}
-
-func sortKeys(keys []string, data map[string]FileInfo, sortByModTime bool) {
-	if sortByModTime {
-		sort.Slice(keys, func(i, j int) bool {
-			return data[keys[i]].ModTime.After(data[keys[j]].ModTime)
-		})
-	} else {
-		sort.Slice(keys, func(i, j int) bool {
-			return data[keys[i]].Size > data[keys[j]].Size
-		})
-	}
-}
-
-func processFile(path string, typ os.FileMode) {
-	if typ.IsDir() {
-		return
-	}
-
-	info, err := os.Stat(path)
-	if err != nil {
-		fmt.Printf("Error stating file: %s, Error: %s\n", path, err)
-		return
-	}
-
-	var buf bytes.Buffer
-	enc := gob.NewEncoder(&buf)
-	if err := enc.Encode(FileInfo{Size: info.Size(), ModTime: info.ModTime()}); err != nil {
-		fmt.Printf("Error encoding: %s, File: %s\n", err, path)
-		return
-	}
-
-	// Generate hash for the file path
-	hashedKey := generateHash(path)
-
-	// 使用管道批量处理Redis命令
-	pipe := rdb.Pipeline()
-
-	// 这里我们添加命令到管道，但不立即检查错误
-	pipe.Set(ctx, hashedKey, buf.Bytes(), 0)
-	pipe.Set(ctx, "path:"+hashedKey, path, 0)
-
-	if _, err = pipe.Exec(ctx); err != nil {
-		fmt.Printf("Error executing pipeline for file: %s: %s\n", path, err)
-		return
-	}
-
-	// Update progress counter atomically
-	atomic.AddInt32(&progressCounter, 1)
-}
-
-func main() {
-	if len(os.Args) < 2 {
-		fmt.Println("Usage: ./find_large_files_with_cache <directory>")
-		return
-	}
-
-	// Root directory to start the search
-	rootDir := os.Args[1]
-
-	// Minimum file size in bytes
-	minSize := 200 // Default size is 200MB
-	minSizeBytes := int64(minSize * 1024 * 1024)
-
-	excludePatterns, err := loadExcludePatterns(filepath.Join(rootDir, "exclude_patterns.txt"))
-	if err != nil {
-		fmt.Println("Warning: Could not read exclude patterns:", err)
-	}
-
-	excludeRegexps := make([]*regexp.Regexp, len(excludePatterns))
-	for i, pattern := range excludePatterns {
-		// 将通配符模式转换为正则表达式
-		regexPattern := strings.Replace(pattern, "*", ".*", -1)
-		excludeRegexps[i], err = regexp.Compile(regexPattern)
-		if err != nil {
-			fmt.Printf("Invalid regex pattern '%s': %s\n", regexPattern, err)
-			return
-		}
-	}
-
-	// Start a goroutine to periodically print progress
-	go func() {
-		for {
-			time.Sleep(1 * time.Second)
-			fmt.Printf("Progress: %d files processed.\n", atomic.LoadInt32(&progressCounter))
-		}
-	}()
-
-	// Use godirwalk.Walk instead of fastwalk.Walk or filepath.Walk
-	// 初始化工作池
-	workerCount := 20 // 可以根据需要调整工作池的大小
-	taskQueue, poolWg := NewWorkerPool(workerCount)
-
-	// 使用 godirwalk.Walk 遍历文件
-	err = godirwalk.Walk(rootDir, &godirwalk.Options{
-		Callback: func(osPathname string, de *godirwalk.Dirent) error {
-			// 排除模式匹配
-			for _, re := range excludeRegexps {
-				if re.MatchString(osPathname) {
-					return nil
-				}
-			}
-
-			fileInfo, err := os.Lstat(osPathname)
-			if err != nil {
-				fmt.Printf("Error getting file info: %s\n", err)
-				return err
-			}
-
-			// 检查文件大小是否满足最小阈值
-			if fileInfo.Size() < minSizeBytes {
-				return nil
-			}
-
-			// 将任务发送到工作池
-			taskQueue <- func() {
-				if fileInfo.Mode().IsDir() {
-					processDirectory(osPathname)
-				} else if fileInfo.Mode().IsRegular() {
-					processFile(osPathname, fileInfo.Mode())
-				} else if fileInfo.Mode()&os.ModeSymlink != 0 {
-					processSymlink(osPathname)
-				} else {
-					fmt.Printf("Skipping unknown type: %s\n", osPathname)
-				}
-			}
-
-			return nil
-		},
-		Unsorted: true,
-	})
-
-	// 关闭任务队列，并等待所有任务完成
-	close(taskQueue)
-	poolWg.Wait()
-	fmt.Printf("Final progress: %d files processed.\n", atomic.LoadInt32(&progressCounter))
-
-	// 文件处理完成后的保存操作
-	if err := saveToFile(rootDir, "fav.log", false); err != nil {
-		fmt.Printf("Error saving to fav.log: %s\n", err)
-	} else {
-		fmt.Printf("Saved data to %s\n", filepath.Join(rootDir, "fav.log"))
-	}
-
-	if err := saveToFile(rootDir, "fav.log.sort", true); err != nil {
-		fmt.Printf("Error saving to fav.log.sort: %s\n", err)
-	} else {
-		fmt.Printf("Saved sorted data to %s\n", filepath.Join(rootDir, "fav.log.sort"))
-	}
-}
diff --git a/main.go b/main.go
new file mode 100644
--- /dev/null
+++ ./main.go
@@ -0,0 +1,144 @@
+// main.go
+// 此文件是Go程序的主要入口点，包含了文件处理程序的核心逻辑和初始化代码。
+
+package main
+
+import (
+	"context"
+	"fmt"
+	"github.com/go-redis/redis/v8"
+	"github.com/karrick/godirwalk"
+	"os"
+	"path/filepath"
+	"regexp"
+	"sync/atomic"
+	"time"
+)
+
+var progressCounter int32 // Progress counter
+
+func main() {
+	startTime := time.Now().Unix()
+	rootDir, minSizeBytes, excludeRegexps, rdb, ctx, err := initializeApp(os.Args)
+	if err != nil {
+		fmt.Println(err)
+		return
+	}
+
+	ctx, cancel := context.WithCancel(context.Background())
+	defer cancel()
+
+	go monitorProgress(ctx, &progressCounter)
+
+	workerCount := 20
+	taskQueue, poolWg := NewWorkerPool(workerCount)
+
+	walkFiles(rootDir, minSizeBytes, excludeRegexps, taskQueue, rdb, ctx, startTime)
+
+	close(taskQueue)
+	poolWg.Wait()
+	fmt.Printf("Final progress: %d files processed.\n", atomic.LoadInt32(&progressCounter))
+
+	err = cleanUpOldRecords(rdb, ctx, startTime)
+	if err != nil {
+		fmt.Println("Error cleaning up old records:", err)
+	}
+
+	// 文件处理完成后的保存操作
+	performSaveOperation(rootDir, "fav.log", false, rdb, ctx)
+	performSaveOperation(rootDir, "fav.log.sort", true, rdb, ctx)
+}
+
+// 初始化Redis客户端
+func newRedisClient(ctx context.Context) *redis.Client {
+	rdb := redis.NewClient(&redis.Options{
+		Addr: "localhost:6379", // 该地址应从配置中获取
+	})
+	_, err := rdb.Ping(ctx).Result()
+	if err != nil {
+		fmt.Println("Error connecting to Redis:", err)
+		os.Exit(1)
+	}
+	return rdb
+}
+
+// initializeApp 初始化应用程序设置
+func initializeApp(args []string) (string, int64, []*regexp.Regexp, *redis.Client, context.Context, error) {
+	if len(args) < 2 {
+		return "", 0, nil, nil, nil, fmt.Errorf("usage: ./find_large_files_with_cache <directory>")
+	}
+
+	// Root directory to start the search
+	rootDir := args[1]
+
+	// Minimum file size in bytes
+	minSize := 200 // Default size is 200MB
+	minSizeBytes := int64(minSize * 1024 * 1024)
+
+	excludeRegexps, err := compileExcludePatterns(filepath.Join(rootDir, "exclude_patterns.txt"))
+	if err != nil {
+		return "", 0, nil, nil, nil, err
+	}
+
+	// 创建 Redis 客户端
+	ctx := context.Background()
+	rdb := newRedisClient(ctx)
+
+	return rootDir, minSizeBytes, excludeRegexps, rdb, ctx, nil
+}
+
+// walkFiles 遍历指定目录下的文件，并根据条件进行处理
+func walkFiles(rootDir string, minSizeBytes int64, excludeRegexps []*regexp.Regexp, taskQueue chan<- Task, rdb *redis.Client, ctx context.Context, startTime int64) error {
+	return godirwalk.Walk(rootDir, &godirwalk.Options{
+		Callback: func(osPathname string, dirent *godirwalk.Dirent) error {
+			// 排除模式匹配
+			for _, re := range excludeRegexps {
+				if re.MatchString(osPathname) {
+					return nil
+				}
+			}
+
+			fileInfo, err := os.Lstat(osPathname)
+			if err != nil {
+				fmt.Printf("Error getting file info: %s\n", err)
+				return err
+			}
+
+			// 检查文件大小是否满足最小阈值
+			if fileInfo.Size() < minSizeBytes {
+				return nil
+			}
+
+			// 将任务发送到工作池
+			taskQueue <- func() {
+				if fileInfo.Mode().IsDir() {
+					processDirectory(osPathname)
+				} else if fileInfo.Mode().IsRegular() {
+					processFile(osPathname, fileInfo.Mode(), rdb, ctx, startTime)
+				} else if fileInfo.Mode()&os.ModeSymlink != 0 {
+					processSymlink(osPathname)
+				} else {
+					fmt.Printf("Skipping unknown type: %s\n", osPathname)
+				}
+			}
+			return nil
+		},
+		Unsorted: true, // 设置为true以提高性能
+	})
+}
+
+// monitorProgress 在给定的上下文中定期打印处理进度
+func monitorProgress(ctx context.Context, progressCounter *int32) {
+	ticker := time.NewTicker(1 * time.Second)
+	defer ticker.Stop()
+
+	for {
+		select {
+		case <-ctx.Done(): // 检查上下文是否被取消
+			return
+		case <-ticker.C: // 每秒触发一次
+			processed := atomic.LoadInt32(progressCounter)
+			fmt.Printf("Progress: %d files processed.\n", processed)
+		}
+	}
+}
diff --git a/main.go.sh b/main.go.sh
new file mode 100755
--- /dev/null
+++ ./main.go.sh
@@ -0,0 +1,16 @@
+#!/bin/zsh
+SCRIPT=$(realpath "$0")
+SCRIPTPATH=$(dirname "$SCRIPT")
+cd "$SCRIPTPATH"
+
+
+go mod init github.com/huangyingw/FileSorter
+go get -u github.com/allegro/bigcache
+go get -u github.com/go-redis/redis/v8
+go get -u github.com/mattn/go-zglob/fastwalk
+go get -u github.com/karrick/godirwalk
+
+#docker-compose down -v
+docker-compose up -d
+
+go run . /media
diff --git a/redis_client.go b/redis_client.go
new file mode 100644
--- /dev/null
+++ ./redis_client.go
@@ -0,0 +1,113 @@
+// redis_client.go
+package main
+
+import (
+	"bytes"
+	"context"
+	"crypto/sha256"
+	"encoding/gob"
+	"encoding/hex"
+	"fmt"
+	"github.com/go-redis/redis/v8"
+	"strings"
+)
+
+// Generate a SHA-256 hash for the given string
+func generateHash(s string) string {
+	hasher := sha256.New()
+	hasher.Write([]byte(s))
+	return hex.EncodeToString(hasher.Sum(nil))
+}
+
+func cleanUpOldRecords(rdb *redis.Client, ctx context.Context, startTime int64) error {
+	iter := rdb.Scan(ctx, 0, "updateTime:*", 0).Iterator()
+	for iter.Next(ctx) {
+		updateTimeKey := iter.Val()
+		updateTime, err := rdb.Get(ctx, updateTimeKey).Int64()
+		if err != nil {
+			fmt.Printf("Error retrieving updateTime for key %s: %s\n", updateTimeKey, err)
+			continue
+		}
+
+		if updateTime < startTime {
+			// 解析出原始的hashedKey
+			hashedKey := strings.TrimPrefix(updateTimeKey, "updateTime:")
+
+			// 获取与文件相关的数据
+			fileInfoData, err := rdb.Get(ctx, hashedKey).Bytes()
+			if err != nil {
+				fmt.Printf("Error retrieving fileInfo for key %s: %s\n", hashedKey, err)
+				continue
+			}
+
+			// 解码文件信息
+			var fileInfo FileInfo
+			buf := bytes.NewBuffer(fileInfoData)
+			dec := gob.NewDecoder(buf)
+			if err := dec.Decode(&fileInfo); err != nil {
+				fmt.Printf("Error decoding fileInfo for key %s: %s\n", hashedKey, err)
+				continue
+			}
+
+			// 获取文件路径
+			filePath, err := rdb.Get(ctx, "path:"+hashedKey).Result()
+			if err != nil {
+				fmt.Printf("Error retrieving filePath for key %s: %s\n", hashedKey, err)
+				continue
+			}
+
+			// 删除记录
+			pipe := rdb.Pipeline()
+			pipe.Del(ctx, updateTimeKey)     // 删除updateTime键
+			pipe.Del(ctx, hashedKey)         // 删除与hashedKey相关的数据
+			pipe.Del(ctx, "path:"+hashedKey) // 删除与path:hashedKey相关的数据
+
+			// 获取文件哈希值
+			fileHash, err := rdb.Get(ctx, "hash:"+hashedKey).Result()
+			if err != nil {
+				fmt.Printf("Error retrieving file hash for key %s: %s\n", hashedKey, err)
+			} else {
+				fmt.Printf("Deleting record with hash %s\n", fileHash)
+			}
+
+			pipe.Del(ctx, "hash:"+hashedKey) // 删除与文件哈希值相关的键
+
+			_, err = pipe.Exec(ctx)
+			if err != nil {
+				fmt.Printf("Error deleting keys for outdated record %s: %s\n", hashedKey, err)
+			} else {
+				fmt.Printf("Deleted outdated record: path=%s, size=%d, modTime=%s, hash=%s\n", filePath, fileInfo.Size, fileInfo.ModTime, fileHash)
+			}
+		}
+	}
+	return nil
+}
+
+// getAllFileHashes 从Redis中检索所有文件的哈希值及其对应的路径。
+func getAllFileHashes(rdb *redis.Client, ctx context.Context) (map[string][]string, error) {
+	fileHashes := make(map[string][]string)
+
+	// Scan用于查找所有哈希键
+	iter := rdb.Scan(ctx, 0, "hash:*", 0).Iterator()
+	for iter.Next(ctx) {
+		hashKey := iter.Val()
+		// 获取原始的hashedKey
+		hashedKey := strings.TrimPrefix(hashKey, "hash:")
+
+		// 获取与hashedKey相关的文件路径
+		filePath, err := rdb.Get(ctx, "path:"+hashedKey).Result()
+		if err != nil {
+			// 处理错误
+			continue
+		}
+
+		// 提取哈希值
+		hashValue := strings.TrimPrefix(hashKey, "hash:")
+		fileHashes[hashValue] = append(fileHashes[hashValue], filePath)
+	}
+	if err := iter.Err(); err != nil {
+		return nil, err
+	}
+
+	return fileHashes, nil
+}
diff --git a/utils.go b/utils.go
new file mode 100644
--- /dev/null
+++ ./utils.go
@@ -0,0 +1,103 @@
+// utils.go
+// 该文件包含用于整个应用程序的通用工具函数。
+
+package main
+
+import (
+	"bufio"
+	"context"
+	"fmt"
+	"github.com/go-redis/redis/v8"
+	"os"
+	"path/filepath"
+	"regexp"
+	"sort"
+	"strings"
+)
+
+func loadExcludePatterns(filename string) ([]string, error) {
+	file, err := os.Open(filename)
+	if err != nil {
+		return nil, err
+	}
+	defer file.Close()
+
+	var patterns []string
+	scanner := bufio.NewScanner(file)
+	for scanner.Scan() {
+		pattern := scanner.Text()
+		fmt.Printf("Loaded exclude pattern: %s\n", pattern) // 打印每个加载的模式
+		patterns = append(patterns, pattern)
+	}
+	return patterns, scanner.Err()
+}
+
+func sortKeys(keys []string, data map[string]FileInfo, sortByModTime bool) {
+	if sortByModTime {
+		sort.Slice(keys, func(i, j int) bool {
+			return data[keys[i]].ModTime.After(data[keys[j]].ModTime)
+		})
+	} else {
+		sort.Slice(keys, func(i, j int) bool {
+			return data[keys[i]].Size > data[keys[j]].Size
+		})
+	}
+}
+
+func compileExcludePatterns(filename string) ([]*regexp.Regexp, error) {
+	excludePatterns, err := loadExcludePatterns(filename)
+	if err != nil {
+		return nil, err
+	}
+
+	excludeRegexps := make([]*regexp.Regexp, len(excludePatterns))
+	for i, pattern := range excludePatterns {
+		regexPattern := strings.Replace(pattern, "*", ".*", -1)
+		excludeRegexps[i], err = regexp.Compile(regexPattern)
+		if err != nil {
+			return nil, fmt.Errorf("Invalid regex pattern '%s': %v", regexPattern, err)
+		}
+	}
+	return excludeRegexps, nil
+}
+
+func performSaveOperation(rootDir, filename string, sortByModTime bool, rdb *redis.Client, ctx context.Context) {
+	if err := saveToFile(rootDir, filename, sortByModTime, rdb, ctx); err != nil {
+		fmt.Printf("Error saving to %s: %s\n", filepath.Join(rootDir, filename), err)
+	} else {
+		fmt.Printf("Saved data to %s\n", filepath.Join(rootDir, filename))
+	}
+}
+
+func writeLinesToFile(filename string, lines []string) error {
+	file, err := os.Create(filename)
+	if err != nil {
+		return err
+	}
+	defer file.Close()
+
+	for _, line := range lines {
+		if _, err := fmt.Fprintln(file, line); err != nil {
+			return err
+		}
+	}
+	return nil
+}
+
+func findAndLogDuplicates(rdb *redis.Client, ctx context.Context) error {
+	hashes, err := getAllFileHashes(rdb, ctx)
+	if err != nil {
+		return err
+	}
+
+	var lines []string
+	for hash, paths := range hashes {
+		if len(paths) > 1 {
+			lines = append(lines, fmt.Sprintf("Duplicate files for hash %s:", hash))
+			lines = append(lines, paths...) // 将所有重复文件的路径添加到列表中
+		}
+	}
+
+	// 使用 writeLinesToFile 来写入文件
+	return writeLinesToFile("fav.log.dup", lines)
+}
diff --git a/worker_pool.go b/worker_pool.go
new file mode 100644
--- /dev/null
+++ ./worker_pool.go
@@ -0,0 +1,26 @@
+package main
+
+import (
+	"sync"
+)
+
+// Task 定义了工作池中的任务类型
+type Task func()
+
+// NewWorkerPool 创建并返回一个工作池
+func NewWorkerPool(workerCount int) (chan<- Task, *sync.WaitGroup) {
+	var wg sync.WaitGroup
+	taskQueue := make(chan Task)
+
+	for i := 0; i < workerCount; i++ {
+		wg.Add(1)
+		go func() {
+			defer wg.Done()
+			for task := range taskQueue {
+				task()
+			}
+		}()
+	}
+
+	return taskQueue, &wg
+}
